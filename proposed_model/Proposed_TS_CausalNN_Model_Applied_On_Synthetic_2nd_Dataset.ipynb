{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data (Synthetic Dataset-2)\n",
        "\n",
        "\n",
        "This notebook contains the proposed TS-CausalNN model. Here we have developed the proposed Causal Conv2D layer and the optimization function.\n",
        "\n",
        "The functions to visualize the predicted causal graph are available after the model training codes. The predicted graph is compared with ground truth using an adjacency matrix (array).   \n",
        "\n",
        "In this notebook, we applied the proposed model to the synthetic dataset-2 to generate a full causal graph and summary graph."
      ],
      "metadata": {
        "id": "6E2EDvOozQ_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/synthetic_2nd_dataset.csv')"
      ],
      "metadata": {
        "id": "HwXqQRuThTMN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z96f-sD2rAOk",
        "outputId": "00c14cd8-59d3-47ba-ce22-486e009ec323"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         S1        S2        S3         S4\n",
              "0  8.913554  0.000000  0.000000   0.000000\n",
              "1  9.103935  0.864968  0.931469   9.985518\n",
              "2  9.693701  0.220185 -0.186650  10.338397\n",
              "3 -1.339934 -0.060682  0.493351   3.208927\n",
              "4 -1.206586  1.666979 -3.916721   2.004742"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1e1532b-5b5f-4a8f-8302-9222869e7cbc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S1</th>\n",
              "      <th>S2</th>\n",
              "      <th>S3</th>\n",
              "      <th>S4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.913554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.103935</td>\n",
              "      <td>0.864968</td>\n",
              "      <td>0.931469</td>\n",
              "      <td>9.985518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.693701</td>\n",
              "      <td>0.220185</td>\n",
              "      <td>-0.186650</td>\n",
              "      <td>10.338397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.339934</td>\n",
              "      <td>-0.060682</td>\n",
              "      <td>0.493351</td>\n",
              "      <td>3.208927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.206586</td>\n",
              "      <td>1.666979</td>\n",
              "      <td>-3.916721</td>\n",
              "      <td>2.004742</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1e1532b-5b5f-4a8f-8302-9222869e7cbc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1e1532b-5b5f-4a8f-8302-9222869e7cbc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1e1532b-5b5f-4a8f-8302-9222869e7cbc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fb59a32f-859a-4352-8e55-a2fd2fa70fc5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb59a32f-859a-4352-8e55-a2fd2fa70fc5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fb59a32f-859a-4352-8e55-a2fd2fa70fc5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data.iloc[:5,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_graph = np.zeros((4,4))\n",
        "true_graph[0,0]=1\n",
        "true_graph[0,1]=1\n",
        "true_graph[0,2]=1\n",
        "true_graph[0,3]=1\n",
        "true_graph[2,3]=1\n",
        "true_graph[3,3]=1\n",
        "true_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b80PfUYGIwQZ",
        "outputId": "66b0cec8-d444-43c4-a40c-0da418b90092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_full_graph = np.zeros((4,24))\n",
        "true_full_graph[0,0]=1\n",
        "true_full_graph[0,12]=1\n",
        "true_full_graph[1,16]=1\n",
        "true_full_graph[2,16]=1\n",
        "true_full_graph[3,16]=1\n",
        "true_full_graph[3,18]=1\n",
        "true_full_graph[3,19]=1\n",
        "true_full_graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkng21cFeX4a",
        "outputId": "ab9e447a-76b2-41d7-ada0-fec352fdef34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 1., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install this library to measure SHD\n",
        "\n",
        "!pip install cdt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S9iW6BIFQsw",
        "outputId": "c8746be2-a2f8-49a7-df09-a1e470f05feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdt\n",
            "  Downloading cdt-0.6.0-py3-none-any.whl (921 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m921.1/921.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cdt) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cdt) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cdt) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from cdt) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cdt) (1.5.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from cdt) (0.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from cdt) (3.2.1)\n",
            "Collecting skrebate (from cdt)\n",
            "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cdt) (4.66.1)\n",
            "Collecting GPUtil (from cdt)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cdt) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdt) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cdt) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdt) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->cdt) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->cdt) (23.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels->cdt) (1.16.0)\n",
            "Building wheels for collected packages: GPUtil, skrebate\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=ccae85a98db4de8e5c02b484fd01884826689795cd0ab6fdf3f545d6ef07e02a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29253 sha256=4b012a4d05f9eac47947d035b8ed6f7b24aea59a94188965864b9fdaaedfc01a\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/67/40/683074a684607162bd0e34dcf7ccdfcab5861c3b2a83286f3a\n",
            "Successfully built GPUtil skrebate\n",
            "Installing collected packages: GPUtil, skrebate, cdt\n",
            "Successfully installed GPUtil-1.4.0 cdt-0.6.0 skrebate-0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cdt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zOqtLmEFTV5",
        "outputId": "60944923-56d1-4360-e678-5b6fb1c740e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-processing"
      ],
      "metadata": {
        "id": "_c4DRyupElS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def data_preprocessing(data, max_lag=5):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "  #convert to numpy array\n",
        "  syn_data_np = data.to_numpy()\n",
        "\n",
        "  #normalize the dataset\n",
        "  scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "  syn_data_np_nor = scaler_X.fit_transform(syn_data_np)\n",
        "  syn_data_np = syn_data_np_nor\n",
        "\n",
        "  #transform into 2D data\n",
        "  syn_data_np_T= syn_data_np.T\n",
        "  syn_data_pro = np.zeros((syn_data_np.shape[0]-max_lag,syn_data_np.shape[1],(max_lag+1)))\n",
        "  for i in range(0, (syn_data_np.shape[0]-max_lag)):\n",
        "    syn_data_pro[i,:,:]= syn_data_np_T[:, i:i+(max_lag+1)]\n",
        "  syn_data_2d = np.expand_dims(syn_data_pro, axis =-1)\n",
        "\n",
        "  #make datafrom with normalized data\n",
        "  syn_data_nor_df =pd.DataFrame(data = syn_data_np,\n",
        "                  columns = data.columns)\n",
        "\n",
        "  #transform normalized data into 1D shape with lagged and current time values\n",
        "  size_1d = syn_data_np.shape[1]*(max_lag+1)\n",
        "  print(size_1d)\n",
        "  syn_data_1d = np.zeros((syn_data_np.shape[0]-max_lag,size_1d))\n",
        "  for i in range(0, (syn_data_np.shape[0]-max_lag)):\n",
        "    for j in range(0,(max_lag+1)):\n",
        "      j_end = j * syn_data_np.shape[1]\n",
        "      syn_data_1d[i,j_end:j_end+syn_data_np.shape[1]]= syn_data_np[i+j, :]\n",
        "\n",
        "  #transform non-normalized data into 1D shape with lagged and current time values\n",
        "  syn_data_np_2 = data.to_numpy()\n",
        "  syn_data_1d_not_norm = np.zeros((syn_data_np_2.shape[0]-max_lag,size_1d))\n",
        "  for i in range(0, (syn_data_np_2.shape[0]-max_lag)):\n",
        "    for j in range(0,(max_lag+1)):\n",
        "      j_end = j * syn_data_np_2.shape[1]\n",
        "      syn_data_1d_not_norm[i,j_end:j_end+syn_data_np_2.shape[1]]= syn_data_np_2[i+j, :]\n",
        "\n",
        "\n",
        "  return syn_data_np_nor, syn_data_2d, syn_data_nor_df, syn_data_1d,  syn_data_1d_not_norm"
      ],
      "metadata": {
        "id": "4DVdqlr5lFs-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_np, syn_data_2d, syn_data_nor_df, syn_data_1d, syn_data_1d_not_norm = data_preprocessing(data, max_lag=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJH74-HmjFE5",
        "outputId": "e43f4b06-a8d7-449b-eb5a-61ccdaa3af58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_2d.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-sM8O9yjX_k",
        "outputId": "22611d09-cdfa-4d1c-ac44-edc887e08dc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99995, 4, 6, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_np.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrkQ-tqyjbgM",
        "outputId": "f94c99a8-2159-4e59-de5b-ca2103538be4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_nor_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7G3LI5yjeDz",
        "outputId": "3b86eeeb-b4e2-4bfe-c992-72dbced81d86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_1d.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHgkpfgljzhB",
        "outputId": "1c9e8871-aeb5-4c34-81d5-1d64e5d141e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99995, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data_1d_not_norm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP6kicJpj13B",
        "outputId": "a702abe5-10a6-4401-c656-a7a2c74b3f53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99995, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc79919e-7817-464a-ebcb-ebc885b0cd37",
        "id": "OLmfnPF3z9Lq"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100000, 4), (99995, 4, 6, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "syn_data_np.shape, syn_data_2d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d374d770-cf48-45f9-bc5e-b20d425fc97b",
        "id": "5a90zdwoz9Lq"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99995, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data_y_syn = syn_data_np[5:,0:4]\n",
        "data_y_syn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KVllbLUkHjt"
      },
      "source": [
        "# Proposed TS-CausalNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nyklufMFkH7O"
      },
      "outputs": [],
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout, AveragePooling2D, LSTM, Activation, ConvLSTM2D, TimeDistributed, Input, Reshape\n",
        "from keras.layers import UpSampling1D, Conv2DTranspose, UpSampling2D, Conv1D, AveragePooling1D, LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras import callbacks\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import concatenate\n",
        "from keras.regularizers import l1, l2\n",
        "from time import time\n",
        "\n",
        "keras.utils.set_random_seed(1001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proposed Custom Conv2d Layer\n",
        "\n",
        "class CausalConv2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_outputs, counter, *args, **kwargs):\n",
        "        super(CausalConv2D, self).__init__()\n",
        "        self.conv2d = tf.keras.layers.Conv2D(*args, **kwargs)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.counter = counter\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name='kernel',\n",
        "                           shape=(input_shape[1], input_shape[2],input_shape[3], 1),\n",
        "                           #initializer = keras.initializers.RandomUniform(minval=0.05, maxval=0.5),\n",
        "                           initializer = tf.keras.initializers.glorot_uniform(seed=8),\n",
        "                           trainable=True)\n",
        "        self.mask = np.ones(shape=self.W.shape)\n",
        "        print(self.W)\n",
        "        self.mask[self.counter,(input_shape[2]-1),...] = 0.0\n",
        "\n",
        "    def get_weights(self):\n",
        "        return super().get_weights()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.W.assign(tf.math.multiply(self.W, self.mask))\n",
        "        return self.conv2d.convolution_op(inputs, self.W)"
      ],
      "metadata": {
        "id": "z2o2-R2mFIm5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_2d(input_dims):\n",
        "    input_batch = Input(shape = input_dims)\n",
        "\n",
        "    conv_model = Sequential()\n",
        "    conv_model = Conv2D(filters=4, kernel_size=1, strides=(1,1), padding='valid', activation=\"linear\", name='conv1')(input_batch)\n",
        "    conv_model = tf.math.reduce_mean(conv_model, axis=-1)\n",
        "    conv_model = Reshape((4, 6, 1))(conv_model)\n",
        "    pooled_outputs = []\n",
        "    for i in range(0, 4):\n",
        "      layer = CausalConv2D(filters=1, kernel_size=(4,6), num_outputs=1, counter=i, padding='valid', activation=\"linear\",)(conv_model)\n",
        "      pooled_outputs.append(layer)\n",
        "    output = concatenate(pooled_outputs)\n",
        "    output = Flatten()(output)\n",
        "\n",
        "    model = Model(inputs=input_batch, outputs=output, name='cpred')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "WwughzzGFJIR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalNNModel(object):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 alpha=0.0,\n",
        "                 rho = 1.0,\n",
        "                 rho_max = 10e20,\n",
        "                 h_tol = 1e-8,\n",
        "                 init='glorot_uniform'):\n",
        "\n",
        "        super(CausalNNModel, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "        self.alpha = alpha\n",
        "        self.rho = rho\n",
        "        self.h_p = np.Inf\n",
        "        self.rho_max = rho_max\n",
        "        self.h_tol = h_tol\n",
        "        self.model_2d = get_model_2d(self.dims)\n",
        "        print(\"====Model created=====\")\n",
        "\n",
        "        self.model = Model(inputs=self.model_2d.input, outputs=self.model_2d.output)\n",
        "\n",
        "\n",
        "    def custom_loss_function(self, y_true, y_pred):\n",
        "      mse = keras.losses.mean_squared_error(y_true, y_pred)\n",
        "      h_val = self.causal_loss_h()\n",
        "      h_loss = 0.5 * self.rho * h_val * h_val + self.alpha * h_val\n",
        "      lambda1 = 0.1\n",
        "      adj_mat = self.get_mat()\n",
        "      sparse_loss = lambda1 * tf.math.reduce_sum(tf.abs(adj_mat))\n",
        "      neg_weight = np.sum(adj_mat, where=adj_mat<0)\n",
        "      print('MSE Loss is: {}, h Loss is: {}, L1 loss: {}, Total Loss is: {}'.format(tf.reduce_mean(mse), h_loss, sparse_loss, tf.reduce_mean(mse)+h_loss))\n",
        "      return mse + h_loss + sparse_loss\n",
        "\n",
        "    def causal_loss_h(self):\n",
        "      mat = self.get_mat()\n",
        "      h_val = self.h_acy_1(mat[:, 20:])\n",
        "      return h_val\n",
        "\n",
        "    def get_mat(self):\n",
        "      w1_2d_s = self.model.get_layer(index=-6).get_weights()\n",
        "      w2_2d_s = self.model.get_layer(index=-5).get_weights()\n",
        "      w3_2d_s = self.model.get_layer(index=-4).get_weights()\n",
        "      w4_2d_s = self.model.get_layer(index=-3).get_weights()\n",
        "      arr1_2d_s = np.expand_dims(np.squeeze(np.array(w1_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr2_2d_s = np.expand_dims(np.squeeze(np.array(w2_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr3_2d_s = np.expand_dims(np.squeeze(np.array(w3_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      arr4_2d_s = np.expand_dims(np.squeeze(np.array(w4_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "      mat_2d_s = np.concatenate((arr1_2d_s, arr2_2d_s, arr3_2d_s, arr4_2d_s))\n",
        "      #print(mat_2d_s)\n",
        "      return mat_2d_s\n",
        "\n",
        "    def h_acy_1(self, A):\n",
        "      n_var = A.shape[0]\n",
        "      h = tf.linalg.trace(tf.linalg.expm(A * A)) - n_var\n",
        "      return h\n",
        "\n",
        "\n",
        "    def h_acy(self, A):\n",
        "      '''Calculate the constraint of A ensure that it's a DAG'''\n",
        "      n_var = A.shape[0]\n",
        "      M = tf.eye(n_var, num_columns = n_var) + A/n_var\n",
        "      E = M\n",
        "      for _ in range(n_var - 2):\n",
        "        E = tf.linalg.matmul(E, M)\n",
        "      h = tf.math.reduce_sum(tf.transpose(E) * M) - n_var\n",
        "      return h\n",
        "\n",
        "    def compile(self, optimizer='adam'):\n",
        "        self.model.compile(optimizer=optimizer, loss= self.custom_loss_function) # ['mse', self.causal_loss()])\n",
        "\n",
        "    def fit(self, x, y=None, maxiter=100, batch_size=512, save_dir='./results/temp'):\n",
        "        t1 = time()\n",
        "\n",
        "        # logging file\n",
        "        import csv\n",
        "        logfile = open(save_dir + '/causalnn_log.csv', 'w')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter','loss'])\n",
        "        logwriter.writeheader()\n",
        "        train_loader = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
        "        optimizer = tf.keras.optimizers.Adam(1e-2)\n",
        "        w1_2d_s = self.model.get_layer(index=-6).get_weights()\n",
        "        arr1_2d_s = np.expand_dims(np.squeeze(np.array(w1_2d_s), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "\n",
        "\n",
        "        for epoch in range(int(maxiter)):\n",
        "          print('Epoch: {}', epoch)\n",
        "          h_n = None\n",
        "          for (x, y) in train_loader:\n",
        "            #eval loss and compute gradients\n",
        "            with tf.GradientTape() as tape:\n",
        "              tape.watch(self.model.trainable_variables)\n",
        "              #passing through neural network\n",
        "              output = self.model(x)\n",
        "              #calculate loss\n",
        "              loss = self.custom_loss_function(y, output)\n",
        "              gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "              optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "              h_n = self.causal_loss_h()\n",
        "\n",
        "          if h_n > 0.25 * self.h_p:\n",
        "                self.rho = self.rho*1.1\n",
        "          self.h_p = h_n\n",
        "          print('New h_val is :', h_n)\n",
        "          self.alpha += self.rho * self.h_p\n",
        "\n",
        "          if self.h_p <= self.h_tol or self.rho >= self.rho_max:\n",
        "            print('Before the loop end # h_val is: {}, rho is: {}'.format(self.h_p, self.rho))\n",
        "            break\n",
        "\n",
        "        #for ite in range(int(maxiter)):\n",
        "        #  print('Epoch: {}', ite)\n",
        "        #  self.model.fit(x, y, epochs=1, batch_size=batch_size, verbose=True)\n",
        "\n",
        "        # save the trained model\n",
        "        logfile.close()\n",
        "        file_name  = \"/CausalNN_model_final_\" + str(round(time()))+ \".h5\"\n",
        "        print('saving model to:', save_dir + file_name)\n",
        "        self.model.save_weights(save_dir + file_name)\n",
        "\n",
        "        w1_2d_s_1 = self.model.get_layer(index=-6).get_weights()\n",
        "        arr1_2d_s_1 = np.expand_dims(np.squeeze(np.array(w1_2d_s_1), axis=(0,3,4)).flatten('F'), axis=0)\n",
        "\n",
        "        y_pred = self.model.predict(x)\n",
        "        adj_mat = self.get_mat()\n",
        "\n",
        "        print('The conv layer 1 weights before training :', arr1_2d_s)\n",
        "        print('The conv layer 1 weights after training :', arr1_2d_s_1)\n",
        "\n",
        "        return y_pred, adj_mat"
      ],
      "metadata": {
        "id": "HbY_kEfhJ2HV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model creation and training:"
      ],
      "metadata": {
        "id": "iiveh2S10nxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(1001)\n",
        "\n",
        "cnnmodel = CausalNNModel(dims=syn_data_2d.shape[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rxv1FjaM_u6",
        "outputId": "2322b2b1-c7bc-4d63-f611-09fc568c6cf0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'causal_conv2d/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_1/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_2/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "<tf.Variable 'causal_conv2d_3/kernel:0' shape=(4, 6, 1, 1) dtype=float32, numpy=\n",
            "array([[[[ 0.01729116]],\n",
            "\n",
            "        [[ 0.05369946]],\n",
            "\n",
            "        [[-0.11558445]],\n",
            "\n",
            "        [[-0.1930318 ]],\n",
            "\n",
            "        [[-0.22727013]],\n",
            "\n",
            "        [[-0.10506889]]],\n",
            "\n",
            "\n",
            "       [[[-0.03241783]],\n",
            "\n",
            "        [[ 0.34967968]],\n",
            "\n",
            "        [[-0.22822307]],\n",
            "\n",
            "        [[ 0.18624505]],\n",
            "\n",
            "        [[-0.03919902]],\n",
            "\n",
            "        [[-0.03960952]]],\n",
            "\n",
            "\n",
            "       [[[ 0.20739552]],\n",
            "\n",
            "        [[-0.14644605]],\n",
            "\n",
            "        [[ 0.21469983]],\n",
            "\n",
            "        [[-0.17902574]],\n",
            "\n",
            "        [[ 0.06509387]],\n",
            "\n",
            "        [[ 0.17076865]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21393666]],\n",
            "\n",
            "        [[-0.3412853 ]],\n",
            "\n",
            "        [[ 0.0880048 ]],\n",
            "\n",
            "        [[-0.3153036 ]],\n",
            "\n",
            "        [[-0.07369781]],\n",
            "\n",
            "        [[ 0.15640828]]]], dtype=float32)>\n",
            "====Model created=====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnnmodel.model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqt4qRYdNCdx",
        "outputId": "ea634b7a-f36f-40a6-da43-ea5d41fde443"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 4, 6, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, 4, 6, 4)              8         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  (None, 4, 6)                 0         ['conv1[0][0]']               \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 4, 6, 1)              0         ['tf.math.reduce_mean[0][0]'] \n",
            "                                                                                                  \n",
            " causal_conv2d (CausalConv2  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " causal_conv2d_1 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " causal_conv2d_2 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " causal_conv2d_3 (CausalCon  (None, 1, 1, 1)              24        ['reshape[0][0]']             \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1, 1, 4)              0         ['causal_conv2d[0][0]',       \n",
            "                                                                     'causal_conv2d_1[0][0]',     \n",
            "                                                                     'causal_conv2d_2[0][0]',     \n",
            "                                                                     'causal_conv2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4)                    0         ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 104 (416.00 Byte)\n",
            "Trainable params: 104 (416.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnnmodel.compile()"
      ],
      "metadata": {
        "id": "cH2nDfu0NFUI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, mat = cnnmodel.fit(x=syn_data_2d, y=data_y_syn, maxiter=25, batch_size=2048, save_dir='/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQOuTYC6y9yR",
        "outputId": "0de07cd0-ed8c-44bb-cb73-e719932db0f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: {} 0\n",
            "MSE Loss is: 0.25980424880981445, h Loss is: 1.003495640361507e-06, L1 loss: 1.4565690755844116, Total Loss is: 0.25980526208877563\n",
            "MSE Loss is: 0.24516421556472778, h Loss is: 1.1561634210011107e-06, L1 loss: 1.4485690593719482, Total Loss is: 0.2451653778553009\n",
            "MSE Loss is: 0.23852862417697906, h Loss is: 1.372049609926762e-06, L1 loss: 1.4407554864883423, Total Loss is: 0.2385299950838089\n",
            "MSE Loss is: 0.2322341799736023, h Loss is: 1.6685318087183987e-06, L1 loss: 1.4332563877105713, Total Loss is: 0.232235848903656\n",
            "MSE Loss is: 0.2245505303144455, h Loss is: 2.0786701497854665e-06, L1 loss: 1.431054711341858, Total Loss is: 0.22455260157585144\n",
            "MSE Loss is: 0.21495100855827332, h Loss is: 2.652188868523808e-06, L1 loss: 1.4432461261749268, Total Loss is: 0.21495366096496582\n",
            "MSE Loss is: 0.20060357451438904, h Loss is: 3.464083420112729e-06, L1 loss: 1.456545352935791, Total Loss is: 0.200607031583786\n",
            "MSE Loss is: 0.1802404373884201, h Loss is: 4.629005161405075e-06, L1 loss: 1.4700061082839966, Total Loss is: 0.1802450716495514\n",
            "MSE Loss is: 0.15629717707633972, h Loss is: 6.320071406662464e-06, L1 loss: 1.484678030014038, Total Loss is: 0.1563034951686859\n",
            "MSE Loss is: 0.13209319114685059, h Loss is: 8.781912583799567e-06, L1 loss: 1.5062700510025024, Total Loss is: 0.13210196793079376\n",
            "MSE Loss is: 0.10276807844638824, h Loss is: 1.236029766005231e-05, L1 loss: 1.5281777381896973, Total Loss is: 0.10278043895959854\n",
            "MSE Loss is: 0.07760535180568695, h Loss is: 1.7548220057506114e-05, L1 loss: 1.5502922534942627, Total Loss is: 0.0776228979229927\n",
            "MSE Loss is: 0.0537317730486393, h Loss is: 2.498279354767874e-05, L1 loss: 1.5780771970748901, Total Loss is: 0.05375675484538078\n",
            "MSE Loss is: 0.035697661340236664, h Loss is: 3.5476230550557375e-05, L1 loss: 1.6131508350372314, Total Loss is: 0.03573313727974892\n",
            "MSE Loss is: 0.02668929472565651, h Loss is: 4.981649180990644e-05, L1 loss: 1.6479965448379517, Total Loss is: 0.02673911117017269\n",
            "MSE Loss is: 0.02717057429254055, h Loss is: 6.829624180682003e-05, L1 loss: 1.680166244506836, Total Loss is: 0.027238870039582253\n",
            "MSE Loss is: 0.03678972274065018, h Loss is: 8.976187382359058e-05, L1 loss: 1.7115204334259033, Total Loss is: 0.03687948361039162\n",
            "MSE Loss is: 0.049628086388111115, h Loss is: 0.00011080924014095217, L1 loss: 1.7346893548965454, Total Loss is: 0.049738895148038864\n",
            "MSE Loss is: 0.057904914021492004, h Loss is: 0.00012721208622679114, L1 loss: 1.7478973865509033, Total Loss is: 0.05803212523460388\n",
            "MSE Loss is: 0.059955399483442307, h Loss is: 0.00013603054685518146, L1 loss: 1.751688838005066, Total Loss is: 0.06009142845869064\n",
            "MSE Loss is: 0.05521726235747337, h Loss is: 0.00013698387192562222, L1 loss: 1.7475861310958862, Total Loss is: 0.055354245007038116\n",
            "MSE Loss is: 0.04663422703742981, h Loss is: 0.0001315766421612352, L1 loss: 1.7374067306518555, Total Loss is: 0.046765804290771484\n",
            "MSE Loss is: 0.03730140998959541, h Loss is: 0.00012207776308059692, L1 loss: 1.7229260206222534, Total Loss is: 0.03742348775267601\n",
            "MSE Loss is: 0.030250951647758484, h Loss is: 0.00011078795068897307, L1 loss: 1.7057888507843018, Total Loss is: 0.030361739918589592\n",
            "MSE Loss is: 0.024867990985512733, h Loss is: 9.943967597791925e-05, L1 loss: 1.6884678602218628, Total Loss is: 0.024967430159449577\n",
            "MSE Loss is: 0.02330637350678444, h Loss is: 8.913048077374697e-05, L1 loss: 1.6713284254074097, Total Loss is: 0.02339550480246544\n",
            "MSE Loss is: 0.024208061397075653, h Loss is: 8.047655137488618e-05, L1 loss: 1.6557506322860718, Total Loss is: 0.024288538843393326\n",
            "MSE Loss is: 0.025700047612190247, h Loss is: 7.371628453256562e-05, L1 loss: 1.6431066989898682, Total Loss is: 0.02577376365661621\n",
            "MSE Loss is: 0.028897931799292564, h Loss is: 6.883229070808738e-05, L1 loss: 1.6334632635116577, Total Loss is: 0.028966763988137245\n",
            "MSE Loss is: 0.031307511031627655, h Loss is: 6.572394340764731e-05, L1 loss: 1.6265944242477417, Total Loss is: 0.03137323632836342\n",
            "MSE Loss is: 0.03287258744239807, h Loss is: 6.423453305615112e-05, L1 loss: 1.6223303079605103, Total Loss is: 0.03293682262301445\n",
            "MSE Loss is: 0.03336341679096222, h Loss is: 6.425614992622286e-05, L1 loss: 1.6206222772598267, Total Loss is: 0.03342767432332039\n",
            "MSE Loss is: 0.03339533507823944, h Loss is: 6.563650094904006e-05, L1 loss: 1.6212921142578125, Total Loss is: 0.03346097096800804\n",
            "MSE Loss is: 0.03180813789367676, h Loss is: 6.832410872448236e-05, L1 loss: 1.62407648563385, Total Loss is: 0.03187646344304085\n",
            "MSE Loss is: 0.029447034001350403, h Loss is: 7.224726869026199e-05, L1 loss: 1.6287040710449219, Total Loss is: 0.029519280418753624\n",
            "MSE Loss is: 0.027279451489448547, h Loss is: 7.729631761321798e-05, L1 loss: 1.635467529296875, Total Loss is: 0.027356747537851334\n",
            "MSE Loss is: 0.025212997570633888, h Loss is: 8.336340397363529e-05, L1 loss: 1.6436424255371094, Total Loss is: 0.02529636025428772\n",
            "MSE Loss is: 0.024081245064735413, h Loss is: 9.02993488125503e-05, L1 loss: 1.6525007486343384, Total Loss is: 0.02417154423892498\n",
            "MSE Loss is: 0.02317623421549797, h Loss is: 9.775896614883095e-05, L1 loss: 1.6614032983779907, Total Loss is: 0.023273993283510208\n",
            "MSE Loss is: 0.023104406893253326, h Loss is: 0.000105341496237088, L1 loss: 1.6697906255722046, Total Loss is: 0.023209748789668083\n",
            "MSE Loss is: 0.02316029742360115, h Loss is: 0.00011248369264649227, L1 loss: 1.6775330305099487, Total Loss is: 0.02327278070151806\n",
            "MSE Loss is: 0.024185646325349808, h Loss is: 0.00011869648733409122, L1 loss: 1.684159517288208, Total Loss is: 0.02430434338748455\n",
            "MSE Loss is: 0.0252534132450819, h Loss is: 0.0001234750379808247, L1 loss: 1.6887520551681519, Total Loss is: 0.02537688799202442\n",
            "MSE Loss is: 0.025273513048887253, h Loss is: 0.000126498140161857, L1 loss: 1.6911033391952515, Total Loss is: 0.025400010868906975\n",
            "MSE Loss is: 0.025550294667482376, h Loss is: 0.00012769171735271811, L1 loss: 1.6911629438400269, Total Loss is: 0.025677986443042755\n",
            "MSE Loss is: 0.024540476500988007, h Loss is: 0.000127090432215482, L1 loss: 1.6891489028930664, Total Loss is: 0.024667566642165184\n",
            "MSE Loss is: 0.024686554446816444, h Loss is: 0.0001250611967407167, L1 loss: 1.685380220413208, Total Loss is: 0.02481161616742611\n",
            "MSE Loss is: 0.022868279367685318, h Loss is: 0.00012214483285788447, L1 loss: 1.680288314819336, Total Loss is: 0.022990424185991287\n",
            "MSE Loss is: 0.022655360400676727, h Loss is: 0.00011882141552632675, L1 loss: 1.6744579076766968, Total Loss is: 0.02277418226003647\n",
            "New h_val is : tf.Tensor(0.015216351, shape=(), dtype=float32)\n",
            "Epoch: {} 1\n",
            "MSE Loss is: 0.02246985025703907, h Loss is: 0.00034710284671746194, L1 loss: 1.6687380075454712, Total Loss is: 0.02281695231795311\n",
            "MSE Loss is: 0.02191166952252388, h Loss is: 0.0003416061226744205, L1 loss: 1.663761019706726, Total Loss is: 0.02225327491760254\n",
            "MSE Loss is: 0.022499753162264824, h Loss is: 0.0003375769010744989, L1 loss: 1.659322738647461, Total Loss is: 0.022837329655885696\n",
            "MSE Loss is: 0.0221283920109272, h Loss is: 0.000335368444211781, L1 loss: 1.655699610710144, Total Loss is: 0.022463761270046234\n",
            "MSE Loss is: 0.0228116437792778, h Loss is: 0.0003351822670083493, L1 loss: 1.6530812978744507, Total Loss is: 0.023146826773881912\n",
            "MSE Loss is: 0.023109328001737595, h Loss is: 0.0003372323699295521, L1 loss: 1.6515493392944336, Total Loss is: 0.02344655990600586\n",
            "MSE Loss is: 0.02330019325017929, h Loss is: 0.0003412024234421551, L1 loss: 1.651165246963501, Total Loss is: 0.023641396313905716\n",
            "MSE Loss is: 0.022993236780166626, h Loss is: 0.00034720441908575594, L1 loss: 1.651867151260376, Total Loss is: 0.023340441286563873\n",
            "MSE Loss is: 0.022885218262672424, h Loss is: 0.00035504362313076854, L1 loss: 1.6539138555526733, Total Loss is: 0.023240262642502785\n",
            "MSE Loss is: 0.02259841561317444, h Loss is: 0.0003643808886408806, L1 loss: 1.6568907499313354, Total Loss is: 0.022962797433137894\n",
            "MSE Loss is: 0.021709106862545013, h Loss is: 0.00037492954288609326, L1 loss: 1.6603847742080688, Total Loss is: 0.022084036841988564\n",
            "MSE Loss is: 0.021841194480657578, h Loss is: 0.000386272557079792, L1 loss: 1.6640809774398804, Total Loss is: 0.022227466106414795\n",
            "MSE Loss is: 0.0217013917863369, h Loss is: 0.0003976993029937148, L1 loss: 1.6677467823028564, Total Loss is: 0.022099090740084648\n",
            "MSE Loss is: 0.021535422652959824, h Loss is: 0.0004089919966645539, L1 loss: 1.6711326837539673, Total Loss is: 0.02194441482424736\n",
            "MSE Loss is: 0.02194991335272789, h Loss is: 0.00041968951700255275, L1 loss: 1.673993706703186, Total Loss is: 0.02236960269510746\n",
            "MSE Loss is: 0.0215989388525486, h Loss is: 0.00042917177779600024, L1 loss: 1.6762008666992188, Total Loss is: 0.022028110921382904\n",
            "MSE Loss is: 0.022260397672653198, h Loss is: 0.0004372138937469572, L1 loss: 1.6776793003082275, Total Loss is: 0.022697610780596733\n",
            "MSE Loss is: 0.02223612368106842, h Loss is: 0.00044336786959320307, L1 loss: 1.6783455610275269, Total Loss is: 0.022679490968585014\n",
            "MSE Loss is: 0.022213023155927658, h Loss is: 0.00044792162952944636, L1 loss: 1.6782020330429077, Total Loss is: 0.02266094461083412\n",
            "MSE Loss is: 0.02228044718503952, h Loss is: 0.00045099982526153326, L1 loss: 1.6774218082427979, Total Loss is: 0.022731447592377663\n",
            "MSE Loss is: 0.02183454856276512, h Loss is: 0.00045308881090022624, L1 loss: 1.676284670829773, Total Loss is: 0.022287636995315552\n",
            "MSE Loss is: 0.021498799324035645, h Loss is: 0.00045445674913935363, L1 loss: 1.6750551462173462, Total Loss is: 0.02195325680077076\n",
            "MSE Loss is: 0.021338634192943573, h Loss is: 0.0004554233164526522, L1 loss: 1.6738909482955933, Total Loss is: 0.02179405838251114\n",
            "MSE Loss is: 0.02198001742362976, h Loss is: 0.0004566488496493548, L1 loss: 1.6728824377059937, Total Loss is: 0.02243666537106037\n",
            "MSE Loss is: 0.02131795883178711, h Loss is: 0.0004584896087180823, L1 loss: 1.6722360849380493, Total Loss is: 0.02177644893527031\n",
            "MSE Loss is: 0.021543199196457863, h Loss is: 0.0004609971074387431, L1 loss: 1.6720236539840698, Total Loss is: 0.022004196420311928\n",
            "MSE Loss is: 0.021841445937752724, h Loss is: 0.0004644193686544895, L1 loss: 1.6722824573516846, Total Loss is: 0.022305864840745926\n",
            "MSE Loss is: 0.021283265203237534, h Loss is: 0.00046892743557691574, L1 loss: 1.673019289970398, Total Loss is: 0.021752193570137024\n",
            "MSE Loss is: 0.021683748811483383, h Loss is: 0.0004745337937492877, L1 loss: 1.6741875410079956, Total Loss is: 0.02215828187763691\n",
            "MSE Loss is: 0.02134992554783821, h Loss is: 0.0004811385879293084, L1 loss: 1.6757194995880127, Total Loss is: 0.02183106355369091\n",
            "MSE Loss is: 0.021245690062642097, h Loss is: 0.0004887247923761606, L1 loss: 1.6775559186935425, Total Loss is: 0.021734414622187614\n",
            "MSE Loss is: 0.0212335754185915, h Loss is: 0.0004971433081664145, L1 loss: 1.6795673370361328, Total Loss is: 0.02173071913421154\n",
            "MSE Loss is: 0.02161666937172413, h Loss is: 0.000505922653246671, L1 loss: 1.6816198825836182, Total Loss is: 0.022122591733932495\n",
            "MSE Loss is: 0.021380163729190826, h Loss is: 0.0005149337812326849, L1 loss: 1.6835955381393433, Total Loss is: 0.021895097568631172\n",
            "MSE Loss is: 0.02119343727827072, h Loss is: 0.0005238903686404228, L1 loss: 1.6854032278060913, Total Loss is: 0.02171732857823372\n",
            "MSE Loss is: 0.0210249200463295, h Loss is: 0.0005324807716533542, L1 loss: 1.6869159936904907, Total Loss is: 0.0215574000030756\n",
            "MSE Loss is: 0.02110985852777958, h Loss is: 0.0005405236734077334, L1 loss: 1.6881474256515503, Total Loss is: 0.02165038138628006\n",
            "MSE Loss is: 0.021403878927230835, h Loss is: 0.0005483207060024142, L1 loss: 1.6890605688095093, Total Loss is: 0.021952198818325996\n",
            "MSE Loss is: 0.021362632513046265, h Loss is: 0.0005555696552619338, L1 loss: 1.6896768808364868, Total Loss is: 0.021918201819062233\n",
            "MSE Loss is: 0.021406948566436768, h Loss is: 0.0005624714540317655, L1 loss: 1.69003427028656, Total Loss is: 0.021969420835375786\n",
            "MSE Loss is: 0.021020226180553436, h Loss is: 0.0005689497338607907, L1 loss: 1.6902328729629517, Total Loss is: 0.02158917672932148\n",
            "MSE Loss is: 0.021187936887145042, h Loss is: 0.0005753523437306285, L1 loss: 1.6902637481689453, Total Loss is: 0.021763289347290993\n",
            "MSE Loss is: 0.021432258188724518, h Loss is: 0.0005818739300593734, L1 loss: 1.690301537513733, Total Loss is: 0.022014131769537926\n",
            "MSE Loss is: 0.020876530557870865, h Loss is: 0.0005886408034712076, L1 loss: 1.6904058456420898, Total Loss is: 0.02146517112851143\n",
            "MSE Loss is: 0.02120918035507202, h Loss is: 0.0005959980189800262, L1 loss: 1.6905750036239624, Total Loss is: 0.021805178374052048\n",
            "MSE Loss is: 0.020635558292269707, h Loss is: 0.0006036466220393777, L1 loss: 1.6909793615341187, Total Loss is: 0.021239204332232475\n",
            "MSE Loss is: 0.02134571224451065, h Loss is: 0.0006116270087659359, L1 loss: 1.6915963888168335, Total Loss is: 0.021957339718937874\n",
            "MSE Loss is: 0.020217228680849075, h Loss is: 0.0006203099619597197, L1 loss: 1.6923980712890625, Total Loss is: 0.020837537944316864\n",
            "MSE Loss is: 0.020557228475809097, h Loss is: 0.0006295230705291033, L1 loss: 1.6933107376098633, Total Loss is: 0.02118675224483013\n",
            "New h_val is : tf.Tensor(0.023686409, shape=(), dtype=float32)\n",
            "Epoch: {} 2\n",
            "MSE Loss is: 0.020743347704410553, h Loss is: 0.00128322618547827, L1 loss: 1.6942920684814453, Total Loss is: 0.02202657423913479\n",
            "MSE Loss is: 0.02030935510993004, h Loss is: 0.0013007475063204765, L1 loss: 1.695383071899414, Total Loss is: 0.02161010354757309\n",
            "MSE Loss is: 0.020685657858848572, h Loss is: 0.0013190226163715124, L1 loss: 1.6965421438217163, Total Loss is: 0.022004680708050728\n",
            "MSE Loss is: 0.020120246335864067, h Loss is: 0.0013376702554523945, L1 loss: 1.6977291107177734, Total Loss is: 0.021457916125655174\n",
            "MSE Loss is: 0.020546531304717064, h Loss is: 0.0013562047388404608, L1 loss: 1.6988509893417358, Total Loss is: 0.02190273627638817\n",
            "MSE Loss is: 0.02060171216726303, h Loss is: 0.0013750169891864061, L1 loss: 1.6998627185821533, Total Loss is: 0.021976729854941368\n",
            "MSE Loss is: 0.020816918462514877, h Loss is: 0.0013924301601946354, L1 loss: 1.7008036375045776, Total Loss is: 0.022209348157048225\n",
            "MSE Loss is: 0.020711053162813187, h Loss is: 0.0014092857018113136, L1 loss: 1.701666235923767, Total Loss is: 0.022120337933301926\n",
            "MSE Loss is: 0.020759420469403267, h Loss is: 0.0014257088769227266, L1 loss: 1.7023597955703735, Total Loss is: 0.022185130044817924\n",
            "MSE Loss is: 0.02077650837600231, h Loss is: 0.0014416940975934267, L1 loss: 1.7031316757202148, Total Loss is: 0.02221820317208767\n",
            "MSE Loss is: 0.020108863711357117, h Loss is: 0.0014577372930943966, L1 loss: 1.7039501667022705, Total Loss is: 0.0215666014701128\n",
            "MSE Loss is: 0.02036299556493759, h Loss is: 0.001474139979109168, L1 loss: 1.7046581506729126, Total Loss is: 0.021837135776877403\n",
            "MSE Loss is: 0.02024669200181961, h Loss is: 0.0014904681593179703, L1 loss: 1.7053626775741577, Total Loss is: 0.02173716016113758\n",
            "MSE Loss is: 0.020026246085762978, h Loss is: 0.001508139306679368, L1 loss: 1.7060863971710205, Total Loss is: 0.021534385159611702\n",
            "MSE Loss is: 0.020217444747686386, h Loss is: 0.0015273718163371086, L1 loss: 1.7068274021148682, Total Loss is: 0.02174481749534607\n",
            "MSE Loss is: 0.019940152764320374, h Loss is: 0.0015476392582058907, L1 loss: 1.7076538801193237, Total Loss is: 0.02148779109120369\n",
            "MSE Loss is: 0.02039746753871441, h Loss is: 0.0015691251028329134, L1 loss: 1.7086155414581299, Total Loss is: 0.021966593340039253\n",
            "MSE Loss is: 0.020405158400535583, h Loss is: 0.0015905378386378288, L1 loss: 1.7096538543701172, Total Loss is: 0.021995697170495987\n",
            "MSE Loss is: 0.020410969853401184, h Loss is: 0.0016127047128975391, L1 loss: 1.7106748819351196, Total Loss is: 0.022023674100637436\n",
            "MSE Loss is: 0.02055056020617485, h Loss is: 0.0016350443474948406, L1 loss: 1.7117308378219604, Total Loss is: 0.02218560501933098\n",
            "MSE Loss is: 0.02020137384533882, h Loss is: 0.0016581156523898244, L1 loss: 1.7128044366836548, Total Loss is: 0.021859489381313324\n",
            "MSE Loss is: 0.019931010901927948, h Loss is: 0.0016812620451673865, L1 loss: 1.7138980627059937, Total Loss is: 0.021612273529171944\n",
            "MSE Loss is: 0.019870314747095108, h Loss is: 0.001703672925941646, L1 loss: 1.7149986028671265, Total Loss is: 0.0215739868581295\n",
            "MSE Loss is: 0.020408427342772484, h Loss is: 0.001726185786537826, L1 loss: 1.7161133289337158, Total Loss is: 0.02213461324572563\n",
            "MSE Loss is: 0.019778313115239143, h Loss is: 0.001748800277709961, L1 loss: 1.7173715829849243, Total Loss is: 0.021527113392949104\n",
            "MSE Loss is: 0.019930247217416763, h Loss is: 0.001770303351804614, L1 loss: 1.7186638116836548, Total Loss is: 0.021700549870729446\n",
            "MSE Loss is: 0.020226769149303436, h Loss is: 0.0017911456525325775, L1 loss: 1.7199467420578003, Total Loss is: 0.022017914801836014\n",
            "MSE Loss is: 0.01968992128968239, h Loss is: 0.0018117489526048303, L1 loss: 1.7212038040161133, Total Loss is: 0.02150166966021061\n",
            "MSE Loss is: 0.020060231909155846, h Loss is: 0.0018320375820621848, L1 loss: 1.7223776578903198, Total Loss is: 0.02189227007329464\n",
            "MSE Loss is: 0.019738508388400078, h Loss is: 0.0018524054903537035, L1 loss: 1.7234760522842407, Total Loss is: 0.021590914577245712\n",
            "MSE Loss is: 0.019689463078975677, h Loss is: 0.001873362110927701, L1 loss: 1.7245286703109741, Total Loss is: 0.02156282588839531\n",
            "MSE Loss is: 0.019728023558855057, h Loss is: 0.0018955712439492345, L1 loss: 1.7255419492721558, Total Loss is: 0.02162359468638897\n",
            "MSE Loss is: 0.02005258947610855, h Loss is: 0.0019181668758392334, L1 loss: 1.726532220840454, Total Loss is: 0.021970756351947784\n",
            "MSE Loss is: 0.019883660599589348, h Loss is: 0.001941779046319425, L1 loss: 1.7275434732437134, Total Loss is: 0.021825440227985382\n",
            "MSE Loss is: 0.01970672234892845, h Loss is: 0.001966234762221575, L1 loss: 1.7286485433578491, Total Loss is: 0.02167295664548874\n",
            "MSE Loss is: 0.019507180899381638, h Loss is: 0.001991246361285448, L1 loss: 1.7297605276107788, Total Loss is: 0.0214984267950058\n",
            "MSE Loss is: 0.01953679695725441, h Loss is: 0.0020168947521597147, L1 loss: 1.7309391498565674, Total Loss is: 0.02155369147658348\n",
            "MSE Loss is: 0.019788190722465515, h Loss is: 0.002044388558715582, L1 loss: 1.7321808338165283, Total Loss is: 0.021832579746842384\n",
            "MSE Loss is: 0.019773315638303757, h Loss is: 0.0020725075155496597, L1 loss: 1.7334545850753784, Total Loss is: 0.021845823153853416\n",
            "MSE Loss is: 0.01982886902987957, h Loss is: 0.002101676072925329, L1 loss: 1.7351316213607788, Total Loss is: 0.021930545568466187\n",
            "MSE Loss is: 0.019508708268404007, h Loss is: 0.0021309175062924623, L1 loss: 1.7370628118515015, Total Loss is: 0.0216396264731884\n",
            "MSE Loss is: 0.01962587796151638, h Loss is: 0.0021606897935271263, L1 loss: 1.7389248609542847, Total Loss is: 0.021786566823720932\n",
            "MSE Loss is: 0.01986107788980007, h Loss is: 0.0021909603383392096, L1 loss: 1.740767478942871, Total Loss is: 0.022052038460969925\n",
            "MSE Loss is: 0.019336998462677002, h Loss is: 0.0022212702315300703, L1 loss: 1.7425537109375, Total Loss is: 0.02155826799571514\n",
            "MSE Loss is: 0.01958320662379265, h Loss is: 0.0022523957304656506, L1 loss: 1.7442833185195923, Total Loss is: 0.021835602819919586\n",
            "MSE Loss is: 0.0191123615950346, h Loss is: 0.002282547764480114, L1 loss: 1.7460399866104126, Total Loss is: 0.02139490842819214\n",
            "MSE Loss is: 0.019804466515779495, h Loss is: 0.0023115919902920723, L1 loss: 1.7477760314941406, Total Loss is: 0.022116057574748993\n",
            "MSE Loss is: 0.018726065754890442, h Loss is: 0.002341128885746002, L1 loss: 1.7494605779647827, Total Loss is: 0.021067194640636444\n",
            "MSE Loss is: 0.01900675520300865, h Loss is: 0.0023705684579908848, L1 loss: 1.7510782480239868, Total Loss is: 0.02137732319533825\n",
            "New h_val is : tf.Tensor(0.038515568, shape=(), dtype=float32)\n",
            "Epoch: {} 3\n",
            "MSE Loss is: 0.019160263240337372, h Loss is: 0.004274358972907066, L1 loss: 1.7526333332061768, Total Loss is: 0.023434622213244438\n",
            "MSE Loss is: 0.018797820433974266, h Loss is: 0.004323597997426987, L1 loss: 1.7541874647140503, Total Loss is: 0.023121418431401253\n",
            "MSE Loss is: 0.019129052758216858, h Loss is: 0.004374547395855188, L1 loss: 1.7557621002197266, Total Loss is: 0.02350359968841076\n",
            "MSE Loss is: 0.01865469664335251, h Loss is: 0.0044272225350141525, L1 loss: 1.7573814392089844, Total Loss is: 0.02308191917836666\n",
            "MSE Loss is: 0.019041072577238083, h Loss is: 0.004480470437556505, L1 loss: 1.759055733680725, Total Loss is: 0.0235215425491333\n",
            "MSE Loss is: 0.01898316480219364, h Loss is: 0.004536512307822704, L1 loss: 1.7607450485229492, Total Loss is: 0.02351967617869377\n",
            "MSE Loss is: 0.019292719662189484, h Loss is: 0.004589681979268789, L1 loss: 1.7624794244766235, Total Loss is: 0.02388240210711956\n",
            "MSE Loss is: 0.01925484463572502, h Loss is: 0.0046432968229055405, L1 loss: 1.7642452716827393, Total Loss is: 0.023898141458630562\n",
            "MSE Loss is: 0.0192144513130188, h Loss is: 0.004697688389569521, L1 loss: 1.7659988403320312, Total Loss is: 0.023912139236927032\n",
            "MSE Loss is: 0.019380614161491394, h Loss is: 0.004752268549054861, L1 loss: 1.7676976919174194, Total Loss is: 0.024132883176207542\n",
            "MSE Loss is: 0.018662236630916595, h Loss is: 0.004808562342077494, L1 loss: 1.7693922519683838, Total Loss is: 0.023470798507332802\n",
            "MSE Loss is: 0.018933366984128952, h Loss is: 0.004866852890700102, L1 loss: 1.7710002660751343, Total Loss is: 0.02380022034049034\n",
            "MSE Loss is: 0.018796607851982117, h Loss is: 0.004924620036035776, L1 loss: 1.772589087486267, Total Loss is: 0.023721227422356606\n",
            "MSE Loss is: 0.018555447459220886, h Loss is: 0.004986416082829237, L1 loss: 1.77414071559906, Total Loss is: 0.02354186400771141\n",
            "MSE Loss is: 0.018628735095262527, h Loss is: 0.005052285268902779, L1 loss: 1.7756325006484985, Total Loss is: 0.023681020364165306\n",
            "MSE Loss is: 0.01847628504037857, h Loss is: 0.005119974259287119, L1 loss: 1.7770954370498657, Total Loss is: 0.023596258834004402\n",
            "MSE Loss is: 0.018876390531659126, h Loss is: 0.00518950168043375, L1 loss: 1.778753638267517, Total Loss is: 0.02406589314341545\n",
            "MSE Loss is: 0.01893487572669983, h Loss is: 0.005256386008113623, L1 loss: 1.7803611755371094, Total Loss is: 0.02419126220047474\n",
            "MSE Loss is: 0.018913155421614647, h Loss is: 0.0053235371597111225, L1 loss: 1.781866431236267, Total Loss is: 0.024236692115664482\n",
            "MSE Loss is: 0.01905890926718712, h Loss is: 0.005389304831624031, L1 loss: 1.7833393812179565, Total Loss is: 0.02444821409881115\n",
            "MSE Loss is: 0.018694497644901276, h Loss is: 0.005456221755594015, L1 loss: 1.7847713232040405, Total Loss is: 0.024150719866156578\n",
            "MSE Loss is: 0.01842956617474556, h Loss is: 0.0055224294774234295, L1 loss: 1.7862236499786377, Total Loss is: 0.023951996117830276\n",
            "MSE Loss is: 0.01842903345823288, h Loss is: 0.00558597082272172, L1 loss: 1.7876957654953003, Total Loss is: 0.024015003815293312\n",
            "MSE Loss is: 0.018890243023633957, h Loss is: 0.005650021601468325, L1 loss: 1.7892509698867798, Total Loss is: 0.024540264159440994\n",
            "MSE Loss is: 0.018301084637641907, h Loss is: 0.005715286359190941, L1 loss: 1.7909015417099, Total Loss is: 0.024016370996832848\n",
            "MSE Loss is: 0.018405627459287643, h Loss is: 0.00577790942043066, L1 loss: 1.7925589084625244, Total Loss is: 0.02418353781104088\n",
            "MSE Loss is: 0.0187178123742342, h Loss is: 0.005839483812451363, L1 loss: 1.794211983680725, Total Loss is: 0.024557296186685562\n",
            "MSE Loss is: 0.01820245012640953, h Loss is: 0.005901692435145378, L1 loss: 1.7958179712295532, Total Loss is: 0.02410414256155491\n",
            "MSE Loss is: 0.01854986883699894, h Loss is: 0.005963973235338926, L1 loss: 1.7973359823226929, Total Loss is: 0.024513842537999153\n",
            "MSE Loss is: 0.018220383673906326, h Loss is: 0.006027534604072571, L1 loss: 1.7988020181655884, Total Loss is: 0.024247918277978897\n",
            "MSE Loss is: 0.018212517723441124, h Loss is: 0.006093960255384445, L1 loss: 1.800218939781189, Total Loss is: 0.02430647797882557\n",
            "MSE Loss is: 0.018277589231729507, h Loss is: 0.006165431812405586, L1 loss: 1.8016350269317627, Total Loss is: 0.024443021044135094\n",
            "MSE Loss is: 0.01852424256503582, h Loss is: 0.006237897090613842, L1 loss: 1.8029965162277222, Total Loss is: 0.024762138724327087\n",
            "MSE Loss is: 0.01843053661286831, h Loss is: 0.006313244812190533, L1 loss: 1.804323434829712, Total Loss is: 0.024743780493736267\n",
            "MSE Loss is: 0.018287280574440956, h Loss is: 0.006390635389834642, L1 loss: 1.8056429624557495, Total Loss is: 0.02467791549861431\n",
            "MSE Loss is: 0.018058421090245247, h Loss is: 0.006468558683991432, L1 loss: 1.8069483041763306, Total Loss is: 0.02452697977423668\n",
            "MSE Loss is: 0.01804247871041298, h Loss is: 0.006547090131789446, L1 loss: 1.8082979917526245, Total Loss is: 0.024589568376541138\n",
            "MSE Loss is: 0.018252987414598465, h Loss is: 0.006630349904298782, L1 loss: 1.809722900390625, Total Loss is: 0.024883337318897247\n",
            "MSE Loss is: 0.018263261765241623, h Loss is: 0.006713815964758396, L1 loss: 1.8111780881881714, Total Loss is: 0.024977076798677444\n",
            "MSE Loss is: 0.01832234114408493, h Loss is: 0.0067995632998645306, L1 loss: 1.8126957416534424, Total Loss is: 0.02512190490961075\n",
            "MSE Loss is: 0.018067672848701477, h Loss is: 0.006883520632982254, L1 loss: 1.8142417669296265, Total Loss is: 0.02495119348168373\n",
            "MSE Loss is: 0.01812492311000824, h Loss is: 0.006968501955270767, L1 loss: 1.8157097101211548, Total Loss is: 0.025093425065279007\n",
            "MSE Loss is: 0.018351014703512192, h Loss is: 0.007053765468299389, L1 loss: 1.8171504735946655, Total Loss is: 0.025404781103134155\n",
            "MSE Loss is: 0.01786147989332676, h Loss is: 0.0071382541209459305, L1 loss: 1.8185261487960815, Total Loss is: 0.02499973401427269\n",
            "MSE Loss is: 0.018033379688858986, h Loss is: 0.00722528574988246, L1 loss: 1.8198425769805908, Total Loss is: 0.025258665904402733\n",
            "MSE Loss is: 0.017674464732408524, h Loss is: 0.007308501750230789, L1 loss: 1.8212076425552368, Total Loss is: 0.024982966482639313\n",
            "MSE Loss is: 0.01835869625210762, h Loss is: 0.007388080470263958, L1 loss: 1.822630524635315, Total Loss is: 0.025746777653694153\n",
            "MSE Loss is: 0.017319008708000183, h Loss is: 0.0074693383648991585, L1 loss: 1.8240604400634766, Total Loss is: 0.024788346141576767\n",
            "MSE Loss is: 0.017549239099025726, h Loss is: 0.007551368325948715, L1 loss: 1.8254261016845703, Total Loss is: 0.02510060742497444\n",
            "New h_val is : tf.Tensor(0.06122923, shape=(), dtype=float32)\n",
            "Epoch: {} 4\n",
            "MSE Loss is: 0.017675071954727173, h Loss is: 0.012846313416957855, L1 loss: 1.826690673828125, Total Loss is: 0.030521385371685028\n",
            "MSE Loss is: 0.017386488616466522, h Loss is: 0.012981265783309937, L1 loss: 1.8279424905776978, Total Loss is: 0.03036775439977646\n",
            "MSE Loss is: 0.01766928657889366, h Loss is: 0.013122976757586002, L1 loss: 1.82918381690979, Total Loss is: 0.03079226240515709\n",
            "MSE Loss is: 0.017282651737332344, h Loss is: 0.013270417228341103, L1 loss: 1.8304575681686401, Total Loss is: 0.030553068965673447\n",
            "MSE Loss is: 0.017638888210058212, h Loss is: 0.013420246541500092, L1 loss: 1.8317985534667969, Total Loss is: 0.031059134751558304\n",
            "MSE Loss is: 0.01747688464820385, h Loss is: 0.01357770524919033, L1 loss: 1.8331127166748047, Total Loss is: 0.03105458989739418\n",
            "MSE Loss is: 0.017888061702251434, h Loss is: 0.013724591583013535, L1 loss: 1.8344885110855103, Total Loss is: 0.03161265328526497\n",
            "MSE Loss is: 0.01792225055396557, h Loss is: 0.013871186412870884, L1 loss: 1.8358920812606812, Total Loss is: 0.03179343789815903\n",
            "MSE Loss is: 0.017803359776735306, h Loss is: 0.014018584974110126, L1 loss: 1.8372589349746704, Total Loss is: 0.03182194381952286\n",
            "MSE Loss is: 0.01811954751610756, h Loss is: 0.014164949767291546, L1 loss: 1.8385406732559204, Total Loss is: 0.03228449821472168\n",
            "MSE Loss is: 0.017352832481265068, h Loss is: 0.01431494951248169, L1 loss: 1.8398109674453735, Total Loss is: 0.03166778385639191\n",
            "MSE Loss is: 0.017644550651311874, h Loss is: 0.014470094814896584, L1 loss: 1.8409572839736938, Total Loss is: 0.03211464732885361\n",
            "MSE Loss is: 0.017487406730651855, h Loss is: 0.014622372575104237, L1 loss: 1.8421016931533813, Total Loss is: 0.03210977837443352\n",
            "MSE Loss is: 0.01722293719649315, h Loss is: 0.014786175452172756, L1 loss: 1.8432369232177734, Total Loss is: 0.03200911357998848\n",
            "MSE Loss is: 0.01719062030315399, h Loss is: 0.014962486922740936, L1 loss: 1.844298243522644, Total Loss is: 0.03215310722589493\n",
            "MSE Loss is: 0.017171185463666916, h Loss is: 0.015142660588026047, L1 loss: 1.8453972339630127, Total Loss is: 0.03231384605169296\n",
            "MSE Loss is: 0.017519749701023102, h Loss is: 0.01532698143273592, L1 loss: 1.8466007709503174, Total Loss is: 0.03284673020243645\n",
            "MSE Loss is: 0.017643027007579803, h Loss is: 0.015500855632126331, L1 loss: 1.8477455377578735, Total Loss is: 0.03314388170838356\n",
            "MSE Loss is: 0.0175931416451931, h Loss is: 0.015673283487558365, L1 loss: 1.848912239074707, Total Loss is: 0.033266425132751465\n",
            "MSE Loss is: 0.01774946041405201, h Loss is: 0.015839548781514168, L1 loss: 1.8500769138336182, Total Loss is: 0.03358900919556618\n",
            "MSE Loss is: 0.01738375425338745, h Loss is: 0.016008242964744568, L1 loss: 1.8511933088302612, Total Loss is: 0.03339199721813202\n",
            "MSE Loss is: 0.017117004841566086, h Loss is: 0.016173383221030235, L1 loss: 1.852320909500122, Total Loss is: 0.03329038619995117\n",
            "MSE Loss is: 0.0171784907579422, h Loss is: 0.016329679638147354, L1 loss: 1.8534832000732422, Total Loss is: 0.033508170396089554\n",
            "MSE Loss is: 0.017576364800333977, h Loss is: 0.016487713903188705, L1 loss: 1.85470712184906, Total Loss is: 0.03406407684087753\n",
            "MSE Loss is: 0.01702577993273735, h Loss is: 0.016650201752781868, L1 loss: 1.8560545444488525, Total Loss is: 0.03367598354816437\n",
            "MSE Loss is: 0.01710263267159462, h Loss is: 0.016805287450551987, L1 loss: 1.8574097156524658, Total Loss is: 0.033907920122146606\n",
            "MSE Loss is: 0.01742786169052124, h Loss is: 0.016958482563495636, L1 loss: 1.8587398529052734, Total Loss is: 0.034386344254016876\n",
            "MSE Loss is: 0.01694542169570923, h Loss is: 0.017115605995059013, L1 loss: 1.8599733114242554, Total Loss is: 0.03406102955341339\n",
            "MSE Loss is: 0.017285816371440887, h Loss is: 0.017274998128414154, L1 loss: 1.8611942529678345, Total Loss is: 0.03456081449985504\n",
            "MSE Loss is: 0.016955912113189697, h Loss is: 0.0174400731921196, L1 loss: 1.8625118732452393, Total Loss is: 0.034395985305309296\n",
            "MSE Loss is: 0.016988491639494896, h Loss is: 0.017615068703889847, L1 loss: 1.8637666702270508, Total Loss is: 0.034603558480739594\n",
            "MSE Loss is: 0.01708255335688591, h Loss is: 0.01780599355697632, L1 loss: 1.8650137186050415, Total Loss is: 0.03488854691386223\n",
            "MSE Loss is: 0.017259890213608742, h Loss is: 0.0179983489215374, L1 loss: 1.866196632385254, Total Loss is: 0.03525824099779129\n",
            "MSE Loss is: 0.017241500318050385, h Loss is: 0.018196778371930122, L1 loss: 1.8673527240753174, Total Loss is: 0.03543827682733536\n",
            "MSE Loss is: 0.017134055495262146, h Loss is: 0.018397219479084015, L1 loss: 1.868501901626587, Total Loss is: 0.03553127497434616\n",
            "MSE Loss is: 0.016889654099941254, h Loss is: 0.01859448291361332, L1 loss: 1.8696542978286743, Total Loss is: 0.035484135150909424\n",
            "MSE Loss is: 0.016839120537042618, h Loss is: 0.018787195906043053, L1 loss: 1.8708833456039429, Total Loss is: 0.03562631458044052\n",
            "MSE Loss is: 0.01702004112303257, h Loss is: 0.018988188356161118, L1 loss: 1.8721332550048828, Total Loss is: 0.03600823134183884\n",
            "MSE Loss is: 0.017055094242095947, h Loss is: 0.01918460987508297, L1 loss: 1.8734461069107056, Total Loss is: 0.036239705979824066\n",
            "MSE Loss is: 0.017127230763435364, h Loss is: 0.019383298233151436, L1 loss: 1.874857783317566, Total Loss is: 0.03651052713394165\n",
            "MSE Loss is: 0.016927246004343033, h Loss is: 0.019572613760828972, L1 loss: 1.8763340711593628, Total Loss is: 0.036499857902526855\n",
            "MSE Loss is: 0.016940150409936905, h Loss is: 0.019763201475143433, L1 loss: 1.8776905536651611, Total Loss is: 0.03670335188508034\n",
            "MSE Loss is: 0.017166070640087128, h Loss is: 0.01995329186320305, L1 loss: 1.8790225982666016, Total Loss is: 0.037119362503290176\n",
            "MSE Loss is: 0.016720591112971306, h Loss is: 0.02014205977320671, L1 loss: 1.880312204360962, Total Loss is: 0.03686264902353287\n",
            "MSE Loss is: 0.01683002896606922, h Loss is: 0.020341286435723305, L1 loss: 1.8814760446548462, Total Loss is: 0.037171315401792526\n",
            "MSE Loss is: 0.016571316868066788, h Loss is: 0.020531926304101944, L1 loss: 1.8827142715454102, Total Loss is: 0.03710324317216873\n",
            "MSE Loss is: 0.01726871356368065, h Loss is: 0.020715419203042984, L1 loss: 1.8839292526245117, Total Loss is: 0.03798413276672363\n",
            "MSE Loss is: 0.01625470444560051, h Loss is: 0.02090817131102085, L1 loss: 1.8849996328353882, Total Loss is: 0.03716287761926651\n",
            "MSE Loss is: 0.016462305560708046, h Loss is: 0.021106651052832603, L1 loss: 1.8859211206436157, Total Loss is: 0.03756895661354065\n",
            "New h_val is : tf.Tensor(0.09244585, shape=(), dtype=float32)\n",
            "Epoch: {} 5\n",
            "MSE Loss is: 0.01656411588191986, h Loss is: 0.03438369184732437, L1 loss: 1.8867079019546509, Total Loss is: 0.05094780772924423\n",
            "MSE Loss is: 0.01633918285369873, h Loss is: 0.0347098745405674, L1 loss: 1.8874977827072144, Total Loss is: 0.05104905739426613\n",
            "MSE Loss is: 0.016586046665906906, h Loss is: 0.035053934901952744, L1 loss: 1.8882826566696167, Total Loss is: 0.05163998156785965\n",
            "MSE Loss is: 0.016273248940706253, h Loss is: 0.035408396273851395, L1 loss: 1.8891133069992065, Total Loss is: 0.05168164521455765\n",
            "MSE Loss is: 0.016611963510513306, h Loss is: 0.03576400876045227, L1 loss: 1.8900082111358643, Total Loss is: 0.052375972270965576\n",
            "MSE Loss is: 0.01639517955482006, h Loss is: 0.03613140061497688, L1 loss: 1.8908497095108032, Total Loss is: 0.052526578307151794\n",
            "MSE Loss is: 0.016881506890058517, h Loss is: 0.03645653277635574, L1 loss: 1.8918002843856812, Total Loss is: 0.05333803966641426\n",
            "MSE Loss is: 0.01697065681219101, h Loss is: 0.036768779158592224, L1 loss: 1.8928078413009644, Total Loss is: 0.053739435970783234\n",
            "MSE Loss is: 0.016802653670310974, h Loss is: 0.03707638755440712, L1 loss: 1.8937408924102783, Total Loss is: 0.053879041224718094\n",
            "MSE Loss is: 0.01722898706793785, h Loss is: 0.03737460449337959, L1 loss: 1.894570231437683, Total Loss is: 0.054603591561317444\n",
            "MSE Loss is: 0.01643868163228035, h Loss is: 0.03768099099397659, L1 loss: 1.895350694656372, Total Loss is: 0.05411967262625694\n",
            "MSE Loss is: 0.016738131642341614, h Loss is: 0.03800291568040848, L1 loss: 1.8959625959396362, Total Loss is: 0.05474104732275009\n",
            "MSE Loss is: 0.016585659235715866, h Loss is: 0.03831954300403595, L1 loss: 1.8965791463851929, Total Loss is: 0.054905202239751816\n",
            "MSE Loss is: 0.016306646168231964, h Loss is: 0.038673900067806244, L1 loss: 1.8972030878067017, Total Loss is: 0.05498054623603821\n",
            "MSE Loss is: 0.016199253499507904, h Loss is: 0.0390653982758522, L1 loss: 1.897740364074707, Total Loss is: 0.05526465177536011\n",
            "MSE Loss is: 0.016275878995656967, h Loss is: 0.03947017341852188, L1 loss: 1.898375153541565, Total Loss is: 0.05574605241417885\n",
            "MSE Loss is: 0.016598043963313103, h Loss is: 0.039887018501758575, L1 loss: 1.8991355895996094, Total Loss is: 0.05648506432771683\n",
            "MSE Loss is: 0.016784530133008957, h Loss is: 0.04027201607823372, L1 loss: 1.8998478651046753, Total Loss is: 0.057056546211242676\n",
            "MSE Loss is: 0.016706807538866997, h Loss is: 0.04064883291721344, L1 loss: 1.9003146886825562, Total Loss is: 0.057355642318725586\n",
            "MSE Loss is: 0.01687108352780342, h Loss is: 0.04100308567285538, L1 loss: 1.9007281064987183, Total Loss is: 0.0578741692006588\n",
            "MSE Loss is: 0.01652463898062706, h Loss is: 0.04135952889919281, L1 loss: 1.9010709524154663, Total Loss is: 0.05788416787981987\n",
            "MSE Loss is: 0.016249436885118484, h Loss is: 0.04170247167348862, L1 loss: 1.901422142982483, Total Loss is: 0.0579519085586071\n",
            "MSE Loss is: 0.01634962111711502, h Loss is: 0.042019180953502655, L1 loss: 1.9018274545669556, Total Loss is: 0.058368802070617676\n",
            "MSE Loss is: 0.01671566814184189, h Loss is: 0.04234101623296738, L1 loss: 1.9023106098175049, Total Loss is: 0.059056684374809265\n",
            "MSE Loss is: 0.016196005046367645, h Loss is: 0.04267401248216629, L1 loss: 1.9029535055160522, Total Loss is: 0.058870017528533936\n",
            "MSE Loss is: 0.01627149060368538, h Loss is: 0.04298924282193184, L1 loss: 1.9036130905151367, Total Loss is: 0.05926073342561722\n",
            "MSE Loss is: 0.016595758497714996, h Loss is: 0.04330155998468399, L1 loss: 1.9042495489120483, Total Loss is: 0.05989731848239899\n",
            "MSE Loss is: 0.016156025230884552, h Loss is: 0.04362796992063522, L1 loss: 1.9047611951828003, Total Loss is: 0.059783995151519775\n",
            "MSE Loss is: 0.01649853028357029, h Loss is: 0.04396294802427292, L1 loss: 1.9050453901290894, Total Loss is: 0.06046147644519806\n",
            "MSE Loss is: 0.016168823465704918, h Loss is: 0.04431530833244324, L1 loss: 1.9051929712295532, Total Loss is: 0.060484133660793304\n",
            "MSE Loss is: 0.016224458813667297, h Loss is: 0.04469398036599159, L1 loss: 1.9052734375, Total Loss is: 0.06091843917965889\n",
            "MSE Loss is: 0.016342680901288986, h Loss is: 0.045113615691661835, L1 loss: 1.90536367893219, Total Loss is: 0.06145629659295082\n",
            "MSE Loss is: 0.016471486538648605, h Loss is: 0.045529864728450775, L1 loss: 1.9053977727890015, Total Loss is: 0.06200135126709938\n",
            "MSE Loss is: 0.016507431864738464, h Loss is: 0.04595132917165756, L1 loss: 1.9054145812988281, Total Loss is: 0.06245876103639603\n",
            "MSE Loss is: 0.016426151618361473, h Loss is: 0.046364594250917435, L1 loss: 1.9054374694824219, Total Loss is: 0.06279074400663376\n",
            "MSE Loss is: 0.01617857813835144, h Loss is: 0.04675769433379173, L1 loss: 1.9054831266403198, Total Loss is: 0.06293627619743347\n",
            "MSE Loss is: 0.01611746847629547, h Loss is: 0.04712658002972603, L1 loss: 1.9056240320205688, Total Loss is: 0.0632440447807312\n",
            "MSE Loss is: 0.01627674512565136, h Loss is: 0.04750850424170494, L1 loss: 1.9057892560958862, Total Loss is: 0.06378524750471115\n",
            "MSE Loss is: 0.016321508213877678, h Loss is: 0.04787270352244377, L1 loss: 1.9060157537460327, Total Loss is: 0.0641942098736763\n",
            "MSE Loss is: 0.01640891656279564, h Loss is: 0.048243504017591476, L1 loss: 1.9063466787338257, Total Loss is: 0.06465242058038712\n",
            "MSE Loss is: 0.016239292919635773, h Loss is: 0.04859158396720886, L1 loss: 1.9067333936691284, Total Loss is: 0.06483087688684464\n",
            "MSE Loss is: 0.01622709259390831, h Loss is: 0.0489489883184433, L1 loss: 1.9069699048995972, Total Loss is: 0.0651760846376419\n",
            "MSE Loss is: 0.01646358147263527, h Loss is: 0.049312300980091095, L1 loss: 1.9071601629257202, Total Loss is: 0.06577588617801666\n",
            "MSE Loss is: 0.016045188531279564, h Loss is: 0.04968131706118584, L1 loss: 1.9072927236557007, Total Loss is: 0.06572650372982025\n",
            "MSE Loss is: 0.016122575849294662, h Loss is: 0.05008804053068161, L1 loss: 1.9072891473770142, Total Loss is: 0.06621061265468597\n",
            "MSE Loss is: 0.015924591571092606, h Loss is: 0.05047963559627533, L1 loss: 1.9073795080184937, Total Loss is: 0.06640422344207764\n",
            "MSE Loss is: 0.016635099425911903, h Loss is: 0.05085856467485428, L1 loss: 1.9074649810791016, Total Loss is: 0.06749366223812103\n",
            "MSE Loss is: 0.01563354767858982, h Loss is: 0.05126045644283295, L1 loss: 1.9074009656906128, Total Loss is: 0.06689400225877762\n",
            "MSE Loss is: 0.01583973504602909, h Loss is: 0.05167604982852936, L1 loss: 1.9071931838989258, Total Loss is: 0.0675157830119133\n",
            "New h_val is : tf.Tensor(0.13037968, shape=(), dtype=float32)\n",
            "Epoch: {} 6\n",
            "MSE Loss is: 0.015916820615530014, h Loss is: 0.08071111142635345, L1 loss: 1.9068691730499268, Total Loss is: 0.09662793576717377\n",
            "MSE Loss is: 0.01572795957326889, h Loss is: 0.0813615694642067, L1 loss: 1.9065666198730469, Total Loss is: 0.09708952903747559\n",
            "MSE Loss is: 0.015957061201334, h Loss is: 0.08203276246786118, L1 loss: 1.906280517578125, Total Loss is: 0.09798982739448547\n",
            "MSE Loss is: 0.01569230481982231, h Loss is: 0.08270739763975143, L1 loss: 1.9060662984848022, Total Loss is: 0.09839969873428345\n",
            "MSE Loss is: 0.016019020229578018, h Loss is: 0.08336671441793442, L1 loss: 1.905955195426941, Total Loss is: 0.09938573837280273\n",
            "MSE Loss is: 0.015788983553647995, h Loss is: 0.0840357095003128, L1 loss: 1.9057912826538086, Total Loss is: 0.0998246967792511\n",
            "MSE Loss is: 0.016299985349178314, h Loss is: 0.08458509296178818, L1 loss: 1.9057466983795166, Total Loss is: 0.10088507831096649\n",
            "MSE Loss is: 0.016417361795902252, h Loss is: 0.08509653061628342, L1 loss: 1.9057611227035522, Total Loss is: 0.10151389241218567\n",
            "MSE Loss is: 0.016232308000326157, h Loss is: 0.08560383319854736, L1 loss: 1.9057056903839111, Total Loss is: 0.10183614492416382\n",
            "MSE Loss is: 0.016700468957424164, h Loss is: 0.08610101044178009, L1 loss: 1.9055265188217163, Total Loss is: 0.10280147939920425\n",
            "MSE Loss is: 0.01592349261045456, h Loss is: 0.0866418406367302, L1 loss: 1.9053175449371338, Total Loss is: 0.10256533324718475\n",
            "MSE Loss is: 0.016199614852666855, h Loss is: 0.0872420072555542, L1 loss: 1.9049453735351562, Total Loss is: 0.10344162583351135\n",
            "MSE Loss is: 0.01607680134475231, h Loss is: 0.0878499448299408, L1 loss: 1.9045864343643188, Total Loss is: 0.10392674803733826\n",
            "MSE Loss is: 0.01578972116112709, h Loss is: 0.08856259286403656, L1 loss: 1.9042561054229736, Total Loss is: 0.10435231029987335\n",
            "MSE Loss is: 0.015641793608665466, h Loss is: 0.089368537068367, L1 loss: 1.9038623571395874, Total Loss is: 0.10501033067703247\n",
            "MSE Loss is: 0.015754520893096924, h Loss is: 0.09019415080547333, L1 loss: 1.9035522937774658, Total Loss is: 0.10594867169857025\n",
            "MSE Loss is: 0.016074854880571365, h Loss is: 0.09103348851203918, L1 loss: 1.9033994674682617, Total Loss is: 0.10710833966732025\n",
            "MSE Loss is: 0.016298232600092888, h Loss is: 0.09176775813102722, L1 loss: 1.9031637907028198, Total Loss is: 0.10806599259376526\n",
            "MSE Loss is: 0.016198713332414627, h Loss is: 0.09245350956916809, L1 loss: 1.9026809930801392, Total Loss is: 0.10865221917629242\n",
            "MSE Loss is: 0.01635536178946495, h Loss is: 0.09306104481220245, L1 loss: 1.902143120765686, Total Loss is: 0.1094164103269577\n",
            "MSE Loss is: 0.01604200154542923, h Loss is: 0.09365598857402802, L1 loss: 1.9015415906906128, Total Loss is: 0.10969799011945724\n",
            "MSE Loss is: 0.01575218327343464, h Loss is: 0.09421707689762115, L1 loss: 1.9009840488433838, Total Loss is: 0.10996925830841064\n",
            "MSE Loss is: 0.015860823914408684, h Loss is: 0.09472216665744781, L1 loss: 1.9005041122436523, Total Loss is: 0.11058299243450165\n",
            "MSE Loss is: 0.01620839163661003, h Loss is: 0.09525718539953232, L1 loss: 1.9004369974136353, Total Loss is: 0.11146557331085205\n",
            "MSE Loss is: 0.015709001570940018, h Loss is: 0.09583742916584015, L1 loss: 1.900557518005371, Total Loss is: 0.11154642701148987\n",
            "MSE Loss is: 0.015803087502717972, h Loss is: 0.0963958352804184, L1 loss: 1.9006885290145874, Total Loss is: 0.11219891905784607\n",
            "MSE Loss is: 0.016115941107273102, h Loss is: 0.09696850925683975, L1 loss: 1.9007568359375, Total Loss is: 0.11308445036411285\n",
            "MSE Loss is: 0.015713702887296677, h Loss is: 0.09758928418159485, L1 loss: 1.9006309509277344, Total Loss is: 0.11330299079418182\n",
            "MSE Loss is: 0.016056030988693237, h Loss is: 0.09823767095804214, L1 loss: 1.900213599205017, Total Loss is: 0.11429370194673538\n",
            "MSE Loss is: 0.015721041709184647, h Loss is: 0.09892765432596207, L1 loss: 1.8996094465255737, Total Loss is: 0.11464869976043701\n",
            "MSE Loss is: 0.01577727124094963, h Loss is: 0.09966786205768585, L1 loss: 1.8988834619522095, Total Loss is: 0.11544513702392578\n",
            "MSE Loss is: 0.015908854082226753, h Loss is: 0.10048822313547134, L1 loss: 1.898170828819275, Total Loss is: 0.11639707535505295\n",
            "MSE Loss is: 0.01600930094718933, h Loss is: 0.10127449035644531, L1 loss: 1.8974403142929077, Total Loss is: 0.11728379130363464\n",
            "MSE Loss is: 0.016072668135166168, h Loss is: 0.10203894972801208, L1 loss: 1.8967260122299194, Total Loss is: 0.11811161786317825\n",
            "MSE Loss is: 0.016004599630832672, h Loss is: 0.10275308787822723, L1 loss: 1.8960907459259033, Total Loss is: 0.1187576875090599\n",
            "MSE Loss is: 0.015758272260427475, h Loss is: 0.10339964181184769, L1 loss: 1.895580530166626, Total Loss is: 0.11915791034698486\n",
            "MSE Loss is: 0.015699366107583046, h Loss is: 0.10397530347108841, L1 loss: 1.8952442407608032, Total Loss is: 0.1196746677160263\n",
            "MSE Loss is: 0.015842154622077942, h Loss is: 0.104581817984581, L1 loss: 1.8949979543685913, Total Loss is: 0.12042397260665894\n",
            "MSE Loss is: 0.015877503901720047, h Loss is: 0.10515597462654114, L1 loss: 1.8948224782943726, Total Loss is: 0.12103347480297089\n",
            "MSE Loss is: 0.01597736030817032, h Loss is: 0.10576849430799484, L1 loss: 1.8947362899780273, Total Loss is: 0.12174585461616516\n",
            "MSE Loss is: 0.015824198722839355, h Loss is: 0.10635042190551758, L1 loss: 1.8946613073349, Total Loss is: 0.12217462062835693\n",
            "MSE Loss is: 0.01579313911497593, h Loss is: 0.10698051750659943, L1 loss: 1.8943397998809814, Total Loss is: 0.1227736547589302\n",
            "MSE Loss is: 0.016042131930589676, h Loss is: 0.10764448344707489, L1 loss: 1.893898844718933, Total Loss is: 0.12368661165237427\n",
            "MSE Loss is: 0.015633266419172287, h Loss is: 0.10833539813756943, L1 loss: 1.8933219909667969, Total Loss is: 0.12396866083145142\n",
            "MSE Loss is: 0.015699021518230438, h Loss is: 0.1091260090470314, L1 loss: 1.8926575183868408, Total Loss is: 0.12482503056526184\n",
            "MSE Loss is: 0.015530251897871494, h Loss is: 0.10988336056470871, L1 loss: 1.8921924829483032, Total Loss is: 0.12541361153125763\n",
            "MSE Loss is: 0.016237281262874603, h Loss is: 0.11060275137424469, L1 loss: 1.8918803930282593, Total Loss is: 0.1268400251865387\n",
            "MSE Loss is: 0.015249311923980713, h Loss is: 0.11135473847389221, L1 loss: 1.8914474248886108, Total Loss is: 0.12660405039787292\n",
            "MSE Loss is: 0.015458781272172928, h Loss is: 0.11211422085762024, L1 loss: 1.8908833265304565, Total Loss is: 0.12757299840450287\n",
            "New h_val is : tf.Tensor(0.1728301, shape=(), dtype=float32)\n",
            "Epoch: {} 7\n",
            "MSE Loss is: 0.015510130673646927, h Loss is: 0.16818638145923615, L1 loss: 1.8901909589767456, Total Loss is: 0.18369650840759277\n",
            "MSE Loss is: 0.015337502583861351, h Loss is: 0.1692838817834854, L1 loss: 1.8895740509033203, Total Loss is: 0.18462137877941132\n",
            "MSE Loss is: 0.015560467727482319, h Loss is: 0.17038844525814056, L1 loss: 1.8890129327774048, Total Loss is: 0.18594890832901\n",
            "MSE Loss is: 0.015317022800445557, h Loss is: 0.1714736521244049, L1 loss: 1.88858163356781, Total Loss is: 0.18679067492485046\n",
            "MSE Loss is: 0.015636509284377098, h Loss is: 0.17252373695373535, L1 loss: 1.8883278369903564, Total Loss is: 0.188160240650177\n",
            "MSE Loss is: 0.015408389270305634, h Loss is: 0.17359806597232819, L1 loss: 1.887999415397644, Total Loss is: 0.18900644779205322\n",
            "MSE Loss is: 0.015914496034383774, h Loss is: 0.1744268238544464, L1 loss: 1.8878523111343384, Total Loss is: 0.19034132361412048\n",
            "MSE Loss is: 0.016043448820710182, h Loss is: 0.17520225048065186, L1 loss: 1.8877884149551392, Total Loss is: 0.19124570488929749\n",
            "MSE Loss is: 0.01586271822452545, h Loss is: 0.1760132759809494, L1 loss: 1.887603759765625, Total Loss is: 0.19187599420547485\n",
            "MSE Loss is: 0.016327906399965286, h Loss is: 0.17684313654899597, L1 loss: 1.8872305154800415, Total Loss is: 0.19317103922367096\n",
            "MSE Loss is: 0.015581553801894188, h Loss is: 0.1778160184621811, L1 loss: 1.886781930923462, Total Loss is: 0.19339756667613983\n",
            "MSE Loss is: 0.015821212902665138, h Loss is: 0.17893797159194946, L1 loss: 1.8860504627227783, Total Loss is: 0.19475919008255005\n",
            "MSE Loss is: 0.01572192646563053, h Loss is: 0.18008512258529663, L1 loss: 1.8853752613067627, Total Loss is: 0.1958070546388626\n",
            "MSE Loss is: 0.015434592962265015, h Loss is: 0.18143919110298157, L1 loss: 1.8847054243087769, Total Loss is: 0.19687378406524658\n",
            "MSE Loss is: 0.015273220837116241, h Loss is: 0.18295034766197205, L1 loss: 1.8839126825332642, Total Loss is: 0.1982235610485077\n",
            "MSE Loss is: 0.01539372093975544, h Loss is: 0.184441477060318, L1 loss: 1.8832807540893555, Total Loss is: 0.19983519613742828\n",
            "MSE Loss is: 0.01572134718298912, h Loss is: 0.18590256571769714, L1 loss: 1.8829147815704346, Total Loss is: 0.20162391662597656\n",
            "MSE Loss is: 0.01596047356724739, h Loss is: 0.18707701563835144, L1 loss: 1.8825603723526, Total Loss is: 0.20303748548030853\n",
            "MSE Loss is: 0.015842031687498093, h Loss is: 0.1881086230278015, L1 loss: 1.8819046020507812, Total Loss is: 0.2039506584405899\n",
            "MSE Loss is: 0.01598593220114708, h Loss is: 0.18895980715751648, L1 loss: 1.8812538385391235, Total Loss is: 0.20494574308395386\n",
            "MSE Loss is: 0.01570277288556099, h Loss is: 0.18979868292808533, L1 loss: 1.8805675506591797, Total Loss is: 0.20550145208835602\n",
            "MSE Loss is: 0.015399029478430748, h Loss is: 0.19061630964279175, L1 loss: 1.8799949884414673, Total Loss is: 0.20601533353328705\n",
            "MSE Loss is: 0.015506472438573837, h Loss is: 0.19138048589229584, L1 loss: 1.8795900344848633, Total Loss is: 0.20688696205615997\n",
            "MSE Loss is: 0.015839938074350357, h Loss is: 0.19228751957416534, L1 loss: 1.879362940788269, Total Loss is: 0.2081274539232254\n",
            "MSE Loss is: 0.01535857655107975, h Loss is: 0.19334179162979126, L1 loss: 1.8793572187423706, Total Loss is: 0.20870037376880646\n",
            "MSE Loss is: 0.015476400032639503, h Loss is: 0.19438806176185608, L1 loss: 1.8793563842773438, Total Loss is: 0.20986446738243103\n",
            "MSE Loss is: 0.015774333849549294, h Loss is: 0.19547700881958008, L1 loss: 1.87921941280365, Total Loss is: 0.21125134825706482\n",
            "MSE Loss is: 0.015396163798868656, h Loss is: 0.19665011763572693, L1 loss: 1.8787845373153687, Total Loss is: 0.212046280503273\n",
            "MSE Loss is: 0.015735652297735214, h Loss is: 0.19784513115882874, L1 loss: 1.8779401779174805, Total Loss is: 0.21358078718185425\n",
            "MSE Loss is: 0.01539219543337822, h Loss is: 0.19908061623573303, L1 loss: 1.8768501281738281, Total Loss is: 0.21447281539440155\n",
            "MSE Loss is: 0.01544514112174511, h Loss is: 0.20035789906978607, L1 loss: 1.8756754398345947, Total Loss is: 0.21580304205417633\n",
            "MSE Loss is: 0.015588429756462574, h Loss is: 0.2017507553100586, L1 loss: 1.8745893239974976, Total Loss is: 0.2173391878604889\n",
            "MSE Loss is: 0.015672123059630394, h Loss is: 0.20301777124404907, L1 loss: 1.873594880104065, Total Loss is: 0.21868988871574402\n",
            "MSE Loss is: 0.015746410936117172, h Loss is: 0.20419608056545258, L1 loss: 1.8727295398712158, Total Loss is: 0.21994249522686005\n",
            "MSE Loss is: 0.015679171308875084, h Loss is: 0.20524801313877106, L1 loss: 1.8720448017120361, Total Loss is: 0.2209271788597107\n",
            "MSE Loss is: 0.01543319970369339, h Loss is: 0.20617249608039856, L1 loss: 1.8715428113937378, Total Loss is: 0.22160568833351135\n",
            "MSE Loss is: 0.015389803797006607, h Loss is: 0.2069786936044693, L1 loss: 1.8712610006332397, Total Loss is: 0.2223684936761856\n",
            "MSE Loss is: 0.01551932841539383, h Loss is: 0.20790180563926697, L1 loss: 1.870997667312622, Total Loss is: 0.2234211266040802\n",
            "MSE Loss is: 0.0155391376465559, h Loss is: 0.20881465077400208, L1 loss: 1.8707531690597534, Total Loss is: 0.22435379028320312\n",
            "MSE Loss is: 0.015651201829314232, h Loss is: 0.20986761152744293, L1 loss: 1.8705307245254517, Total Loss is: 0.22551880776882172\n",
            "MSE Loss is: 0.01551193930208683, h Loss is: 0.21090149879455566, L1 loss: 1.8702703714370728, Total Loss is: 0.22641344368457794\n",
            "MSE Loss is: 0.015460428781807423, h Loss is: 0.21205739676952362, L1 loss: 1.869646668434143, Total Loss is: 0.22751782834529877\n",
            "MSE Loss is: 0.015717647969722748, h Loss is: 0.2132810652256012, L1 loss: 1.868883490562439, Total Loss is: 0.22899872064590454\n",
            "MSE Loss is: 0.015310587361454964, h Loss is: 0.2145385891199112, L1 loss: 1.868006944656372, Total Loss is: 0.229849174618721\n",
            "MSE Loss is: 0.015385137870907784, h Loss is: 0.21597251296043396, L1 loss: 1.8669826984405518, Total Loss is: 0.2313576489686966\n",
            "MSE Loss is: 0.01523277536034584, h Loss is: 0.2172972410917282, L1 loss: 1.8662347793579102, Total Loss is: 0.23253001272678375\n",
            "MSE Loss is: 0.015919910743832588, h Loss is: 0.2184944450855255, L1 loss: 1.8656542301177979, Total Loss is: 0.23441435396671295\n",
            "MSE Loss is: 0.014951827004551888, h Loss is: 0.21970315277576447, L1 loss: 1.8649959564208984, Total Loss is: 0.2346549779176712\n",
            "MSE Loss is: 0.015166003257036209, h Loss is: 0.22088778018951416, L1 loss: 1.864249587059021, Total Loss is: 0.23605377972126007\n",
            "New h_val is : tf.Tensor(0.21902943, shape=(), dtype=float32)\n",
            "Epoch: {} 8\n",
            "MSE Loss is: 0.015191201120615005, h Loss is: 0.31979382038116455, L1 loss: 1.8633760213851929, Total Loss is: 0.33498501777648926\n",
            "MSE Loss is: 0.015029877424240112, h Loss is: 0.3214241862297058, L1 loss: 1.862578272819519, Total Loss is: 0.3364540636539459\n",
            "MSE Loss is: 0.015249043703079224, h Loss is: 0.32307079434394836, L1 loss: 1.8618147373199463, Total Loss is: 0.3383198380470276\n",
            "MSE Loss is: 0.01500590518116951, h Loss is: 0.32471445202827454, L1 loss: 1.861164927482605, Total Loss is: 0.33972036838531494\n",
            "MSE Loss is: 0.015333021059632301, h Loss is: 0.32636404037475586, L1 loss: 1.8606948852539062, Total Loss is: 0.3416970670223236\n",
            "MSE Loss is: 0.015107013285160065, h Loss is: 0.32811421155929565, L1 loss: 1.8601036071777344, Total Loss is: 0.3432212173938751\n",
            "MSE Loss is: 0.015601716004312038, h Loss is: 0.32943323254585266, L1 loss: 1.8596737384796143, Total Loss is: 0.34503495693206787\n",
            "MSE Loss is: 0.015739761292934418, h Loss is: 0.3306867182254791, L1 loss: 1.8593043088912964, Total Loss is: 0.34642648696899414\n",
            "MSE Loss is: 0.015566364862024784, h Loss is: 0.3320564031600952, L1 loss: 1.858801245689392, Total Loss is: 0.3476227819919586\n",
            "MSE Loss is: 0.016015715897083282, h Loss is: 0.33346524834632874, L1 loss: 1.8580780029296875, Total Loss is: 0.3494809567928314\n",
            "MSE Loss is: 0.015293537639081478, h Loss is: 0.3351634740829468, L1 loss: 1.857308268547058, Total Loss is: 0.35045701265335083\n",
            "MSE Loss is: 0.015506807714700699, h Loss is: 0.33711326122283936, L1 loss: 1.8562424182891846, Total Loss is: 0.35262006521224976\n",
            "MSE Loss is: 0.015422016382217407, h Loss is: 0.33905351161956787, L1 loss: 1.8552826642990112, Total Loss is: 0.3544755280017853\n",
            "MSE Loss is: 0.01514166034758091, h Loss is: 0.3413138687610626, L1 loss: 1.8543528318405151, Total Loss is: 0.356455534696579\n",
            "MSE Loss is: 0.014985155314207077, h Loss is: 0.34378382563591003, L1 loss: 1.8533061742782593, Total Loss is: 0.3587689697742462\n",
            "MSE Loss is: 0.015095990151166916, h Loss is: 0.34611883759498596, L1 loss: 1.8524467945098877, Total Loss is: 0.361214816570282\n",
            "MSE Loss is: 0.015426790341734886, h Loss is: 0.34834766387939453, L1 loss: 1.8519238233566284, Total Loss is: 0.36377444863319397\n",
            "MSE Loss is: 0.015677662566304207, h Loss is: 0.34998559951782227, L1 loss: 1.8514156341552734, Total Loss is: 0.3656632602214813\n",
            "MSE Loss is: 0.015539908781647682, h Loss is: 0.3513716161251068, L1 loss: 1.850581169128418, Total Loss is: 0.36691153049468994\n",
            "MSE Loss is: 0.015678944066166878, h Loss is: 0.3524797260761261, L1 loss: 1.8497475385665894, Total Loss is: 0.3681586682796478\n",
            "MSE Loss is: 0.01542163360863924, h Loss is: 0.3536646366119385, L1 loss: 1.8488606214523315, Total Loss is: 0.36908626556396484\n",
            "MSE Loss is: 0.015108780935406685, h Loss is: 0.3549453020095825, L1 loss: 1.8481006622314453, Total Loss is: 0.37005409598350525\n",
            "MSE Loss is: 0.015215270221233368, h Loss is: 0.35623687505722046, L1 loss: 1.8475137948989868, Total Loss is: 0.3714521527290344\n",
            "MSE Loss is: 0.015531228855252266, h Loss is: 0.3578932583332062, L1 loss: 1.8471187353134155, Total Loss is: 0.3734245002269745\n",
            "MSE Loss is: 0.01506921648979187, h Loss is: 0.3598499894142151, L1 loss: 1.8469265699386597, Total Loss is: 0.37491920590400696\n",
            "MSE Loss is: 0.015206903219223022, h Loss is: 0.36174359917640686, L1 loss: 1.8466898202896118, Total Loss is: 0.3769505023956299\n",
            "MSE Loss is: 0.015487528406083584, h Loss is: 0.3636337220668793, L1 loss: 1.8462575674057007, Total Loss is: 0.37912124395370483\n",
            "MSE Loss is: 0.015124435536563396, h Loss is: 0.365566611289978, L1 loss: 1.8454707860946655, Total Loss is: 0.3806910514831543\n",
            "MSE Loss is: 0.015468550845980644, h Loss is: 0.3674260377883911, L1 loss: 1.844246745109558, Total Loss is: 0.3828945755958557\n",
            "MSE Loss is: 0.015121162869036198, h Loss is: 0.369272917509079, L1 loss: 1.8428114652633667, Total Loss is: 0.3843940794467926\n",
            "MSE Loss is: 0.015173767693340778, h Loss is: 0.3711327314376831, L1 loss: 1.8413623571395874, Total Loss is: 0.386306494474411\n",
            "MSE Loss is: 0.015320083126425743, h Loss is: 0.3731933832168579, L1 loss: 1.8401192426681519, Total Loss is: 0.3885134756565094\n",
            "MSE Loss is: 0.015390638262033463, h Loss is: 0.375022292137146, L1 loss: 1.83907151222229, Total Loss is: 0.39041292667388916\n",
            "MSE Loss is: 0.01546694990247488, h Loss is: 0.3767040967941284, L1 loss: 1.8382145166397095, Total Loss is: 0.39217105507850647\n",
            "MSE Loss is: 0.01539117842912674, h Loss is: 0.3781976103782654, L1 loss: 1.837563395500183, Total Loss is: 0.3935887813568115\n",
            "MSE Loss is: 0.015152033418416977, h Loss is: 0.3795255422592163, L1 loss: 1.8370863199234009, Total Loss is: 0.3946775794029236\n",
            "MSE Loss is: 0.015135139226913452, h Loss is: 0.3807058036327362, L1 loss: 1.8367680311203003, Total Loss is: 0.39584094285964966\n",
            "MSE Loss is: 0.015254711732268333, h Loss is: 0.38218653202056885, L1 loss: 1.8363676071166992, Total Loss is: 0.39744123816490173\n",
            "MSE Loss is: 0.015256223268806934, h Loss is: 0.3836994767189026, L1 loss: 1.835898995399475, Total Loss is: 0.39895570278167725\n",
            "MSE Loss is: 0.01537879928946495, h Loss is: 0.38551241159439087, L1 loss: 1.8353769779205322, Total Loss is: 0.4008912146091461\n",
            "MSE Loss is: 0.015241253189742565, h Loss is: 0.3872765302658081, L1 loss: 1.8347549438476562, Total Loss is: 0.40251779556274414\n",
            "MSE Loss is: 0.01517249085009098, h Loss is: 0.3892282247543335, L1 loss: 1.8337596654891968, Total Loss is: 0.40440070629119873\n",
            "MSE Loss is: 0.015439305454492569, h Loss is: 0.391235888004303, L1 loss: 1.8326717615127563, Total Loss is: 0.40667518973350525\n",
            "MSE Loss is: 0.015036894008517265, h Loss is: 0.3932209610939026, L1 loss: 1.8315414190292358, Total Loss is: 0.4082578420639038\n",
            "MSE Loss is: 0.015129493549466133, h Loss is: 0.3954804241657257, L1 loss: 1.8304195404052734, Total Loss is: 0.4106099307537079\n",
            "MSE Loss is: 0.0149855837225914, h Loss is: 0.39747166633605957, L1 loss: 1.8297127485275269, Total Loss is: 0.41245725750923157\n",
            "MSE Loss is: 0.015650592744350433, h Loss is: 0.39918357133865356, L1 loss: 1.8292614221572876, Total Loss is: 0.4148341715335846\n",
            "MSE Loss is: 0.014699166640639305, h Loss is: 0.40088462829589844, L1 loss: 1.8287619352340698, Total Loss is: 0.4155837893486023\n",
            "MSE Loss is: 0.014928224496543407, h Loss is: 0.4025512933731079, L1 loss: 1.8281402587890625, Total Loss is: 0.4174795150756836\n",
            "New h_val is : tf.Tensor(0.26813698, shape=(), dtype=float32)\n",
            "Epoch: {} 9\n",
            "MSE Loss is: 0.014920325949788094, h Loss is: 0.5653734803199768, L1 loss: 1.8273426294326782, Total Loss is: 0.5802938342094421\n",
            "MSE Loss is: 0.014773081988096237, h Loss is: 0.5677186846733093, L1 loss: 1.826499581336975, Total Loss is: 0.5824917554855347\n",
            "MSE Loss is: 0.014987077564001083, h Loss is: 0.5701724290847778, L1 loss: 1.8255983591079712, Total Loss is: 0.5851594805717468\n",
            "MSE Loss is: 0.01473676785826683, h Loss is: 0.5727131962776184, L1 loss: 1.8247644901275635, Total Loss is: 0.5874499678611755\n",
            "MSE Loss is: 0.015074525028467178, h Loss is: 0.5753734111785889, L1 loss: 1.8241204023361206, Total Loss is: 0.5904479622840881\n",
            "MSE Loss is: 0.014861948788166046, h Loss is: 0.5782532691955566, L1 loss: 1.8233757019042969, Total Loss is: 0.5931152105331421\n",
            "MSE Loss is: 0.01533483900129795, h Loss is: 0.5803247094154358, L1 loss: 1.8227695226669312, Total Loss is: 0.5956595540046692\n",
            "MSE Loss is: 0.015481601469218731, h Loss is: 0.5822194218635559, L1 loss: 1.8222416639328003, Total Loss is: 0.5977010130882263\n",
            "MSE Loss is: 0.015310895629227161, h Loss is: 0.5842846632003784, L1 loss: 1.8216450214385986, Total Loss is: 0.5995955467224121\n",
            "MSE Loss is: 0.01574629917740822, h Loss is: 0.5863376259803772, L1 loss: 1.8208364248275757, Total Loss is: 0.6020839214324951\n",
            "MSE Loss is: 0.01504499465227127, h Loss is: 0.5888798236846924, L1 loss: 1.8200653791427612, Total Loss is: 0.6039248108863831\n",
            "MSE Loss is: 0.015234531834721565, h Loss is: 0.5918052196502686, L1 loss: 1.8190096616744995, Total Loss is: 0.607039749622345\n",
            "MSE Loss is: 0.015172520652413368, h Loss is: 0.594658613204956, L1 loss: 1.8180774450302124, Total Loss is: 0.6098311543464661\n",
            "MSE Loss is: 0.014901014044880867, h Loss is: 0.5980389714241028, L1 loss: 1.8171542882919312, Total Loss is: 0.6129400134086609\n",
            "MSE Loss is: 0.014744684100151062, h Loss is: 0.6017513275146484, L1 loss: 1.8161001205444336, Total Loss is: 0.6164960265159607\n",
            "MSE Loss is: 0.014833441935479641, h Loss is: 0.6051706075668335, L1 loss: 1.8151549100875854, Total Loss is: 0.6200040578842163\n",
            "MSE Loss is: 0.015167487785220146, h Loss is: 0.6084339022636414, L1 loss: 1.814586877822876, Total Loss is: 0.6236013770103455\n",
            "MSE Loss is: 0.015439800918102264, h Loss is: 0.6106433272361755, L1 loss: 1.8139698505401611, Total Loss is: 0.6260831356048584\n",
            "MSE Loss is: 0.015283079817891121, h Loss is: 0.6124765276908875, L1 loss: 1.8130401372909546, Total Loss is: 0.6277596354484558\n",
            "MSE Loss is: 0.015417298302054405, h Loss is: 0.6139094829559326, L1 loss: 1.8121166229248047, Total Loss is: 0.6293267607688904\n",
            "MSE Loss is: 0.015185337513685226, h Loss is: 0.6155935525894165, L1 loss: 1.811142921447754, Total Loss is: 0.6307789087295532\n",
            "MSE Loss is: 0.01486491784453392, h Loss is: 0.6175745129585266, L1 loss: 1.810323715209961, Total Loss is: 0.6324394345283508\n",
            "MSE Loss is: 0.014967309311032295, h Loss is: 0.6196478009223938, L1 loss: 1.8096882104873657, Total Loss is: 0.6346151232719421\n",
            "MSE Loss is: 0.0152653269469738, h Loss is: 0.6224123239517212, L1 loss: 1.8092974424362183, Total Loss is: 0.6376776695251465\n",
            "MSE Loss is: 0.014822461642324924, h Loss is: 0.6256304383277893, L1 loss: 1.8090778589248657, Total Loss is: 0.6404529213905334\n",
            "MSE Loss is: 0.014978825114667416, h Loss is: 0.6285926103591919, L1 loss: 1.8087787628173828, Total Loss is: 0.6435714364051819\n",
            "MSE Loss is: 0.01524418219923973, h Loss is: 0.6313893795013428, L1 loss: 1.8082250356674194, Total Loss is: 0.6466335654258728\n",
            "MSE Loss is: 0.014892123639583588, h Loss is: 0.6341068148612976, L1 loss: 1.807247519493103, Total Loss is: 0.6489989161491394\n",
            "MSE Loss is: 0.015244646929204464, h Loss is: 0.6365969777107239, L1 loss: 1.805821418762207, Total Loss is: 0.6518416404724121\n",
            "MSE Loss is: 0.014894476160407066, h Loss is: 0.6390494704246521, L1 loss: 1.8042871952056885, Total Loss is: 0.6539439558982849\n",
            "MSE Loss is: 0.014943232759833336, h Loss is: 0.6415440440177917, L1 loss: 1.8028236627578735, Total Loss is: 0.6564872860908508\n",
            "MSE Loss is: 0.015088552609086037, h Loss is: 0.6444830298423767, L1 loss: 1.8016670942306519, Total Loss is: 0.6595715880393982\n",
            "MSE Loss is: 0.01514771580696106, h Loss is: 0.6470789313316345, L1 loss: 1.800762414932251, Total Loss is: 0.662226676940918\n",
            "MSE Loss is: 0.015225963667035103, h Loss is: 0.6494626998901367, L1 loss: 1.8000411987304688, Total Loss is: 0.6646886467933655\n",
            "MSE Loss is: 0.015144402161240578, h Loss is: 0.6515465974807739, L1 loss: 1.7994846105575562, Total Loss is: 0.66669100522995\n",
            "MSE Loss is: 0.014913734048604965, h Loss is: 0.6533854603767395, L1 loss: 1.7990344762802124, Total Loss is: 0.6682991981506348\n",
            "MSE Loss is: 0.014918719418346882, h Loss is: 0.6549941301345825, L1 loss: 1.7987778186798096, Total Loss is: 0.6699128746986389\n",
            "MSE Loss is: 0.01503254845738411, h Loss is: 0.6571837067604065, L1 loss: 1.7983601093292236, Total Loss is: 0.6722162365913391\n",
            "MSE Loss is: 0.01501532644033432, h Loss is: 0.6594411134719849, L1 loss: 1.797716498374939, Total Loss is: 0.6744564175605774\n",
            "MSE Loss is: 0.015144586563110352, h Loss is: 0.6622435450553894, L1 loss: 1.7969709634780884, Total Loss is: 0.6773881316184998\n",
            "MSE Loss is: 0.015004140324890614, h Loss is: 0.6649231314659119, L1 loss: 1.7961273193359375, Total Loss is: 0.6799272894859314\n",
            "MSE Loss is: 0.01492452621459961, h Loss is: 0.6678672432899475, L1 loss: 1.7948442697525024, Total Loss is: 0.6827917695045471\n",
            "MSE Loss is: 0.015203832648694515, h Loss is: 0.6708121299743652, L1 loss: 1.7935307025909424, Total Loss is: 0.6860159635543823\n",
            "MSE Loss is: 0.014807179570198059, h Loss is: 0.6736053228378296, L1 loss: 1.792236566543579, Total Loss is: 0.6884124875068665\n",
            "MSE Loss is: 0.014909633435308933, h Loss is: 0.6768343448638916, L1 loss: 1.7910774946212769, Total Loss is: 0.6917439699172974\n",
            "MSE Loss is: 0.014770294539630413, h Loss is: 0.6795441508293152, L1 loss: 1.7905168533325195, Total Loss is: 0.6943144202232361\n",
            "MSE Loss is: 0.015419642440974712, h Loss is: 0.6817417144775391, L1 loss: 1.7903006076812744, Total Loss is: 0.6971613764762878\n",
            "MSE Loss is: 0.014482390135526657, h Loss is: 0.683925449848175, L1 loss: 1.7899370193481445, Total Loss is: 0.6984078288078308\n",
            "MSE Loss is: 0.014731121249496937, h Loss is: 0.6861215233802795, L1 loss: 1.7893552780151367, Total Loss is: 0.700852632522583\n",
            "New h_val is : tf.Tensor(0.31843472, shape=(), dtype=float32)\n",
            "Epoch: {} 10\n",
            "MSE Loss is: 0.014691147953271866, h Loss is: 0.9384545087814331, L1 loss: 1.7884520292282104, Total Loss is: 0.9531456828117371\n",
            "MSE Loss is: 0.014555774629116058, h Loss is: 0.9417538642883301, L1 loss: 1.787401795387268, Total Loss is: 0.9563096165657043\n",
            "MSE Loss is: 0.014763684943318367, h Loss is: 0.9453444480895996, L1 loss: 1.7861648797988892, Total Loss is: 0.9601081609725952\n",
            "MSE Loss is: 0.014508364722132683, h Loss is: 0.949172854423523, L1 loss: 1.7850620746612549, Total Loss is: 0.9636812210083008\n",
            "MSE Loss is: 0.014849754981696606, h Loss is: 0.9532809853553772, L1 loss: 1.7843563556671143, Total Loss is: 0.9681307673454285\n",
            "MSE Loss is: 0.01466052420437336, h Loss is: 0.9576807618141174, L1 loss: 1.7837570905685425, Total Loss is: 0.9723412990570068\n",
            "MSE Loss is: 0.015104785561561584, h Loss is: 0.9605222940444946, L1 loss: 1.783433198928833, Total Loss is: 0.975627064704895\n",
            "MSE Loss is: 0.015261795371770859, h Loss is: 0.9628668427467346, L1 loss: 1.7831960916519165, Total Loss is: 0.9781286120414734\n",
            "MSE Loss is: 0.015093070454895496, h Loss is: 0.9654085636138916, L1 loss: 1.7828781604766846, Total Loss is: 0.980501651763916\n",
            "MSE Loss is: 0.015515444800257683, h Loss is: 0.9678400754928589, L1 loss: 1.782178521156311, Total Loss is: 0.9833555221557617\n",
            "MSE Loss is: 0.014838127419352531, h Loss is: 0.9711712002754211, L1 loss: 1.7815227508544922, Total Loss is: 0.9860092997550964\n",
            "MSE Loss is: 0.014996284618973732, h Loss is: 0.97520911693573, L1 loss: 1.7803808450698853, Total Loss is: 0.9902054071426392\n",
            "MSE Loss is: 0.014964457601308823, h Loss is: 0.979202389717102, L1 loss: 1.779441475868225, Total Loss is: 0.9941668510437012\n",
            "MSE Loss is: 0.014698543585836887, h Loss is: 0.9841431379318237, L1 loss: 1.7784557342529297, Total Loss is: 0.9988417029380798\n",
            "MSE Loss is: 0.014538964256644249, h Loss is: 0.9896353483200073, L1 loss: 1.7772330045700073, Total Loss is: 1.0041743516921997\n",
            "MSE Loss is: 0.014607485383749008, h Loss is: 0.9945532083511353, L1 loss: 1.7761805057525635, Total Loss is: 1.0091606378555298\n",
            "MSE Loss is: 0.014948496595025063, h Loss is: 0.9991647005081177, L1 loss: 1.7757962942123413, Total Loss is: 1.014113187789917\n",
            "MSE Loss is: 0.015244086273014545, h Loss is: 1.0018815994262695, L1 loss: 1.7754381895065308, Total Loss is: 1.0171257257461548\n",
            "MSE Loss is: 0.015066285617649555, h Loss is: 1.00394606590271, L1 loss: 1.7746905088424683, Total Loss is: 1.0190123319625854\n",
            "MSE Loss is: 0.015192302875220776, h Loss is: 1.0053861141204834, L1 loss: 1.7739337682724, Total Loss is: 1.020578384399414\n",
            "MSE Loss is: 0.014983058907091618, h Loss is: 1.0073652267456055, L1 loss: 1.7730190753936768, Total Loss is: 1.0223482847213745\n",
            "MSE Loss is: 0.014656662940979004, h Loss is: 1.0100481510162354, L1 loss: 1.7723249197006226, Total Loss is: 1.0247048139572144\n",
            "MSE Loss is: 0.014755692332983017, h Loss is: 1.0130385160446167, L1 loss: 1.7718547582626343, Total Loss is: 1.0277942419052124\n",
            "MSE Loss is: 0.015040192753076553, h Loss is: 1.0173100233078003, L1 loss: 1.7717117071151733, Total Loss is: 1.0323501825332642\n",
            "MSE Loss is: 0.014612200669944286, h Loss is: 1.0222723484039307, L1 loss: 1.771727442741394, Total Loss is: 1.0368845462799072\n",
            "MSE Loss is: 0.014784076251089573, h Loss is: 1.0266101360321045, L1 loss: 1.7715977430343628, Total Loss is: 1.0413942337036133\n",
            "MSE Loss is: 0.015037491917610168, h Loss is: 1.0304491519927979, L1 loss: 1.7711702585220337, Total Loss is: 1.0454866886138916\n",
            "MSE Loss is: 0.014695672318339348, h Loss is: 1.0339323282241821, L1 loss: 1.7702213525772095, Total Loss is: 1.0486279726028442\n",
            "MSE Loss is: 0.015056701377034187, h Loss is: 1.036901831626892, L1 loss: 1.7687162160873413, Total Loss is: 1.0519585609436035\n",
            "MSE Loss is: 0.014700050465762615, h Loss is: 1.039798617362976, L1 loss: 1.7670978307724, Total Loss is: 1.0544986724853516\n",
            "MSE Loss is: 0.01474527083337307, h Loss is: 1.0428138971328735, L1 loss: 1.765607476234436, Total Loss is: 1.0575591325759888\n",
            "MSE Loss is: 0.01488858088850975, h Loss is: 1.0467193126678467, L1 loss: 1.7646416425704956, Total Loss is: 1.061607837677002\n",
            "MSE Loss is: 0.01493795309215784, h Loss is: 1.050150752067566, L1 loss: 1.764046549797058, Total Loss is: 1.0650887489318848\n",
            "MSE Loss is: 0.015018513426184654, h Loss is: 1.0532761812210083, L1 loss: 1.76364266872406, Total Loss is: 1.068294644355774\n",
            "MSE Loss is: 0.014932580292224884, h Loss is: 1.05592942237854, L1 loss: 1.7633682489395142, Total Loss is: 1.070862054824829\n",
            "MSE Loss is: 0.014710522256791592, h Loss is: 1.0582362413406372, L1 loss: 1.763253092765808, Total Loss is: 1.0729467868804932\n",
            "MSE Loss is: 0.014732484705746174, h Loss is: 1.0601962804794312, L1 loss: 1.7631291151046753, Total Loss is: 1.0749287605285645\n",
            "MSE Loss is: 0.014843033626675606, h Loss is: 1.0632011890411377, L1 loss: 1.7626848220825195, Total Loss is: 1.0780441761016846\n",
            "MSE Loss is: 0.014806238003075123, h Loss is: 1.066338300704956, L1 loss: 1.7619781494140625, Total Loss is: 1.0811445713043213\n",
            "MSE Loss is: 0.014942123554646969, h Loss is: 1.0703959465026855, L1 loss: 1.7612656354904175, Total Loss is: 1.0853381156921387\n",
            "MSE Loss is: 0.014799227006733418, h Loss is: 1.0741536617279053, L1 loss: 1.7605164051055908, Total Loss is: 1.088952898979187\n",
            "MSE Loss is: 0.014711549505591393, h Loss is: 1.0782111883163452, L1 loss: 1.7593170404434204, Total Loss is: 1.0929226875305176\n",
            "MSE Loss is: 0.01500327792018652, h Loss is: 1.082076072692871, L1 loss: 1.7581161260604858, Total Loss is: 1.0970793962478638\n",
            "MSE Loss is: 0.014613721519708633, h Loss is: 1.0855143070220947, L1 loss: 1.7569864988327026, Total Loss is: 1.1001280546188354\n",
            "MSE Loss is: 0.014717252925038338, h Loss is: 1.0896366834640503, L1 loss: 1.7560396194458008, Total Loss is: 1.104353904724121\n",
            "MSE Loss is: 0.014581030234694481, h Loss is: 1.0929001569747925, L1 loss: 1.755820631980896, Total Loss is: 1.1074812412261963\n",
            "MSE Loss is: 0.0152202770113945, h Loss is: 1.095386266708374, L1 loss: 1.7559689283370972, Total Loss is: 1.1106065511703491\n",
            "MSE Loss is: 0.014296552166342735, h Loss is: 1.0979621410369873, L1 loss: 1.7557562589645386, Total Loss is: 1.1122586727142334\n",
            "MSE Loss is: 0.014565935358405113, h Loss is: 1.100757360458374, L1 loss: 1.7551838159561157, Total Loss is: 1.1153233051300049\n",
            "New h_val is : tf.Tensor(0.36746073, shape=(), dtype=float32)\n",
            "Epoch: {} 11\n",
            "MSE Loss is: 0.014497434720396996, h Loss is: 1.4701755046844482, L1 loss: 1.7540619373321533, Total Loss is: 1.4846729040145874\n",
            "MSE Loss is: 0.014369400218129158, h Loss is: 1.4747858047485352, L1 loss: 1.752809762954712, Total Loss is: 1.4891551733016968\n",
            "MSE Loss is: 0.014570480212569237, h Loss is: 1.4798909425735474, L1 loss: 1.7514402866363525, Total Loss is: 1.4944614171981812\n",
            "MSE Loss is: 0.014314483851194382, h Loss is: 1.48532235622406, L1 loss: 1.7503492832183838, Total Loss is: 1.4996368885040283\n",
            "MSE Loss is: 0.01465719286352396, h Loss is: 1.4911046028137207, L1 loss: 1.749949336051941, Total Loss is: 1.505761742591858\n",
            "MSE Loss is: 0.01449156366288662, h Loss is: 1.4970496892929077, L1 loss: 1.7494852542877197, Total Loss is: 1.5115412473678589\n",
            "MSE Loss is: 0.014905241318047047, h Loss is: 1.5000978708267212, L1 loss: 1.749389886856079, Total Loss is: 1.5150030851364136\n",
            "MSE Loss is: 0.015073845162987709, h Loss is: 1.5021049976348877, L1 loss: 1.7493246793746948, Total Loss is: 1.5171788930892944\n",
            "MSE Loss is: 0.01490767952054739, h Loss is: 1.5044859647750854, L1 loss: 1.7490509748458862, Total Loss is: 1.5193936824798584\n",
            "MSE Loss is: 0.015317241661250591, h Loss is: 1.5068460702896118, L1 loss: 1.7482271194458008, Total Loss is: 1.5221632719039917\n",
            "MSE Loss is: 0.014665613882243633, h Loss is: 1.5110809803009033, L1 loss: 1.7473715543746948, Total Loss is: 1.5257465839385986\n",
            "MSE Loss is: 0.014790878631174564, h Loss is: 1.516746997833252, L1 loss: 1.7459267377853394, Total Loss is: 1.5315378904342651\n",
            "MSE Loss is: 0.014786633662879467, h Loss is: 1.5225374698638916, L1 loss: 1.7447865009307861, Total Loss is: 1.537324070930481\n",
            "MSE Loss is: 0.014522826299071312, h Loss is: 1.5298086404800415, L1 loss: 1.743677020072937, Total Loss is: 1.544331431388855\n",
            "MSE Loss is: 0.014360398054122925, h Loss is: 1.537693977355957, L1 loss: 1.7424558401107788, Total Loss is: 1.5520544052124023\n",
            "MSE Loss is: 0.014416723512113094, h Loss is: 1.5441876649856567, L1 loss: 1.7417181730270386, Total Loss is: 1.55860435962677\n",
            "MSE Loss is: 0.014765841886401176, h Loss is: 1.549849510192871, L1 loss: 1.7416887283325195, Total Loss is: 1.5646153688430786\n",
            "MSE Loss is: 0.015085184946656227, h Loss is: 1.5521564483642578, L1 loss: 1.7416332960128784, Total Loss is: 1.5672416687011719\n",
            "MSE Loss is: 0.014879878610372543, h Loss is: 1.553425669670105, L1 loss: 1.7409931421279907, Total Loss is: 1.568305492401123\n",
            "MSE Loss is: 0.01500167790800333, h Loss is: 1.554002046585083, L1 loss: 1.7402164936065674, Total Loss is: 1.569003701210022\n",
            "MSE Loss is: 0.014810662716627121, h Loss is: 1.5559451580047607, L1 loss: 1.7392187118530273, Total Loss is: 1.5707558393478394\n",
            "MSE Loss is: 0.014481877908110619, h Loss is: 1.5596299171447754, L1 loss: 1.7384929656982422, Total Loss is: 1.574111819267273\n",
            "MSE Loss is: 0.014576809480786324, h Loss is: 1.5642285346984863, L1 loss: 1.7380592823028564, Total Loss is: 1.5788053274154663\n",
            "MSE Loss is: 0.01485002227127552, h Loss is: 1.5710490942001343, L1 loss: 1.7380017042160034, Total Loss is: 1.5858991146087646\n",
            "MSE Loss is: 0.014433495700359344, h Loss is: 1.5786659717559814, L1 loss: 1.7381565570831299, Total Loss is: 1.5930994749069214\n",
            "MSE Loss is: 0.014617388136684895, h Loss is: 1.5846776962280273, L1 loss: 1.738133430480957, Total Loss is: 1.5992951393127441\n",
            "MSE Loss is: 0.01486421562731266, h Loss is: 1.5892235040664673, L1 loss: 1.737758994102478, Total Loss is: 1.6040877103805542\n",
            "MSE Loss is: 0.01453438214957714, h Loss is: 1.5926655530929565, L1 loss: 1.7368263006210327, Total Loss is: 1.6071999073028564\n",
            "MSE Loss is: 0.01490186806768179, h Loss is: 1.5950673818588257, L1 loss: 1.7353298664093018, Total Loss is: 1.6099692583084106\n",
            "MSE Loss is: 0.014534582383930683, h Loss is: 1.5975264310836792, L1 loss: 1.7337963581085205, Total Loss is: 1.6120610237121582\n",
            "MSE Loss is: 0.014578813686966896, h Loss is: 1.6005594730377197, L1 loss: 1.7324583530426025, Total Loss is: 1.615138292312622\n",
            "MSE Loss is: 0.014714423567056656, h Loss is: 1.6055576801300049, L1 loss: 1.731745958328247, Total Loss is: 1.620272159576416\n",
            "MSE Loss is: 0.014759326353669167, h Loss is: 1.610182762145996, L1 loss: 1.7314321994781494, Total Loss is: 1.6249420642852783\n",
            "MSE Loss is: 0.014842580072581768, h Loss is: 1.6144864559173584, L1 loss: 1.7312294244766235, Total Loss is: 1.6293290853500366\n",
            "MSE Loss is: 0.014751521870493889, h Loss is: 1.6179872751235962, L1 loss: 1.731039047241211, Total Loss is: 1.6327388286590576\n",
            "MSE Loss is: 0.014539135619997978, h Loss is: 1.620832085609436, L1 loss: 1.7309341430664062, Total Loss is: 1.635371208190918\n",
            "MSE Loss is: 0.014571952633559704, h Loss is: 1.622902750968933, L1 loss: 1.730756402015686, Total Loss is: 1.6374746561050415\n",
            "MSE Loss is: 0.014679040759801865, h Loss is: 1.626513957977295, L1 loss: 1.7302360534667969, Total Loss is: 1.6411930322647095\n",
            "MSE Loss is: 0.014623485505580902, h Loss is: 1.6301790475845337, L1 loss: 1.7295137643814087, Total Loss is: 1.6448025703430176\n",
            "MSE Loss is: 0.014767477288842201, h Loss is: 1.635280728340149, L1 loss: 1.7288707494735718, Total Loss is: 1.6500482559204102\n",
            "MSE Loss is: 0.014624955132603645, h Loss is: 1.6398226022720337, L1 loss: 1.7283306121826172, Total Loss is: 1.6544475555419922\n",
            "MSE Loss is: 0.014528716914355755, h Loss is: 1.6447689533233643, L1 loss: 1.727394938468933, Total Loss is: 1.6592977046966553\n",
            "MSE Loss is: 0.014833807945251465, h Loss is: 1.6493161916732788, L1 loss: 1.7265779972076416, Total Loss is: 1.6641499996185303\n",
            "MSE Loss is: 0.014447548426687717, h Loss is: 1.6531360149383545, L1 loss: 1.7258771657943726, Total Loss is: 1.6675835847854614\n",
            "MSE Loss is: 0.014546504244208336, h Loss is: 1.6581581830978394, L1 loss: 1.7252922058105469, Total Loss is: 1.6727046966552734\n",
            "MSE Loss is: 0.014415020123124123, h Loss is: 1.66194486618042, L1 loss: 1.7254351377487183, Total Loss is: 1.6763598918914795\n",
            "MSE Loss is: 0.01505044475197792, h Loss is: 1.6646432876586914, L1 loss: 1.725868582725525, Total Loss is: 1.6796936988830566\n",
            "MSE Loss is: 0.014138824306428432, h Loss is: 1.6675812005996704, L1 loss: 1.7257784605026245, Total Loss is: 1.6817200183868408\n",
            "MSE Loss is: 0.014424414373934269, h Loss is: 1.671006441116333, L1 loss: 1.725249171257019, Total Loss is: 1.685430884361267\n",
            "New h_val is : tf.Tensor(0.41272545, shape=(), dtype=float32)\n",
            "Epoch: {} 12\n",
            "MSE Loss is: 0.014330209232866764, h Loss is: 2.183391809463501, L1 loss: 1.7240993976593018, Total Loss is: 2.1977219581604004\n",
            "MSE Loss is: 0.014205466024577618, h Loss is: 2.1892876625061035, L1 loss: 1.722861886024475, Total Loss is: 2.203493118286133\n",
            "MSE Loss is: 0.014403363689780235, h Loss is: 2.195753812789917, L1 loss: 1.7216373682022095, Total Loss is: 2.2101571559906006\n",
            "MSE Loss is: 0.014153842814266682, h Loss is: 2.2024872303009033, L1 loss: 1.7208492755889893, Total Loss is: 2.2166411876678467\n",
            "MSE Loss is: 0.014495154842734337, h Loss is: 2.209606647491455, L1 loss: 1.7208431959152222, Total Loss is: 2.2241017818450928\n",
            "MSE Loss is: 0.014344338327646255, h Loss is: 2.216651201248169, L1 loss: 1.72075617313385, Total Loss is: 2.2309956550598145\n",
            "MSE Loss is: 0.01473231427371502, h Loss is: 2.218982696533203, L1 loss: 1.7210947275161743, Total Loss is: 2.233715057373047\n",
            "MSE Loss is: 0.014910604804754257, h Loss is: 2.2197139263153076, L1 loss: 1.7213786840438843, Total Loss is: 2.2346246242523193\n",
            "MSE Loss is: 0.014752394519746304, h Loss is: 2.221477508544922, L1 loss: 1.7212833166122437, Total Loss is: 2.23622989654541\n",
            "MSE Loss is: 0.015147444792091846, h Loss is: 2.2236971855163574, L1 loss: 1.7205063104629517, Total Loss is: 2.238844633102417\n",
            "MSE Loss is: 0.014518670737743378, h Loss is: 2.2294609546661377, L1 loss: 1.7196582555770874, Total Loss is: 2.2439796924591064\n",
            "MSE Loss is: 0.014616727828979492, h Loss is: 2.2376410961151123, L1 loss: 1.718213677406311, Total Loss is: 2.252257823944092\n",
            "MSE Loss is: 0.014626363292336464, h Loss is: 2.245863199234009, L1 loss: 1.7172573804855347, Total Loss is: 2.2604894638061523\n",
            "MSE Loss is: 0.014368066564202309, h Loss is: 2.255772113800049, L1 loss: 1.716391921043396, Total Loss is: 2.2701401710510254\n",
            "MSE Loss is: 0.014205066487193108, h Loss is: 2.265739917755127, L1 loss: 1.715383768081665, Total Loss is: 2.279944896697998\n",
            "MSE Loss is: 0.014255993068218231, h Loss is: 2.272698402404785, L1 loss: 1.7147644758224487, Total Loss is: 2.286954402923584\n",
            "MSE Loss is: 0.014606419019401073, h Loss is: 2.2780301570892334, L1 loss: 1.7150343656539917, Total Loss is: 2.2926366329193115\n",
            "MSE Loss is: 0.014948162250220776, h Loss is: 2.278230905532837, L1 loss: 1.715361475944519, Total Loss is: 2.2931790351867676\n",
            "MSE Loss is: 0.014716072008013725, h Loss is: 2.277538299560547, L1 loss: 1.7150013446807861, Total Loss is: 2.2922544479370117\n",
            "MSE Loss is: 0.01484635565429926, h Loss is: 2.2768800258636475, L1 loss: 1.7144383192062378, Total Loss is: 2.2917263507843018\n",
            "MSE Loss is: 0.0146670863032341, h Loss is: 2.2794291973114014, L1 loss: 1.7134790420532227, Total Loss is: 2.2940962314605713\n",
            "MSE Loss is: 0.014337565749883652, h Loss is: 2.2855188846588135, L1 loss: 1.712751030921936, Total Loss is: 2.299856424331665\n",
            "MSE Loss is: 0.014422772452235222, h Loss is: 2.2931535243988037, L1 loss: 1.7122974395751953, Total Loss is: 2.3075761795043945\n",
            "MSE Loss is: 0.014681890606880188, h Loss is: 2.303649663925171, L1 loss: 1.7121950387954712, Total Loss is: 2.318331480026245\n",
            "MSE Loss is: 0.01428238395601511, h Loss is: 2.3140645027160645, L1 loss: 1.7124284505844116, Total Loss is: 2.3283469676971436\n",
            "MSE Loss is: 0.014479737728834152, h Loss is: 2.320528507232666, L1 loss: 1.7125314474105835, Total Loss is: 2.335008144378662\n",
            "MSE Loss is: 0.01472421083599329, h Loss is: 2.3237080574035645, L1 loss: 1.7122386693954468, Total Loss is: 2.3384323120117188\n",
            "MSE Loss is: 0.014401487074792385, h Loss is: 2.3248682022094727, L1 loss: 1.7113765478134155, Total Loss is: 2.3392696380615234\n",
            "MSE Loss is: 0.014772431924939156, h Loss is: 2.324998617172241, L1 loss: 1.7099863290786743, Total Loss is: 2.339771032333374\n",
            "MSE Loss is: 0.014400580897927284, h Loss is: 2.326462745666504, L1 loss: 1.7087032794952393, Total Loss is: 2.3408632278442383\n",
            "MSE Loss is: 0.014443838968873024, h Loss is: 2.3301005363464355, L1 loss: 1.7076338529586792, Total Loss is: 2.3445444107055664\n",
            "MSE Loss is: 0.014568212442100048, h Loss is: 2.3377740383148193, L1 loss: 1.7070375680923462, Total Loss is: 2.352342367172241\n",
            "MSE Loss is: 0.014612333849072456, h Loss is: 2.3451199531555176, L1 loss: 1.7068556547164917, Total Loss is: 2.3597323894500732\n",
            "MSE Loss is: 0.014698040671646595, h Loss is: 2.351398229598999, L1 loss: 1.706721305847168, Total Loss is: 2.366096258163452\n",
            "MSE Loss is: 0.01460554450750351, h Loss is: 2.3554329872131348, L1 loss: 1.7065613269805908, Total Loss is: 2.3700385093688965\n",
            "MSE Loss is: 0.014398094266653061, h Loss is: 2.357517719268799, L1 loss: 1.706460952758789, Total Loss is: 2.371915817260742\n",
            "MSE Loss is: 0.014439186081290245, h Loss is: 2.3577933311462402, L1 loss: 1.7063324451446533, Total Loss is: 2.372232437133789\n",
            "MSE Loss is: 0.014540052972733974, h Loss is: 2.3603878021240234, L1 loss: 1.7058337926864624, Total Loss is: 2.3749277591705322\n",
            "MSE Loss is: 0.014472778886556625, h Loss is: 2.363466501235962, L1 loss: 1.705099105834961, Total Loss is: 2.377939224243164\n",
            "MSE Loss is: 0.014619726687669754, h Loss is: 2.3694608211517334, L1 loss: 1.704403281211853, Total Loss is: 2.384080648422241\n",
            "MSE Loss is: 0.014481921680271626, h Loss is: 2.3751673698425293, L1 loss: 1.7038853168487549, Total Loss is: 2.3896493911743164\n",
            "MSE Loss is: 0.014376049861311913, h Loss is: 2.3817315101623535, L1 loss: 1.7029495239257812, Total Loss is: 2.3961076736450195\n",
            "MSE Loss is: 0.014695530757308006, h Loss is: 2.387552499771118, L1 loss: 1.7021452188491821, Total Loss is: 2.4022481441497803\n",
            "MSE Loss is: 0.01430447492748499, h Loss is: 2.391838312149048, L1 loss: 1.7013969421386719, Total Loss is: 2.4061427116394043\n",
            "MSE Loss is: 0.014404259622097015, h Loss is: 2.397629499435425, L1 loss: 1.7007503509521484, Total Loss is: 2.412033796310425\n",
            "MSE Loss is: 0.01427848543971777, h Loss is: 2.401259422302246, L1 loss: 1.7009023427963257, Total Loss is: 2.4155378341674805\n",
            "MSE Loss is: 0.014907557517290115, h Loss is: 2.403137683868408, L1 loss: 1.7014051675796509, Total Loss is: 2.4180452823638916\n",
            "MSE Loss is: 0.01400796603411436, h Loss is: 2.4054529666900635, L1 loss: 1.7014135122299194, Total Loss is: 2.4194610118865967\n",
            "MSE Loss is: 0.01430458016693592, h Loss is: 2.408874034881592, L1 loss: 1.7009458541870117, Total Loss is: 2.4231786727905273\n",
            "New h_val is : tf.Tensor(0.45162392, shape=(), dtype=float32)\n",
            "Epoch: {} 13\n",
            "MSE Loss is: 0.014188705012202263, h Loss is: 3.0831594467163086, L1 loss: 1.699774980545044, Total Loss is: 3.097348213195801\n",
            "MSE Loss is: 0.01406717300415039, h Loss is: 3.0903353691101074, L1 loss: 1.6985008716583252, Total Loss is: 3.104402542114258\n",
            "MSE Loss is: 0.014262779615819454, h Loss is: 3.0983200073242188, L1 loss: 1.6972637176513672, Total Loss is: 3.1125826835632324\n",
            "MSE Loss is: 0.014016471803188324, h Loss is: 3.1064629554748535, L1 loss: 1.6965059041976929, Total Loss is: 3.1204793453216553\n",
            "MSE Loss is: 0.014358771033585072, h Loss is: 3.114915609359741, L1 loss: 1.6965703964233398, Total Loss is: 3.129274368286133\n",
            "MSE Loss is: 0.014218861237168312, h Loss is: 3.122694969177246, L1 loss: 1.6965456008911133, Total Loss is: 3.136913776397705\n",
            "MSE Loss is: 0.01458609476685524, h Loss is: 3.12308406829834, L1 loss: 1.697000503540039, Total Loss is: 3.1376702785491943\n",
            "MSE Loss is: 0.014773073606193066, h Loss is: 3.1211531162261963, L1 loss: 1.6973183155059814, Total Loss is: 3.1359262466430664\n",
            "MSE Loss is: 0.014622673392295837, h Loss is: 3.121455669403076, L1 loss: 1.697201132774353, Total Loss is: 3.136078357696533\n",
            "MSE Loss is: 0.015005446039140224, h Loss is: 3.123199224472046, L1 loss: 1.6963523626327515, Total Loss is: 3.138204574584961\n",
            "MSE Loss is: 0.014391425997018814, h Loss is: 3.1310253143310547, L1 loss: 1.6954100131988525, Total Loss is: 3.145416736602783\n",
            "MSE Loss is: 0.014470876194536686, h Loss is: 3.1424825191497803, L1 loss: 1.693853735923767, Total Loss is: 3.1569533348083496\n",
            "MSE Loss is: 0.014488402754068375, h Loss is: 3.1534523963928223, L1 loss: 1.6929073333740234, Total Loss is: 3.167940855026245\n",
            "MSE Loss is: 0.014239685609936714, h Loss is: 3.1657140254974365, L1 loss: 1.6919678449630737, Total Loss is: 3.1799538135528564\n",
            "MSE Loss is: 0.014074445702135563, h Loss is: 3.1767160892486572, L1 loss: 1.6908773183822632, Total Loss is: 3.1907906532287598\n",
            "MSE Loss is: 0.01411464624106884, h Loss is: 3.1822988986968994, L1 loss: 1.69017493724823, Total Loss is: 3.196413516998291\n",
            "MSE Loss is: 0.014463555067777634, h Loss is: 3.1855735778808594, L1 loss: 1.6904877424240112, Total Loss is: 3.2000372409820557\n",
            "MSE Loss is: 0.01483110897243023, h Loss is: 3.1821107864379883, L1 loss: 1.6908702850341797, Total Loss is: 3.19694185256958\n",
            "MSE Loss is: 0.014579253271222115, h Loss is: 3.1789345741271973, L1 loss: 1.690740942955017, Total Loss is: 3.193513870239258\n",
            "MSE Loss is: 0.014720965176820755, h Loss is: 3.177572250366211, L1 loss: 1.6903889179229736, Total Loss is: 3.192293167114258\n",
            "MSE Loss is: 0.014547253958880901, h Loss is: 3.1821651458740234, L1 loss: 1.6895363330841064, Total Loss is: 3.1967124938964844\n",
            "MSE Loss is: 0.014216240495443344, h Loss is: 3.192330837249756, L1 loss: 1.6887043714523315, Total Loss is: 3.2065470218658447\n",
            "MSE Loss is: 0.014291503466665745, h Loss is: 3.20383358001709, L1 loss: 1.6880441904067993, Total Loss is: 3.218125104904175\n",
            "MSE Loss is: 0.014535853639245033, h Loss is: 3.217740535736084, L1 loss: 1.6878070831298828, Total Loss is: 3.232276439666748\n",
            "MSE Loss is: 0.014158233068883419, h Loss is: 3.2292191982269287, L1 loss: 1.687843680381775, Total Loss is: 3.243377447128296\n",
            "MSE Loss is: 0.014367738738656044, h Loss is: 3.233165740966797, L1 loss: 1.6878862380981445, Total Loss is: 3.2475335597991943\n",
            "MSE Loss is: 0.014610344544053078, h Loss is: 3.2319443225860596, L1 loss: 1.687607765197754, Total Loss is: 3.2465546131134033\n",
            "MSE Loss is: 0.01428753137588501, h Loss is: 3.2288498878479004, L1 loss: 1.6868776082992554, Total Loss is: 3.2431373596191406\n",
            "MSE Loss is: 0.014663852751255035, h Loss is: 3.2264294624328613, L1 loss: 1.6858530044555664, Total Loss is: 3.241093397140503\n",
            "MSE Loss is: 0.014298411086201668, h Loss is: 3.2283716201782227, L1 loss: 1.6849228143692017, Total Loss is: 3.2426700592041016\n",
            "MSE Loss is: 0.014334641396999359, h Loss is: 3.2350175380706787, L1 loss: 1.6839741468429565, Total Loss is: 3.249352216720581\n",
            "MSE Loss is: 0.01445043459534645, h Loss is: 3.2477622032165527, L1 loss: 1.6832135915756226, Total Loss is: 3.2622127532958984\n",
            "MSE Loss is: 0.014490529894828796, h Loss is: 3.2586100101470947, L1 loss: 1.6828407049179077, Total Loss is: 3.2731006145477295\n",
            "MSE Loss is: 0.014581268653273582, h Loss is: 3.265528678894043, L1 loss: 1.6824921369552612, Total Loss is: 3.2801098823547363\n",
            "MSE Loss is: 0.014494078233838081, h Loss is: 3.2669663429260254, L1 loss: 1.6822235584259033, Total Loss is: 3.2814605236053467\n",
            "MSE Loss is: 0.014283140189945698, h Loss is: 3.2645883560180664, L1 loss: 1.682155966758728, Total Loss is: 3.278871536254883\n",
            "MSE Loss is: 0.014336488209664822, h Loss is: 3.2600290775299072, L1 loss: 1.6822280883789062, Total Loss is: 3.2743656635284424\n",
            "MSE Loss is: 0.014429125003516674, h Loss is: 3.260591506958008, L1 loss: 1.6819992065429688, Total Loss is: 3.2750205993652344\n",
            "MSE Loss is: 0.014355277642607689, h Loss is: 3.2639167308807373, L1 loss: 1.6814771890640259, Total Loss is: 3.2782719135284424\n",
            "MSE Loss is: 0.014500729739665985, h Loss is: 3.2731540203094482, L1 loss: 1.6807693243026733, Total Loss is: 3.2876546382904053\n",
            "MSE Loss is: 0.014364916831254959, h Loss is: 3.282349109649658, L1 loss: 1.6801643371582031, Total Loss is: 3.2967140674591064\n",
            "MSE Loss is: 0.014251826331019402, h Loss is: 3.291872024536133, L1 loss: 1.679022192955017, Total Loss is: 3.306123733520508\n",
            "MSE Loss is: 0.014582328498363495, h Loss is: 3.2985119819641113, L1 loss: 1.678011178970337, Total Loss is: 3.3130943775177\n",
            "MSE Loss is: 0.014193853363394737, h Loss is: 3.301112651824951, L1 loss: 1.6770931482315063, Total Loss is: 3.3153064250946045\n",
            "MSE Loss is: 0.014293722808361053, h Loss is: 3.305016040802002, L1 loss: 1.6763931512832642, Total Loss is: 3.319309711456299\n",
            "MSE Loss is: 0.014167467132210732, h Loss is: 3.3058855533599854, L1 loss: 1.6767215728759766, Total Loss is: 3.3200531005859375\n",
            "MSE Loss is: 0.01479017361998558, h Loss is: 3.3052635192871094, L1 loss: 1.6775490045547485, Total Loss is: 3.3200535774230957\n",
            "MSE Loss is: 0.013902503997087479, h Loss is: 3.3066532611846924, L1 loss: 1.6779865026474, Total Loss is: 3.3205556869506836\n",
            "MSE Loss is: 0.014215079136192799, h Loss is: 3.3110709190368652, L1 loss: 1.6777317523956299, Total Loss is: 3.3252859115600586\n",
            "New h_val is : tf.Tensor(0.48269987, shape=(), dtype=float32)\n",
            "Epoch: {} 14\n",
            "MSE Loss is: 0.01407969556748867, h Loss is: 4.159385681152344, L1 loss: 1.676461100578308, Total Loss is: 4.173465251922607\n",
            "MSE Loss is: 0.013956800103187561, h Loss is: 4.16935396194458, L1 loss: 1.6749086380004883, Total Loss is: 4.1833109855651855\n",
            "MSE Loss is: 0.014146538451313972, h Loss is: 4.179227352142334, L1 loss: 1.673336386680603, Total Loss is: 4.193373680114746\n",
            "MSE Loss is: 0.013907356187701225, h Loss is: 4.187772750854492, L1 loss: 1.6723884344100952, Total Loss is: 4.2016801834106445\n",
            "MSE Loss is: 0.014249783009290695, h Loss is: 4.195649147033691, L1 loss: 1.6725904941558838, Total Loss is: 4.209898948669434\n",
            "MSE Loss is: 0.01411566324532032, h Loss is: 4.201779365539551, L1 loss: 1.672811508178711, Total Loss is: 4.215895175933838\n",
            "MSE Loss is: 0.014461779966950417, h Loss is: 4.19760799407959, L1 loss: 1.6737264394760132, Total Loss is: 4.212069988250732\n",
            "MSE Loss is: 0.014662294648587704, h Loss is: 4.191311359405518, L1 loss: 1.6743967533111572, Total Loss is: 4.2059736251831055\n",
            "MSE Loss is: 0.014519713819026947, h Loss is: 4.190164566040039, L1 loss: 1.6744842529296875, Total Loss is: 4.204684257507324\n",
            "MSE Loss is: 0.014888188801705837, h Loss is: 4.192561149597168, L1 loss: 1.6735680103302002, Total Loss is: 4.207449436187744\n",
            "MSE Loss is: 0.014286055229604244, h Loss is: 4.204439163208008, L1 loss: 1.6724529266357422, Total Loss is: 4.218725204467773\n",
            "MSE Loss is: 0.014352397993206978, h Loss is: 4.2205963134765625, L1 loss: 1.6705459356307983, Total Loss is: 4.234948635101318\n",
            "MSE Loss is: 0.01437340583652258, h Loss is: 4.2341508865356445, L1 loss: 1.6694698333740234, Total Loss is: 4.248524188995361\n",
            "MSE Loss is: 0.014135818928480148, h Loss is: 4.2470879554748535, L1 loss: 1.6685447692871094, Total Loss is: 4.261223793029785\n",
            "MSE Loss is: 0.013962924480438232, h Loss is: 4.256289482116699, L1 loss: 1.667483925819397, Total Loss is: 4.270252227783203\n",
            "MSE Loss is: 0.013995381072163582, h Loss is: 4.257294178009033, L1 loss: 1.6668787002563477, Total Loss is: 4.271289348602295\n",
            "MSE Loss is: 0.014338888227939606, h Loss is: 4.256470680236816, L1 loss: 1.667535424232483, Total Loss is: 4.270809650421143\n",
            "MSE Loss is: 0.01473604328930378, h Loss is: 4.24855375289917, L1 loss: 1.6683708429336548, Total Loss is: 4.263289928436279\n",
            "MSE Loss is: 0.01446901261806488, h Loss is: 4.243863582611084, L1 loss: 1.668433427810669, Total Loss is: 4.2583327293396\n",
            "MSE Loss is: 0.014621250331401825, h Loss is: 4.243858814239502, L1 loss: 1.668077826499939, Total Loss is: 4.258480072021484\n",
            "MSE Loss is: 0.01444673165678978, h Loss is: 4.252715110778809, L1 loss: 1.66689932346344, Total Loss is: 4.267161846160889\n",
            "MSE Loss is: 0.014116968959569931, h Loss is: 4.268246173858643, L1 loss: 1.6656936407089233, Total Loss is: 4.282362937927246\n",
            "MSE Loss is: 0.014186011627316475, h Loss is: 4.282894134521484, L1 loss: 1.6649608612060547, Total Loss is: 4.297080039978027\n",
            "MSE Loss is: 0.014414090663194656, h Loss is: 4.297781467437744, L1 loss: 1.664876937866211, Total Loss is: 4.312195777893066\n",
            "MSE Loss is: 0.014056123793125153, h Loss is: 4.3066606521606445, L1 loss: 1.6651619672775269, Total Loss is: 4.320716857910156\n",
            "MSE Loss is: 0.014274506829679012, h Loss is: 4.304266929626465, L1 loss: 1.6655460596084595, Total Loss is: 4.318541526794434\n",
            "MSE Loss is: 0.014518952928483486, h Loss is: 4.296344757080078, L1 loss: 1.6655628681182861, Total Loss is: 4.310863494873047\n",
            "MSE Loss is: 0.014191430993378162, h Loss is: 4.289068222045898, L1 loss: 1.665061593055725, Total Loss is: 4.30325984954834\n",
            "MSE Loss is: 0.014576232992112637, h Loss is: 4.286300182342529, L1 loss: 1.6640217304229736, Total Loss is: 4.300876617431641\n",
            "MSE Loss is: 0.014221901074051857, h Loss is: 4.292076587677002, L1 loss: 1.6628856658935547, Total Loss is: 4.30629825592041\n",
            "MSE Loss is: 0.014243914745748043, h Loss is: 4.304443836212158, L1 loss: 1.6616296768188477, Total Loss is: 4.318687915802002\n",
            "MSE Loss is: 0.014358684420585632, h Loss is: 4.323112964630127, L1 loss: 1.6607497930526733, Total Loss is: 4.3374714851379395\n",
            "MSE Loss is: 0.014391841366887093, h Loss is: 4.335254669189453, L1 loss: 1.6604454517364502, Total Loss is: 4.34964656829834\n",
            "MSE Loss is: 0.014490874484181404, h Loss is: 4.338191032409668, L1 loss: 1.660223364830017, Total Loss is: 4.352682113647461\n",
            "MSE Loss is: 0.01440688781440258, h Loss is: 4.331742286682129, L1 loss: 1.6601533889770508, Total Loss is: 4.34614896774292\n",
            "MSE Loss is: 0.01418852899223566, h Loss is: 4.321226119995117, L1 loss: 1.6605595350265503, Total Loss is: 4.335414886474609\n",
            "MSE Loss is: 0.01425950974225998, h Loss is: 4.311008453369141, L1 loss: 1.6610984802246094, Total Loss is: 4.325267791748047\n",
            "MSE Loss is: 0.014348049648106098, h Loss is: 4.312015056610107, L1 loss: 1.6611645221710205, Total Loss is: 4.3263630867004395\n",
            "MSE Loss is: 0.014267517253756523, h Loss is: 4.319674491882324, L1 loss: 1.66069757938385, Total Loss is: 4.33394193649292\n",
            "MSE Loss is: 0.01440980564802885, h Loss is: 4.336080551147461, L1 loss: 1.6599369049072266, Total Loss is: 4.350490570068359\n",
            "MSE Loss is: 0.014266560785472393, h Loss is: 4.3501715660095215, L1 loss: 1.6591907739639282, Total Loss is: 4.364438056945801\n",
            "MSE Loss is: 0.014151962473988533, h Loss is: 4.360876560211182, L1 loss: 1.6577255725860596, Total Loss is: 4.375028610229492\n",
            "MSE Loss is: 0.014489496126770973, h Loss is: 4.363874912261963, L1 loss: 1.656467318534851, Total Loss is: 4.378364562988281\n",
            "MSE Loss is: 0.014119578525424004, h Loss is: 4.35945463180542, L1 loss: 1.6554733514785767, Total Loss is: 4.373574256896973\n",
            "MSE Loss is: 0.014200596138834953, h Loss is: 4.357842445373535, L1 loss: 1.655093789100647, Total Loss is: 4.372043132781982\n",
            "MSE Loss is: 0.014069335535168648, h Loss is: 4.35496711730957, L1 loss: 1.6561994552612305, Total Loss is: 4.369036674499512\n",
            "MSE Loss is: 0.014699665829539299, h Loss is: 4.3535003662109375, L1 loss: 1.6578811407089233, Total Loss is: 4.368199825286865\n",
            "MSE Loss is: 0.013821457512676716, h Loss is: 4.357301712036133, L1 loss: 1.658488154411316, Total Loss is: 4.371123313903809\n",
            "MSE Loss is: 0.014146815985441208, h Loss is: 4.366037368774414, L1 loss: 1.658011794090271, Total Loss is: 4.380184173583984\n",
            "New h_val is : tf.Tensor(0.5057583, shape=(), dtype=float32)\n",
            "Epoch: {} 15\n",
            "MSE Loss is: 0.01399545930325985, h Loss is: 5.393220901489258, L1 loss: 1.6561813354492188, Total Loss is: 5.407216548919678\n",
            "MSE Loss is: 0.013863815926015377, h Loss is: 5.40494966506958, L1 loss: 1.6540535688400269, Total Loss is: 5.418813705444336\n",
            "MSE Loss is: 0.014055784791707993, h Loss is: 5.412726879119873, L1 loss: 1.6521211862564087, Total Loss is: 5.426782608032227\n",
            "MSE Loss is: 0.013835731893777847, h Loss is: 5.4163994789123535, L1 loss: 1.651296854019165, Total Loss is: 5.430235385894775\n",
            "MSE Loss is: 0.014168069697916508, h Loss is: 5.419417858123779, L1 loss: 1.6522220373153687, Total Loss is: 5.433586120605469\n",
            "MSE Loss is: 0.014017397537827492, h Loss is: 5.421854019165039, L1 loss: 1.6532047986984253, Total Loss is: 5.435871601104736\n",
            "MSE Loss is: 0.014362413436174393, h Loss is: 5.413242816925049, L1 loss: 1.6547473669052124, Total Loss is: 5.427605152130127\n",
            "MSE Loss is: 0.014575446024537086, h Loss is: 5.404799938201904, L1 loss: 1.655582070350647, Total Loss is: 5.419375419616699\n",
            "MSE Loss is: 0.014443576335906982, h Loss is: 5.405615329742432, L1 loss: 1.6554206609725952, Total Loss is: 5.420058727264404\n",
            "MSE Loss is: 0.01478430163115263, h Loss is: 5.411306858062744, L1 loss: 1.653839111328125, Total Loss is: 5.426091194152832\n",
            "MSE Loss is: 0.014189825393259525, h Loss is: 5.428275108337402, L1 loss: 1.6521872282028198, Total Loss is: 5.442464828491211\n",
            "MSE Loss is: 0.01426630187779665, h Loss is: 5.447572708129883, L1 loss: 1.6498955488204956, Total Loss is: 5.461839199066162\n",
            "MSE Loss is: 0.014274602755904198, h Loss is: 5.459775447845459, L1 loss: 1.649004340171814, Total Loss is: 5.474050045013428\n",
            "MSE Loss is: 0.01405264437198639, h Loss is: 5.468686580657959, L1 loss: 1.6484276056289673, Total Loss is: 5.482739448547363\n",
            "MSE Loss is: 0.013861052691936493, h Loss is: 5.4722819328308105, L1 loss: 1.6478077173233032, Total Loss is: 5.486143112182617\n",
            "MSE Loss is: 0.013882673345506191, h Loss is: 5.466914176940918, L1 loss: 1.6476143598556519, Total Loss is: 5.480796813964844\n",
            "MSE Loss is: 0.014226958155632019, h Loss is: 5.463191509246826, L1 loss: 1.6487007141113281, Total Loss is: 5.477418422698975\n",
            "MSE Loss is: 0.014652836136519909, h Loss is: 5.453581809997559, L1 loss: 1.6496734619140625, Total Loss is: 5.468234539031982\n",
            "MSE Loss is: 0.014379343017935753, h Loss is: 5.450766086578369, L1 loss: 1.649473786354065, Total Loss is: 5.465145587921143\n",
            "MSE Loss is: 0.01453104056417942, h Loss is: 5.454470634460449, L1 loss: 1.6485986709594727, Total Loss is: 5.469001770019531\n",
            "MSE Loss is: 0.014358969405293465, h Loss is: 5.467870235443115, L1 loss: 1.6467586755752563, Total Loss is: 5.482229232788086\n",
            "MSE Loss is: 0.014036133885383606, h Loss is: 5.486588954925537, L1 loss: 1.6453293561935425, Total Loss is: 5.500625133514404\n",
            "MSE Loss is: 0.014108799397945404, h Loss is: 5.5000834465026855, L1 loss: 1.6447241306304932, Total Loss is: 5.5141921043396\n",
            "MSE Loss is: 0.01430968102067709, h Loss is: 5.511335849761963, L1 loss: 1.6451939344406128, Total Loss is: 5.525645732879639\n",
            "MSE Loss is: 0.013964924961328506, h Loss is: 5.514101028442383, L1 loss: 1.6461321115493774, Total Loss is: 5.528066158294678\n",
            "MSE Loss is: 0.014194784685969353, h Loss is: 5.504010200500488, L1 loss: 1.647018313407898, Total Loss is: 5.518205165863037\n",
            "MSE Loss is: 0.014446452260017395, h Loss is: 5.490841865539551, L1 loss: 1.6471976041793823, Total Loss is: 5.505288124084473\n",
            "MSE Loss is: 0.014106541872024536, h Loss is: 5.482969760894775, L1 loss: 1.6463669538497925, Total Loss is: 5.497076511383057\n",
            "MSE Loss is: 0.014502046629786491, h Loss is: 5.483864784240723, L1 loss: 1.6448023319244385, Total Loss is: 5.498366832733154\n",
            "MSE Loss is: 0.014155806973576546, h Loss is: 5.496107578277588, L1 loss: 1.6431931257247925, Total Loss is: 5.510263442993164\n",
            "MSE Loss is: 0.014163464307785034, h Loss is: 5.513794898986816, L1 loss: 1.6416761875152588, Total Loss is: 5.527958393096924\n",
            "MSE Loss is: 0.014289380051195621, h Loss is: 5.534903526306152, L1 loss: 1.640850305557251, Total Loss is: 5.549192905426025\n",
            "MSE Loss is: 0.014310609549283981, h Loss is: 5.542319297790527, L1 loss: 1.6408824920654297, Total Loss is: 5.5566301345825195\n",
            "MSE Loss is: 0.014417784288525581, h Loss is: 5.535131454467773, L1 loss: 1.6410773992538452, Total Loss is: 5.549549102783203\n",
            "MSE Loss is: 0.014326578006148338, h Loss is: 5.517127990722656, L1 loss: 1.6413519382476807, Total Loss is: 5.531454563140869\n",
            "MSE Loss is: 0.014109717682003975, h Loss is: 5.498932361602783, L1 loss: 1.6419473886489868, Total Loss is: 5.513041973114014\n",
            "MSE Loss is: 0.014197008684277534, h Loss is: 5.487188816070557, L1 loss: 1.6424182653427124, Total Loss is: 5.501385688781738\n",
            "MSE Loss is: 0.014290997758507729, h Loss is: 5.4947285652160645, L1 loss: 1.6421715021133423, Total Loss is: 5.509019374847412\n",
            "MSE Loss is: 0.014195498079061508, h Loss is: 5.511391639709473, L1 loss: 1.6412357091903687, Total Loss is: 5.52558708190918\n",
            "MSE Loss is: 0.014339154586195946, h Loss is: 5.536027908325195, L1 loss: 1.640053391456604, Total Loss is: 5.5503668785095215\n",
            "MSE Loss is: 0.014185097068548203, h Loss is: 5.551167964935303, L1 loss: 1.6391032934188843, Total Loss is: 5.565352916717529\n",
            "MSE Loss is: 0.014072932302951813, h Loss is: 5.555985450744629, L1 loss: 1.6376780271530151, Total Loss is: 5.570058345794678\n",
            "MSE Loss is: 0.014412903226912022, h Loss is: 5.5479936599731445, L1 loss: 1.6367348432540894, Total Loss is: 5.562406539916992\n",
            "MSE Loss is: 0.014065787196159363, h Loss is: 5.532170295715332, L1 loss: 1.6361907720565796, Total Loss is: 5.546236038208008\n",
            "MSE Loss is: 0.014107108116149902, h Loss is: 5.525847434997559, L1 loss: 1.6362812519073486, Total Loss is: 5.539954662322998\n",
            "MSE Loss is: 0.013981558382511139, h Loss is: 5.52412223815918, L1 loss: 1.6377108097076416, Total Loss is: 5.5381035804748535\n",
            "MSE Loss is: 0.014632116071879864, h Loss is: 5.528207302093506, L1 loss: 1.6393711566925049, Total Loss is: 5.542839527130127\n",
            "MSE Loss is: 0.013755165040493011, h Loss is: 5.539095878601074, L1 loss: 1.6395081281661987, Total Loss is: 5.55285120010376\n",
            "MSE Loss is: 0.014073144644498825, h Loss is: 5.5528998374938965, L1 loss: 1.6383392810821533, Total Loss is: 5.5669732093811035\n",
            "New h_val is : tf.Tensor(0.5206342, shape=(), dtype=float32)\n",
            "Epoch: {} 16\n",
            "MSE Loss is: 0.013917462900280952, h Loss is: 6.7497076988220215, L1 loss: 1.635866403579712, Total Loss is: 6.763625144958496\n",
            "MSE Loss is: 0.013782735913991928, h Loss is: 6.755586624145508, L1 loss: 1.6335041522979736, Total Loss is: 6.769369125366211\n",
            "MSE Loss is: 0.01398942805826664, h Loss is: 6.753079414367676, L1 loss: 1.6318687200546265, Total Loss is: 6.767068862915039\n",
            "MSE Loss is: 0.01377684623003006, h Loss is: 6.746718406677246, L1 loss: 1.6317914724349976, Total Loss is: 6.760495185852051\n",
            "MSE Loss is: 0.014098089188337326, h Loss is: 6.744908809661865, L1 loss: 1.6336244344711304, Total Loss is: 6.759006977081299\n",
            "MSE Loss is: 0.013925337232649326, h Loss is: 6.748065948486328, L1 loss: 1.6351474523544312, Total Loss is: 6.761991500854492\n",
            "MSE Loss is: 0.014300435781478882, h Loss is: 6.7408342361450195, L1 loss: 1.6366490125656128, Total Loss is: 6.755134582519531\n",
            "MSE Loss is: 0.014502427540719509, h Loss is: 6.7348761558532715, L1 loss: 1.636827826499939, Total Loss is: 6.749378681182861\n",
            "MSE Loss is: 0.014368416741490364, h Loss is: 6.7393798828125, L1 loss: 1.635745882987976, Total Loss is: 6.753748416900635\n",
            "MSE Loss is: 0.014692716300487518, h Loss is: 6.746108531951904, L1 loss: 1.633345603942871, Total Loss is: 6.760801315307617\n",
            "MSE Loss is: 0.014104599133133888, h Loss is: 6.763249397277832, L1 loss: 1.631455659866333, Total Loss is: 6.777353763580322\n",
            "MSE Loss is: 0.014206208288669586, h Loss is: 6.7796525955200195, L1 loss: 1.6295015811920166, Total Loss is: 6.793859004974365\n",
            "MSE Loss is: 0.014192363247275352, h Loss is: 6.7854814529418945, L1 loss: 1.6294716596603394, Total Loss is: 6.799674034118652\n",
            "MSE Loss is: 0.013986358419060707, h Loss is: 6.788379669189453, L1 loss: 1.6297725439071655, Total Loss is: 6.802366256713867\n",
            "MSE Loss is: 0.013775317929685116, h Loss is: 6.78776216506958, L1 loss: 1.6296974420547485, Total Loss is: 6.80153751373291\n",
            "MSE Loss is: 0.013772761449217796, h Loss is: 6.779739856719971, L1 loss: 1.6294670104980469, Total Loss is: 6.79351282119751\n",
            "MSE Loss is: 0.014139050617814064, h Loss is: 6.7777814865112305, L1 loss: 1.6301196813583374, Total Loss is: 6.7919206619262695\n",
            "MSE Loss is: 0.014574632979929447, h Loss is: 6.769412040710449, L1 loss: 1.6303036212921143, Total Loss is: 6.783986568450928\n",
            "MSE Loss is: 0.014296727254986763, h Loss is: 6.768714427947998, L1 loss: 1.6293655633926392, Total Loss is: 6.783010959625244\n",
            "MSE Loss is: 0.014432551339268684, h Loss is: 6.77340841293335, L1 loss: 1.6281101703643799, Total Loss is: 6.787840843200684\n",
            "MSE Loss is: 0.014286680147051811, h Loss is: 6.7867560386657715, L1 loss: 1.6262998580932617, Total Loss is: 6.801042556762695\n",
            "MSE Loss is: 0.013967343606054783, h Loss is: 6.803771018981934, L1 loss: 1.6253553628921509, Total Loss is: 6.8177385330200195\n",
            "MSE Loss is: 0.014042548835277557, h Loss is: 6.8123884201049805, L1 loss: 1.6255321502685547, Total Loss is: 6.826430797576904\n",
            "MSE Loss is: 0.014218800701200962, h Loss is: 6.8190131187438965, L1 loss: 1.626771330833435, Total Loss is: 6.8332319259643555\n",
            "MSE Loss is: 0.01388512458652258, h Loss is: 6.81732177734375, L1 loss: 1.6280087232589722, Total Loss is: 6.831206798553467\n",
            "MSE Loss is: 0.01412812527269125, h Loss is: 6.8027777671813965, L1 loss: 1.628548502922058, Total Loss is: 6.816905975341797\n",
            "MSE Loss is: 0.014382355846464634, h Loss is: 6.7880706787109375, L1 loss: 1.6279728412628174, Total Loss is: 6.80245304107666\n",
            "MSE Loss is: 0.014024840667843819, h Loss is: 6.7821807861328125, L1 loss: 1.6263235807418823, Total Loss is: 6.796205520629883\n",
            "MSE Loss is: 0.014437275007367134, h Loss is: 6.786923885345459, L1 loss: 1.6243327856063843, Total Loss is: 6.801361083984375\n",
            "MSE Loss is: 0.014088917523622513, h Loss is: 6.8030595779418945, L1 loss: 1.6227566003799438, Total Loss is: 6.817148685455322\n",
            "MSE Loss is: 0.01409265585243702, h Loss is: 6.8211212158203125, L1 loss: 1.6216564178466797, Total Loss is: 6.835213661193848\n",
            "MSE Loss is: 0.01422471459954977, h Loss is: 6.8392229080200195, L1 loss: 1.6217199563980103, Total Loss is: 6.853447437286377\n",
            "MSE Loss is: 0.014237076044082642, h Loss is: 6.837470054626465, L1 loss: 1.622448205947876, Total Loss is: 6.8517069816589355\n",
            "MSE Loss is: 0.014353254809975624, h Loss is: 6.818711280822754, L1 loss: 1.6228469610214233, Total Loss is: 6.833064556121826\n",
            "MSE Loss is: 0.014248166233301163, h Loss is: 6.7914652824401855, L1 loss: 1.622793436050415, Total Loss is: 6.805713653564453\n",
            "MSE Loss is: 0.014046925120055676, h Loss is: 6.770873546600342, L1 loss: 1.6228004693984985, Total Loss is: 6.784920692443848\n",
            "MSE Loss is: 0.014129074290394783, h Loss is: 6.763309955596924, L1 loss: 1.6226929426193237, Total Loss is: 6.777439117431641\n",
            "MSE Loss is: 0.014234738424420357, h Loss is: 6.780920028686523, L1 loss: 1.6220768690109253, Total Loss is: 6.795154571533203\n",
            "MSE Loss is: 0.014126913622021675, h Loss is: 6.805394649505615, L1 loss: 1.6210516691207886, Total Loss is: 6.819521427154541\n",
            "MSE Loss is: 0.014279477298259735, h Loss is: 6.832521438598633, L1 loss: 1.6202446222305298, Total Loss is: 6.846800804138184\n",
            "MSE Loss is: 0.014125863090157509, h Loss is: 6.840183258056641, L1 loss: 1.6198246479034424, Total Loss is: 6.85430908203125\n",
            "MSE Loss is: 0.014005180448293686, h Loss is: 6.8318562507629395, L1 loss: 1.6188358068466187, Total Loss is: 6.845861434936523\n",
            "MSE Loss is: 0.014348781667649746, h Loss is: 6.810328006744385, L1 loss: 1.6181297302246094, Total Loss is: 6.824676990509033\n",
            "MSE Loss is: 0.014008956030011177, h Loss is: 6.786488056182861, L1 loss: 1.6175800561904907, Total Loss is: 6.800497055053711\n",
            "MSE Loss is: 0.014026671648025513, h Loss is: 6.783482551574707, L1 loss: 1.617549180984497, Total Loss is: 6.79750919342041\n",
            "MSE Loss is: 0.013914384879171848, h Loss is: 6.791136741638184, L1 loss: 1.618791937828064, Total Loss is: 6.805051326751709\n",
            "MSE Loss is: 0.014565501362085342, h Loss is: 6.804811477661133, L1 loss: 1.6201895475387573, Total Loss is: 6.8193769454956055\n",
            "MSE Loss is: 0.013692818582057953, h Loss is: 6.820382595062256, L1 loss: 1.6199543476104736, Total Loss is: 6.834075450897217\n",
            "MSE Loss is: 0.01399780623614788, h Loss is: 6.831406593322754, L1 loss: 1.6185121536254883, Total Loss is: 6.845404624938965\n",
            "New h_val is : tf.Tensor(0.52686596, shape=(), dtype=float32)\n",
            "Epoch: {} 17\n",
            "MSE Loss is: 0.013844319619238377, h Loss is: 8.170153617858887, L1 loss: 1.615962028503418, Total Loss is: 8.183998107910156\n",
            "MSE Loss is: 0.013715720735490322, h Loss is: 8.162148475646973, L1 loss: 1.613916039466858, Total Loss is: 8.175864219665527\n",
            "MSE Loss is: 0.013928752392530441, h Loss is: 8.14797306060791, L1 loss: 1.612884521484375, Total Loss is: 8.161901473999023\n",
            "MSE Loss is: 0.013698097318410873, h Loss is: 8.137290954589844, L1 loss: 1.6134971380233765, Total Loss is: 8.150988578796387\n",
            "MSE Loss is: 0.014031556434929371, h Loss is: 8.140447616577148, L1 loss: 1.6158636808395386, Total Loss is: 8.154479026794434\n",
            "MSE Loss is: 0.013870108872652054, h Loss is: 8.152835845947266, L1 loss: 1.6173040866851807, Total Loss is: 8.166706085205078\n",
            "MSE Loss is: 0.0142554622143507, h Loss is: 8.149447441101074, L1 loss: 1.6181353330612183, Total Loss is: 8.163702964782715\n",
            "MSE Loss is: 0.014429123140871525, h Loss is: 8.14167594909668, L1 loss: 1.617382287979126, Total Loss is: 8.156105041503906\n",
            "MSE Loss is: 0.014285585843026638, h Loss is: 8.141436576843262, L1 loss: 1.6156224012374878, Total Loss is: 8.155722618103027\n",
            "MSE Loss is: 0.014637160114943981, h Loss is: 8.140352249145508, L1 loss: 1.6130542755126953, Total Loss is: 8.154989242553711\n",
            "MSE Loss is: 0.014048915356397629, h Loss is: 8.152715682983398, L1 loss: 1.6117874383926392, Total Loss is: 8.166764259338379\n",
            "MSE Loss is: 0.014137882739305496, h Loss is: 8.166398048400879, L1 loss: 1.6108118295669556, Total Loss is: 8.180536270141602\n",
            "MSE Loss is: 0.014129113405942917, h Loss is: 8.170226097106934, L1 loss: 1.6117504835128784, Total Loss is: 8.184354782104492\n",
            "MSE Loss is: 0.013941103592514992, h Loss is: 8.173264503479004, L1 loss: 1.6125290393829346, Total Loss is: 8.18720531463623\n",
            "MSE Loss is: 0.013724986463785172, h Loss is: 8.173216819763184, L1 loss: 1.6121368408203125, Total Loss is: 8.186942100524902\n",
            "MSE Loss is: 0.013692084699869156, h Loss is: 8.163774490356445, L1 loss: 1.6111793518066406, Total Loss is: 8.17746639251709\n",
            "MSE Loss is: 0.014076572842895985, h Loss is: 8.161478042602539, L1 loss: 1.6112442016601562, Total Loss is: 8.175554275512695\n",
            "MSE Loss is: 0.014515334740281105, h Loss is: 8.149559020996094, L1 loss: 1.6111341714859009, Total Loss is: 8.164073944091797\n",
            "MSE Loss is: 0.01422964408993721, h Loss is: 8.14597225189209, L1 loss: 1.6100810766220093, Total Loss is: 8.160202026367188\n",
            "MSE Loss is: 0.014356730505824089, h Loss is: 8.14829158782959, L1 loss: 1.6091057062149048, Total Loss is: 8.16264820098877\n",
            "MSE Loss is: 0.01423007994890213, h Loss is: 8.160903930664062, L1 loss: 1.6078941822052002, Total Loss is: 8.17513370513916\n",
            "MSE Loss is: 0.013903496786952019, h Loss is: 8.177946090698242, L1 loss: 1.6077234745025635, Total Loss is: 8.191849708557129\n",
            "MSE Loss is: 0.013977188616991043, h Loss is: 8.184366226196289, L1 loss: 1.6083768606185913, Total Loss is: 8.198343276977539\n",
            "MSE Loss is: 0.01415350940078497, h Loss is: 8.188977241516113, L1 loss: 1.6096543073654175, Total Loss is: 8.203130722045898\n",
            "MSE Loss is: 0.013828352093696594, h Loss is: 8.184175491333008, L1 loss: 1.610620379447937, Total Loss is: 8.198003768920898\n",
            "MSE Loss is: 0.014072312042117119, h Loss is: 8.16474723815918, L1 loss: 1.6107338666915894, Total Loss is: 8.17881965637207\n",
            "MSE Loss is: 0.014322667382657528, h Loss is: 8.147031784057617, L1 loss: 1.6098216772079468, Total Loss is: 8.161354064941406\n",
            "MSE Loss is: 0.013958455994725227, h Loss is: 8.141284942626953, L1 loss: 1.6080840826034546, Total Loss is: 8.155243873596191\n",
            "MSE Loss is: 0.01439280342310667, h Loss is: 8.147940635681152, L1 loss: 1.606313705444336, Total Loss is: 8.162333488464355\n",
            "MSE Loss is: 0.014038785360753536, h Loss is: 8.166302680969238, L1 loss: 1.6053255796432495, Total Loss is: 8.180341720581055\n",
            "MSE Loss is: 0.01402861438691616, h Loss is: 8.183265686035156, L1 loss: 1.6050493717193604, Total Loss is: 8.197294235229492\n",
            "MSE Loss is: 0.014163536950945854, h Loss is: 8.197625160217285, L1 loss: 1.6056302785873413, Total Loss is: 8.21178913116455\n",
            "MSE Loss is: 0.014181260019540787, h Loss is: 8.186615943908691, L1 loss: 1.6065149307250977, Total Loss is: 8.200797080993652\n",
            "MSE Loss is: 0.014306817203760147, h Loss is: 8.1573486328125, L1 loss: 1.6067556142807007, Total Loss is: 8.171655654907227\n",
            "MSE Loss is: 0.014193281531333923, h Loss is: 8.123082160949707, L1 loss: 1.6063547134399414, Total Loss is: 8.137275695800781\n",
            "MSE Loss is: 0.013995629735291004, h Loss is: 8.102789878845215, L1 loss: 1.6060446500778198, Total Loss is: 8.116785049438477\n",
            "MSE Loss is: 0.01406445074826479, h Loss is: 8.1008939743042, L1 loss: 1.6058155298233032, Total Loss is: 8.114958763122559\n",
            "MSE Loss is: 0.014190122485160828, h Loss is: 8.127767562866211, L1 loss: 1.605303168296814, Total Loss is: 8.14195728302002\n",
            "MSE Loss is: 0.01407067570835352, h Loss is: 8.156096458435059, L1 loss: 1.6047016382217407, Total Loss is: 8.170166969299316\n",
            "MSE Loss is: 0.014227505773305893, h Loss is: 8.180234909057617, L1 loss: 1.6044257879257202, Total Loss is: 8.194462776184082\n",
            "MSE Loss is: 0.014083616435527802, h Loss is: 8.175657272338867, L1 loss: 1.6044906377792358, Total Loss is: 8.189741134643555\n",
            "MSE Loss is: 0.01395252626389265, h Loss is: 8.153106689453125, L1 loss: 1.6037899255752563, Total Loss is: 8.167058944702148\n",
            "MSE Loss is: 0.014302825555205345, h Loss is: 8.1216402053833, L1 loss: 1.6032699346542358, Total Loss is: 8.135943412780762\n",
            "MSE Loss is: 0.013963361270725727, h Loss is: 8.096465110778809, L1 loss: 1.602800726890564, Total Loss is: 8.110428810119629\n",
            "MSE Loss is: 0.013972993940114975, h Loss is: 8.103522300720215, L1 loss: 1.6025463342666626, Total Loss is: 8.1174955368042\n",
            "MSE Loss is: 0.01386645995080471, h Loss is: 8.123137474060059, L1 loss: 1.6036598682403564, Total Loss is: 8.137003898620605\n",
            "MSE Loss is: 0.014511669985949993, h Loss is: 8.142775535583496, L1 loss: 1.604946494102478, Total Loss is: 8.15728759765625\n",
            "MSE Loss is: 0.013646763749420643, h Loss is: 8.154589653015137, L1 loss: 1.6046663522720337, Total Loss is: 8.16823673248291\n",
            "MSE Loss is: 0.013948864303529263, h Loss is: 8.153951644897461, L1 loss: 1.60333251953125, Total Loss is: 8.167900085449219\n",
            "New h_val is : tf.Tensor(0.5254884, shape=(), dtype=float32)\n",
            "Epoch: {} 18\n",
            "MSE Loss is: 0.013783860020339489, h Loss is: 9.605521202087402, L1 loss: 1.601096749305725, Total Loss is: 9.619304656982422\n",
            "MSE Loss is: 0.013665104284882545, h Loss is: 9.58629322052002, L1 loss: 1.5997581481933594, Total Loss is: 9.599958419799805\n",
            "MSE Loss is: 0.013878177851438522, h Loss is: 9.570043563842773, L1 loss: 1.599548578262329, Total Loss is: 9.583921432495117\n",
            "MSE Loss is: 0.013631895184516907, h Loss is: 9.566462516784668, L1 loss: 1.6006860733032227, Total Loss is: 9.580094337463379\n",
            "MSE Loss is: 0.013974886387586594, h Loss is: 9.581927299499512, L1 loss: 1.6030136346817017, Total Loss is: 9.595902442932129\n",
            "MSE Loss is: 0.01384521834552288, h Loss is: 9.60256576538086, L1 loss: 1.603877067565918, Total Loss is: 9.616411209106445\n",
            "MSE Loss is: 0.014211787842214108, h Loss is: 9.593839645385742, L1 loss: 1.6041910648345947, Total Loss is: 9.608051300048828\n",
            "MSE Loss is: 0.014372407458722591, h Loss is: 9.572342872619629, L1 loss: 1.6032981872558594, Total Loss is: 9.586715698242188\n",
            "MSE Loss is: 0.014224853366613388, h Loss is: 9.559526443481445, L1 loss: 1.601788878440857, Total Loss is: 9.573751449584961\n",
            "MSE Loss is: 0.01461070030927658, h Loss is: 9.550432205200195, L1 loss: 1.5999798774719238, Total Loss is: 9.565042495727539\n",
            "MSE Loss is: 0.014010094106197357, h Loss is: 9.56541633605957, L1 loss: 1.599557876586914, Total Loss is: 9.579426765441895\n",
            "MSE Loss is: 0.01407204382121563, h Loss is: 9.586368560791016, L1 loss: 1.5992488861083984, Total Loss is: 9.600440979003906\n",
            "MSE Loss is: 0.014091352932155132, h Loss is: 9.595074653625488, L1 loss: 1.6004914045333862, Total Loss is: 9.609166145324707\n",
            "MSE Loss is: 0.013914092443883419, h Loss is: 9.598796844482422, L1 loss: 1.6012182235717773, Total Loss is: 9.612710952758789\n",
            "MSE Loss is: 0.013687759637832642, h Loss is: 9.593831062316895, L1 loss: 1.6004570722579956, Total Loss is: 9.607519149780273\n",
            "MSE Loss is: 0.013641828671097755, h Loss is: 9.574951171875, L1 loss: 1.5993484258651733, Total Loss is: 9.588592529296875\n",
            "MSE Loss is: 0.014031557366251945, h Loss is: 9.566659927368164, L1 loss: 1.5994176864624023, Total Loss is: 9.58069133758545\n",
            "MSE Loss is: 0.014485448598861694, h Loss is: 9.550002098083496, L1 loss: 1.5997040271759033, Total Loss is: 9.56448745727539\n",
            "MSE Loss is: 0.014181946404278278, h Loss is: 9.54697322845459, L1 loss: 1.5990962982177734, Total Loss is: 9.561155319213867\n",
            "MSE Loss is: 0.01431229431182146, h Loss is: 9.552515983581543, L1 loss: 1.598529577255249, Total Loss is: 9.566828727722168\n",
            "MSE Loss is: 0.014187049120664597, h Loss is: 9.568986892700195, L1 loss: 1.5975791215896606, Total Loss is: 9.583173751831055\n",
            "MSE Loss is: 0.013857746496796608, h Loss is: 9.587396621704102, L1 loss: 1.5973999500274658, Total Loss is: 9.6012544631958\n",
            "MSE Loss is: 0.013930970802903175, h Loss is: 9.589266777038574, L1 loss: 1.5979814529418945, Total Loss is: 9.603198051452637\n",
            "MSE Loss is: 0.014103676192462444, h Loss is: 9.587957382202148, L1 loss: 1.599094271659851, Total Loss is: 9.60206127166748\n",
            "MSE Loss is: 0.013784353621304035, h Loss is: 9.576887130737305, L1 loss: 1.6000293493270874, Total Loss is: 9.59067153930664\n",
            "MSE Loss is: 0.014030815102159977, h Loss is: 9.551665306091309, L1 loss: 1.6002590656280518, Total Loss is: 9.565695762634277\n",
            "MSE Loss is: 0.014280475676059723, h Loss is: 9.532732009887695, L1 loss: 1.5995416641235352, Total Loss is: 9.547012329101562\n",
            "MSE Loss is: 0.013908660039305687, h Loss is: 9.530171394348145, L1 loss: 1.5980010032653809, Total Loss is: 9.544079780578613\n",
            "MSE Loss is: 0.014360016211867332, h Loss is: 9.541011810302734, L1 loss: 1.5962228775024414, Total Loss is: 9.55537223815918\n",
            "MSE Loss is: 0.014006581157445908, h Loss is: 9.561779975891113, L1 loss: 1.5952532291412354, Total Loss is: 9.575786590576172\n",
            "MSE Loss is: 0.013973883353173733, h Loss is: 9.575578689575195, L1 loss: 1.5951050519943237, Total Loss is: 9.589552879333496\n",
            "MSE Loss is: 0.01412157528102398, h Loss is: 9.583553314208984, L1 loss: 1.5959075689315796, Total Loss is: 9.597675323486328\n",
            "MSE Loss is: 0.014141805469989777, h Loss is: 9.561387062072754, L1 loss: 1.5969785451889038, Total Loss is: 9.575529098510742\n",
            "MSE Loss is: 0.014272438362240791, h Loss is: 9.521940231323242, L1 loss: 1.597271203994751, Total Loss is: 9.536212921142578\n",
            "MSE Loss is: 0.01415526494383812, h Loss is: 9.483280181884766, L1 loss: 1.5967751741409302, Total Loss is: 9.497435569763184\n",
            "MSE Loss is: 0.013957709074020386, h Loss is: 9.466836929321289, L1 loss: 1.5963433980941772, Total Loss is: 9.480794906616211\n",
            "MSE Loss is: 0.014017104171216488, h Loss is: 9.472737312316895, L1 loss: 1.5960438251495361, Total Loss is: 9.486754417419434\n",
            "MSE Loss is: 0.014162275940179825, h Loss is: 9.508132934570312, L1 loss: 1.5954313278198242, Total Loss is: 9.522294998168945\n",
            "MSE Loss is: 0.014019275084137917, h Loss is: 9.536470413208008, L1 loss: 1.5948952436447144, Total Loss is: 9.55048942565918\n",
            "MSE Loss is: 0.014184960164129734, h Loss is: 9.553030967712402, L1 loss: 1.594896912574768, Total Loss is: 9.567215919494629\n",
            "MSE Loss is: 0.014056320302188396, h Loss is: 9.53349781036377, L1 loss: 1.5953431129455566, Total Loss is: 9.547554016113281\n",
            "MSE Loss is: 0.013911865651607513, h Loss is: 9.498147010803223, L1 loss: 1.5948594808578491, Total Loss is: 9.512059211730957\n",
            "MSE Loss is: 0.014267830178141594, h Loss is: 9.461907386779785, L1 loss: 1.5943485498428345, Total Loss is: 9.476175308227539\n",
            "MSE Loss is: 0.013923566788434982, h Loss is: 9.441633224487305, L1 loss: 1.5937697887420654, Total Loss is: 9.455556869506836\n",
            "MSE Loss is: 0.013933530077338219, h Loss is: 9.462757110595703, L1 loss: 1.5934264659881592, Total Loss is: 9.476690292358398\n",
            "MSE Loss is: 0.01383280660957098, h Loss is: 9.493020057678223, L1 loss: 1.5944623947143555, Total Loss is: 9.506853103637695\n",
            "MSE Loss is: 0.014477571472525597, h Loss is: 9.512214660644531, L1 loss: 1.595766305923462, Total Loss is: 9.526692390441895\n",
            "MSE Loss is: 0.013610761612653732, h Loss is: 9.512289047241211, L1 loss: 1.59546959400177, Total Loss is: 9.525899887084961\n",
            "MSE Loss is: 0.013914115726947784, h Loss is: 9.495504379272461, L1 loss: 1.5942013263702393, Total Loss is: 9.509418487548828\n",
            "New h_val is : tf.Tensor(0.51892805, shape=(), dtype=float32)\n",
            "Epoch: {} 19\n",
            "MSE Loss is: 0.013739675283432007, h Loss is: 11.041278839111328, L1 loss: 1.592063069343567, Total Loss is: 11.055018424987793\n",
            "MSE Loss is: 0.013628505170345306, h Loss is: 11.019442558288574, L1 loss: 1.5909641981124878, Total Loss is: 11.033071517944336\n",
            "MSE Loss is: 0.013836028054356575, h Loss is: 11.011955261230469, L1 loss: 1.5909950733184814, Total Loss is: 11.02579116821289\n",
            "MSE Loss is: 0.013571667484939098, h Loss is: 11.022346496582031, L1 loss: 1.5923043489456177, Total Loss is: 11.035918235778809\n",
            "MSE Loss is: 0.013934209942817688, h Loss is: 11.04908275604248, L1 loss: 1.5946359634399414, Total Loss is: 11.063016891479492\n",
            "MSE Loss is: 0.013833225704729557, h Loss is: 11.069267272949219, L1 loss: 1.595191478729248, Total Loss is: 11.083100318908691\n",
            "MSE Loss is: 0.01416968647390604, h Loss is: 11.043761253356934, L1 loss: 1.595133662223816, Total Loss is: 11.057930946350098\n",
            "MSE Loss is: 0.014336271211504936, h Loss is: 11.001826286315918, L1 loss: 1.594028115272522, Total Loss is: 11.016162872314453\n",
            "MSE Loss is: 0.014183896593749523, h Loss is: 10.978796005249023, L1 loss: 1.592663049697876, Total Loss is: 10.992980003356934\n",
            "MSE Loss is: 0.014597785659134388, h Loss is: 10.971312522888184, L1 loss: 1.5912449359893799, Total Loss is: 10.985910415649414\n",
            "MSE Loss is: 0.013977402821183205, h Loss is: 11.000341415405273, L1 loss: 1.5913368463516235, Total Loss is: 11.014318466186523\n",
            "MSE Loss is: 0.014023803174495697, h Loss is: 11.034591674804688, L1 loss: 1.5912675857543945, Total Loss is: 11.048615455627441\n",
            "MSE Loss is: 0.01407390646636486, h Loss is: 11.045300483703613, L1 loss: 1.592451572418213, Total Loss is: 11.059374809265137\n",
            "MSE Loss is: 0.013897834345698357, h Loss is: 11.04047679901123, L1 loss: 1.5927770137786865, Total Loss is: 11.054374694824219\n",
            "MSE Loss is: 0.013646267354488373, h Loss is: 11.020926475524902, L1 loss: 1.5915193557739258, Total Loss is: 11.03457260131836\n",
            "MSE Loss is: 0.013618679717183113, h Loss is: 10.988609313964844, L1 loss: 1.5901745557785034, Total Loss is: 11.002227783203125\n",
            "MSE Loss is: 0.014002169482409954, h Loss is: 10.978449821472168, L1 loss: 1.590514898300171, Total Loss is: 10.992451667785645\n",
            "MSE Loss is: 0.014461341314017773, h Loss is: 10.96583366394043, L1 loss: 1.5912853479385376, Total Loss is: 10.980295181274414\n",
            "MSE Loss is: 0.014144912362098694, h Loss is: 10.971529006958008, L1 loss: 1.5910307168960571, Total Loss is: 10.985673904418945\n",
            "MSE Loss is: 0.014292617328464985, h Loss is: 10.983302116394043, L1 loss: 1.5905126333236694, Total Loss is: 10.997594833374023\n",
            "MSE Loss is: 0.01415790244936943, h Loss is: 11.00036334991455, L1 loss: 1.5892924070358276, Total Loss is: 11.014521598815918\n",
            "MSE Loss is: 0.013820754364132881, h Loss is: 11.013169288635254, L1 loss: 1.5888208150863647, Total Loss is: 11.026989936828613\n",
            "MSE Loss is: 0.013904262334108353, h Loss is: 11.004240989685059, L1 loss: 1.5892857313156128, Total Loss is: 11.018145561218262\n",
            "MSE Loss is: 0.014062736183404922, h Loss is: 10.995563507080078, L1 loss: 1.5905466079711914, Total Loss is: 11.009626388549805\n",
            "MSE Loss is: 0.01375020481646061, h Loss is: 10.982056617736816, L1 loss: 1.5917160511016846, Total Loss is: 10.995806694030762\n",
            "MSE Loss is: 0.01400003582239151, h Loss is: 10.957429885864258, L1 loss: 1.5920788049697876, Total Loss is: 10.971429824829102\n",
            "MSE Loss is: 0.01424629520624876, h Loss is: 10.94266128540039, L1 loss: 1.5912996530532837, Total Loss is: 10.956907272338867\n",
            "MSE Loss is: 0.013867724686861038, h Loss is: 10.944890975952148, L1 loss: 1.5895646810531616, Total Loss is: 10.958758354187012\n",
            "MSE Loss is: 0.014335872605443, h Loss is: 10.95712661743164, L1 loss: 1.5876528024673462, Total Loss is: 10.97146224975586\n",
            "MSE Loss is: 0.01397634856402874, h Loss is: 10.975146293640137, L1 loss: 1.586775541305542, Total Loss is: 10.98912239074707\n",
            "MSE Loss is: 0.013923046179115772, h Loss is: 10.981406211853027, L1 loss: 1.5868902206420898, Total Loss is: 10.995328903198242\n",
            "MSE Loss is: 0.014092111960053444, h Loss is: 10.982078552246094, L1 loss: 1.5882328748703003, Total Loss is: 10.996170997619629\n",
            "MSE Loss is: 0.014112487435340881, h Loss is: 10.951409339904785, L1 loss: 1.589516043663025, Total Loss is: 10.965521812438965\n",
            "MSE Loss is: 0.014240140095353127, h Loss is: 10.9066801071167, L1 loss: 1.5895226001739502, Total Loss is: 10.920920372009277\n",
            "MSE Loss is: 0.014116784557700157, h Loss is: 10.868301391601562, L1 loss: 1.5885151624679565, Total Loss is: 10.882418632507324\n",
            "MSE Loss is: 0.013932464644312859, h Loss is: 10.858067512512207, L1 loss: 1.587821364402771, Total Loss is: 10.871999740600586\n",
            "MSE Loss is: 0.013981658034026623, h Loss is: 10.87049674987793, L1 loss: 1.5874861478805542, Total Loss is: 10.884478569030762\n",
            "MSE Loss is: 0.014132969081401825, h Loss is: 10.910176277160645, L1 loss: 1.58711838722229, Total Loss is: 10.924308776855469\n",
            "MSE Loss is: 0.013965756632387638, h Loss is: 10.933274269104004, L1 loss: 1.5871509313583374, Total Loss is: 10.947239875793457\n",
            "MSE Loss is: 0.014148074202239513, h Loss is: 10.939379692077637, L1 loss: 1.587729811668396, Total Loss is: 10.953527450561523\n",
            "MSE Loss is: 0.014039401896297932, h Loss is: 10.906095504760742, L1 loss: 1.5883419513702393, Total Loss is: 10.920134544372559\n",
            "MSE Loss is: 0.013872053474187851, h Loss is: 10.863226890563965, L1 loss: 1.587558627128601, Total Loss is: 10.87709903717041\n",
            "MSE Loss is: 0.014240311458706856, h Loss is: 10.828964233398438, L1 loss: 1.586543083190918, Total Loss is: 10.843204498291016\n",
            "MSE Loss is: 0.013878453522920609, h Loss is: 10.818238258361816, L1 loss: 1.5856050252914429, Total Loss is: 10.832117080688477\n",
            "MSE Loss is: 0.013905676081776619, h Loss is: 10.853470802307129, L1 loss: 1.5853241682052612, Total Loss is: 10.867376327514648\n",
            "MSE Loss is: 0.013804897665977478, h Loss is: 10.888944625854492, L1 loss: 1.5867985486984253, Total Loss is: 10.902749061584473\n",
            "MSE Loss is: 0.014447905123233795, h Loss is: 10.89957046508789, L1 loss: 1.5885809659957886, Total Loss is: 10.914018630981445\n",
            "MSE Loss is: 0.01358456164598465, h Loss is: 10.882181167602539, L1 loss: 1.5884289741516113, Total Loss is: 10.89576530456543\n",
            "MSE Loss is: 0.01389029435813427, h Loss is: 10.85006332397461, L1 loss: 1.5870099067687988, Total Loss is: 10.863953590393066\n",
            "New h_val is : tf.Tensor(0.5094471, shape=(), dtype=float32)\n",
            "Epoch: {} 20\n",
            "MSE Loss is: 0.013711603358387947, h Loss is: 12.484451293945312, L1 loss: 1.5845993757247925, Total Loss is: 12.498163223266602\n",
            "MSE Loss is: 0.013600675389170647, h Loss is: 12.471023559570312, L1 loss: 1.5834020376205444, Total Loss is: 12.484623908996582\n",
            "MSE Loss is: 0.013797037303447723, h Loss is: 12.48032283782959, L1 loss: 1.5835527181625366, Total Loss is: 12.494119644165039\n",
            "MSE Loss is: 0.013514706864953041, h Loss is: 12.504657745361328, L1 loss: 1.5851351022720337, Total Loss is: 12.518172264099121\n",
            "MSE Loss is: 0.013911323621869087, h Loss is: 12.534407615661621, L1 loss: 1.5877598524093628, Total Loss is: 12.548318862915039\n",
            "MSE Loss is: 0.013812452554702759, h Loss is: 12.54245662689209, L1 loss: 1.5882505178451538, Total Loss is: 12.556268692016602\n",
            "MSE Loss is: 0.014119278639554977, h Loss is: 12.492433547973633, L1 loss: 1.5878671407699585, Total Loss is: 12.506552696228027\n",
            "MSE Loss is: 0.014319021254777908, h Loss is: 12.432018280029297, L1 loss: 1.5864756107330322, Total Loss is: 12.446337699890137\n",
            "MSE Loss is: 0.014157509431242943, h Loss is: 12.409711837768555, L1 loss: 1.5851212739944458, Total Loss is: 12.423869132995605\n",
            "MSE Loss is: 0.014574013650417328, h Loss is: 12.41685676574707, L1 loss: 1.5839143991470337, Total Loss is: 12.43143081665039\n",
            "MSE Loss is: 0.013947524130344391, h Loss is: 12.467120170593262, L1 loss: 1.5844682455062866, Total Loss is: 12.481067657470703\n",
            "MSE Loss is: 0.013994771055877209, h Loss is: 12.511148452758789, L1 loss: 1.58464515209198, Total Loss is: 12.52514362335205\n",
            "MSE Loss is: 0.014059843495488167, h Loss is: 12.511808395385742, L1 loss: 1.585830569267273, Total Loss is: 12.52586841583252\n",
            "MSE Loss is: 0.013878757134079933, h Loss is: 12.48620319366455, L1 loss: 1.5859708786010742, Total Loss is: 12.500082015991211\n",
            "MSE Loss is: 0.01360035128891468, h Loss is: 12.44792652130127, L1 loss: 1.584415316581726, Total Loss is: 12.461526870727539\n",
            "MSE Loss is: 0.013616025447845459, h Loss is: 12.408742904663086, L1 loss: 1.5829944610595703, Total Loss is: 12.422358512878418\n",
            "MSE Loss is: 0.013978743925690651, h Loss is: 12.410072326660156, L1 loss: 1.5836455821990967, Total Loss is: 12.424051284790039\n",
            "MSE Loss is: 0.01442047581076622, h Loss is: 12.412569999694824, L1 loss: 1.5846912860870361, Total Loss is: 12.426990509033203\n",
            "MSE Loss is: 0.014120237901806831, h Loss is: 12.429259300231934, L1 loss: 1.5844014883041382, Total Loss is: 12.443379402160645\n",
            "MSE Loss is: 0.014288535341620445, h Loss is: 12.439785957336426, L1 loss: 1.583648443222046, Total Loss is: 12.45407485961914\n",
            "MSE Loss is: 0.014129786752164364, h Loss is: 12.445713996887207, L1 loss: 1.5822076797485352, Total Loss is: 12.459843635559082\n",
            "MSE Loss is: 0.013779900968074799, h Loss is: 12.444731712341309, L1 loss: 1.5818265676498413, Total Loss is: 12.458511352539062\n",
            "MSE Loss is: 0.013898156583309174, h Loss is: 12.425406455993652, L1 loss: 1.5827476978302002, Total Loss is: 12.43930435180664\n",
            "MSE Loss is: 0.014031844213604927, h Loss is: 12.418062210083008, L1 loss: 1.584512710571289, Total Loss is: 12.432093620300293\n",
            "MSE Loss is: 0.013719337992370129, h Loss is: 12.413278579711914, L1 loss: 1.5858709812164307, Total Loss is: 12.426998138427734\n",
            "MSE Loss is: 0.01396620087325573, h Loss is: 12.395821571350098, L1 loss: 1.5858224630355835, Total Loss is: 12.409788131713867\n",
            "MSE Loss is: 0.01421835646033287, h Loss is: 12.384044647216797, L1 loss: 1.5844919681549072, Total Loss is: 12.398262977600098\n",
            "MSE Loss is: 0.013841052539646626, h Loss is: 12.383659362792969, L1 loss: 1.5824438333511353, Total Loss is: 12.397500038146973\n",
            "MSE Loss is: 0.014314152300357819, h Loss is: 12.388553619384766, L1 loss: 1.580579161643982, Total Loss is: 12.402867317199707\n",
            "MSE Loss is: 0.01392902247607708, h Loss is: 12.399362564086914, L1 loss: 1.5802476406097412, Total Loss is: 12.413291931152344\n",
            "MSE Loss is: 0.01388196274638176, h Loss is: 12.400132179260254, L1 loss: 1.5811647176742554, Total Loss is: 12.414013862609863\n",
            "MSE Loss is: 0.014071354642510414, h Loss is: 12.400691986083984, L1 loss: 1.583022952079773, Total Loss is: 12.414763450622559\n",
            "MSE Loss is: 0.014084113761782646, h Loss is: 12.368976593017578, L1 loss: 1.5840896368026733, Total Loss is: 12.383060455322266\n",
            "MSE Loss is: 0.01420203223824501, h Loss is: 12.322893142700195, L1 loss: 1.5833523273468018, Total Loss is: 12.337095260620117\n",
            "MSE Loss is: 0.01407752837985754, h Loss is: 12.283852577209473, L1 loss: 1.581781029701233, Total Loss is: 12.297929763793945\n",
            "MSE Loss is: 0.013919290155172348, h Loss is: 12.275389671325684, L1 loss: 1.5808922052383423, Total Loss is: 12.289308547973633\n",
            "MSE Loss is: 0.013951377011835575, h Loss is: 12.288829803466797, L1 loss: 1.5809520483016968, Total Loss is: 12.302781105041504\n",
            "MSE Loss is: 0.014088878408074379, h Loss is: 12.329200744628906, L1 loss: 1.5812797546386719, Total Loss is: 12.343289375305176\n",
            "MSE Loss is: 0.013921168632805347, h Loss is: 12.346746444702148, L1 loss: 1.5816681385040283, Total Loss is: 12.36066722869873\n",
            "MSE Loss is: 0.014119940809905529, h Loss is: 12.345901489257812, L1 loss: 1.5823954343795776, Total Loss is: 12.360021591186523\n",
            "MSE Loss is: 0.014016121625900269, h Loss is: 12.304383277893066, L1 loss: 1.582617163658142, Total Loss is: 12.318399429321289\n",
            "MSE Loss is: 0.013831814751029015, h Loss is: 12.259243965148926, L1 loss: 1.5812170505523682, Total Loss is: 12.273076057434082\n",
            "MSE Loss is: 0.014223729260265827, h Loss is: 12.229578971862793, L1 loss: 1.5799468755722046, Total Loss is: 12.243803024291992\n",
            "MSE Loss is: 0.013842493295669556, h Loss is: 12.226949691772461, L1 loss: 1.5791233777999878, Total Loss is: 12.240792274475098\n",
            "MSE Loss is: 0.013879653066396713, h Loss is: 12.271405220031738, L1 loss: 1.5792735815048218, Total Loss is: 12.285284996032715\n",
            "MSE Loss is: 0.013773411512374878, h Loss is: 12.305562973022461, L1 loss: 1.58124577999115, Total Loss is: 12.3193359375\n",
            "MSE Loss is: 0.014423754066228867, h Loss is: 12.303240776062012, L1 loss: 1.583342432975769, Total Loss is: 12.31766414642334\n",
            "MSE Loss is: 0.013568172231316566, h Loss is: 12.269618034362793, L1 loss: 1.5828497409820557, Total Loss is: 12.283185958862305\n",
            "MSE Loss is: 0.013870512135326862, h Loss is: 12.229318618774414, L1 loss: 1.5808457136154175, Total Loss is: 12.243188858032227\n",
            "New h_val is : tf.Tensor(0.4988613, shape=(), dtype=float32)\n",
            "Epoch: {} 21\n",
            "MSE Loss is: 0.013697104528546333, h Loss is: 13.95790958404541, L1 loss: 1.5781102180480957, Total Loss is: 13.971606254577637\n",
            "MSE Loss is: 0.013582214713096619, h Loss is: 13.959565162658691, L1 loss: 1.5770622491836548, Total Loss is: 13.97314739227295\n",
            "MSE Loss is: 0.013767387717962265, h Loss is: 13.984228134155273, L1 loss: 1.5777777433395386, Total Loss is: 13.997995376586914\n",
            "MSE Loss is: 0.013477782718837261, h Loss is: 14.01315689086914, L1 loss: 1.5799087285995483, Total Loss is: 14.026634216308594\n",
            "MSE Loss is: 0.013900622725486755, h Loss is: 14.034334182739258, L1 loss: 1.582863450050354, Total Loss is: 14.048234939575195\n",
            "MSE Loss is: 0.013778213411569595, h Loss is: 14.023858070373535, L1 loss: 1.5830180644989014, Total Loss is: 14.037636756896973\n",
            "MSE Loss is: 0.014080422930419445, h Loss is: 13.952900886535645, L1 loss: 1.5818442106246948, Total Loss is: 13.966980934143066\n",
            "MSE Loss is: 0.014305931515991688, h Loss is: 13.886507034301758, L1 loss: 1.5800775289535522, Total Loss is: 13.900813102722168\n",
            "MSE Loss is: 0.014132071286439896, h Loss is: 13.879097938537598, L1 loss: 1.5788592100143433, Total Loss is: 13.893230438232422\n",
            "MSE Loss is: 0.014537614770233631, h Loss is: 13.907225608825684, L1 loss: 1.578142762184143, Total Loss is: 13.92176342010498\n",
            "MSE Loss is: 0.013929481618106365, h Loss is: 13.97303581237793, L1 loss: 1.5795795917510986, Total Loss is: 13.98696517944336\n",
            "MSE Loss is: 0.013971364125609398, h Loss is: 14.011897087097168, L1 loss: 1.5801867246627808, Total Loss is: 14.02586841583252\n",
            "MSE Loss is: 0.01403227262198925, h Loss is: 13.988204956054688, L1 loss: 1.581139326095581, Total Loss is: 14.002237319946289\n",
            "MSE Loss is: 0.013858629390597343, h Loss is: 13.938810348510742, L1 loss: 1.580620288848877, Total Loss is: 13.952669143676758\n",
            "MSE Loss is: 0.013570912182331085, h Loss is: 13.892531394958496, L1 loss: 1.5786679983139038, Total Loss is: 13.906102180480957\n",
            "MSE Loss is: 0.01360800676047802, h Loss is: 13.864068031311035, L1 loss: 1.5772405862808228, Total Loss is: 13.877676010131836\n",
            "MSE Loss is: 0.01395192090421915, h Loss is: 13.889986991882324, L1 loss: 1.5783675909042358, Total Loss is: 13.903939247131348\n",
            "MSE Loss is: 0.014380201697349548, h Loss is: 13.9071683883667, L1 loss: 1.579790472984314, Total Loss is: 13.921548843383789\n",
            "MSE Loss is: 0.014109618961811066, h Loss is: 13.921567916870117, L1 loss: 1.5795992612838745, Total Loss is: 13.935677528381348\n",
            "MSE Loss is: 0.014275280758738518, h Loss is: 13.913830757141113, L1 loss: 1.578687071800232, Total Loss is: 13.928106307983398\n",
            "MSE Loss is: 0.014095509424805641, h Loss is: 13.899886131286621, L1 loss: 1.5769293308258057, Total Loss is: 13.913981437683105\n",
            "MSE Loss is: 0.013746526092290878, h Loss is: 13.890637397766113, L1 loss: 1.5764656066894531, Total Loss is: 13.904383659362793\n",
            "MSE Loss is: 0.013901375234127045, h Loss is: 13.87653923034668, L1 loss: 1.5776113271713257, Total Loss is: 13.890440940856934\n",
            "MSE Loss is: 0.014005031436681747, h Loss is: 13.88618278503418, L1 loss: 1.5796664953231812, Total Loss is: 13.900187492370605\n",
            "MSE Loss is: 0.013691588304936886, h Loss is: 13.895416259765625, L1 loss: 1.5810587406158447, Total Loss is: 13.90910816192627\n",
            "MSE Loss is: 0.013937978073954582, h Loss is: 13.877062797546387, L1 loss: 1.5807245969772339, Total Loss is: 13.891000747680664\n",
            "MSE Loss is: 0.014203120023012161, h Loss is: 13.853407859802246, L1 loss: 1.5790876150131226, Total Loss is: 13.867610931396484\n",
            "MSE Loss is: 0.013822532258927822, h Loss is: 13.839242935180664, L1 loss: 1.5769084692001343, Total Loss is: 13.853065490722656\n",
            "MSE Loss is: 0.014292582869529724, h Loss is: 13.836444854736328, L1 loss: 1.5753583908081055, Total Loss is: 13.850737571716309\n",
            "MSE Loss is: 0.013884587213397026, h Loss is: 13.850128173828125, L1 loss: 1.5756756067276, Total Loss is: 13.864012718200684\n",
            "MSE Loss is: 0.013857181183993816, h Loss is: 13.858667373657227, L1 loss: 1.5769482851028442, Total Loss is: 13.87252426147461\n",
            "MSE Loss is: 0.01405227743089199, h Loss is: 13.867164611816406, L1 loss: 1.5786374807357788, Total Loss is: 13.881217002868652\n",
            "MSE Loss is: 0.014054173603653908, h Loss is: 13.832444190979004, L1 loss: 1.5791891813278198, Total Loss is: 13.846498489379883\n",
            "MSE Loss is: 0.014172926545143127, h Loss is: 13.775945663452148, L1 loss: 1.5779075622558594, Total Loss is: 13.790118217468262\n",
            "MSE Loss is: 0.014058252796530724, h Loss is: 13.727139472961426, L1 loss: 1.5759811401367188, Total Loss is: 13.74119758605957\n",
            "MSE Loss is: 0.01390575896948576, h Loss is: 13.717225074768066, L1 loss: 1.5754477977752686, Total Loss is: 13.731130599975586\n",
            "MSE Loss is: 0.013923097401857376, h Loss is: 13.735331535339355, L1 loss: 1.576288104057312, Total Loss is: 13.74925422668457\n",
            "MSE Loss is: 0.014053711667656898, h Loss is: 13.783702850341797, L1 loss: 1.5773814916610718, Total Loss is: 13.79775619506836\n",
            "MSE Loss is: 0.013896259479224682, h Loss is: 13.801294326782227, L1 loss: 1.5778909921646118, Total Loss is: 13.815190315246582\n",
            "MSE Loss is: 0.014097725041210651, h Loss is: 13.794893264770508, L1 loss: 1.5781346559524536, Total Loss is: 13.808991432189941\n",
            "MSE Loss is: 0.013983878307044506, h Loss is: 13.742788314819336, L1 loss: 1.5775257349014282, Total Loss is: 13.7567720413208\n",
            "MSE Loss is: 0.013804685324430466, h Loss is: 13.692371368408203, L1 loss: 1.575677752494812, Total Loss is: 13.706175804138184\n",
            "MSE Loss is: 0.014216076582670212, h Loss is: 13.665857315063477, L1 loss: 1.574583888053894, Total Loss is: 13.680073738098145\n",
            "MSE Loss is: 0.013824423775076866, h Loss is: 13.67151927947998, L1 loss: 1.57437002658844, Total Loss is: 13.685343742370605\n",
            "MSE Loss is: 0.013851730152964592, h Loss is: 13.726052284240723, L1 loss: 1.5753437280654907, Total Loss is: 13.739904403686523\n",
            "MSE Loss is: 0.013750898651778698, h Loss is: 13.758949279785156, L1 loss: 1.5778449773788452, Total Loss is: 13.772700309753418\n",
            "MSE Loss is: 0.014413119293749332, h Loss is: 13.743494987487793, L1 loss: 1.5795831680297852, Total Loss is: 13.75790786743164\n",
            "MSE Loss is: 0.013549817726016045, h Loss is: 13.695083618164062, L1 loss: 1.5782761573791504, Total Loss is: 13.708633422851562\n",
            "MSE Loss is: 0.013850958086550236, h Loss is: 13.65021800994873, L1 loss: 1.5757410526275635, Total Loss is: 13.664069175720215\n",
            "New h_val is : tf.Tensor(0.48803377, shape=(), dtype=float32)\n",
            "Epoch: {} 22\n",
            "MSE Loss is: 0.013694754801690578, h Loss is: 15.478389739990234, L1 loss: 1.5730464458465576, Total Loss is: 15.492084503173828\n",
            "MSE Loss is: 0.01356923021376133, h Loss is: 15.496173858642578, L1 loss: 1.572662353515625, Total Loss is: 15.509742736816406\n",
            "MSE Loss is: 0.013744909316301346, h Loss is: 15.532261848449707, L1 loss: 1.5741115808486938, Total Loss is: 15.54600715637207\n",
            "MSE Loss is: 0.013453043065965176, h Loss is: 15.558820724487305, L1 loss: 1.576796531677246, Total Loss is: 15.572274208068848\n",
            "MSE Loss is: 0.013898714445531368, h Loss is: 15.566628456115723, L1 loss: 1.5795879364013672, Total Loss is: 15.580527305603027\n",
            "MSE Loss is: 0.013752961531281471, h Loss is: 15.539525032043457, L1 loss: 1.5789692401885986, Total Loss is: 15.553277969360352\n",
            "MSE Loss is: 0.014059238135814667, h Loss is: 15.455714225769043, L1 loss: 1.57705557346344, Total Loss is: 15.469773292541504\n",
            "MSE Loss is: 0.014290942810475826, h Loss is: 15.393526077270508, L1 loss: 1.5750858783721924, Total Loss is: 15.407816886901855\n",
            "MSE Loss is: 0.014110598713159561, h Loss is: 15.406696319580078, L1 loss: 1.574475646018982, Total Loss is: 15.420806884765625\n",
            "MSE Loss is: 0.01451268419623375, h Loss is: 15.452068328857422, L1 loss: 1.5745311975479126, Total Loss is: 15.466581344604492\n",
            "MSE Loss is: 0.013919759541749954, h Loss is: 15.522343635559082, L1 loss: 1.5763514041900635, Total Loss is: 15.536263465881348\n",
            "MSE Loss is: 0.013942122459411621, h Loss is: 15.544804573059082, L1 loss: 1.5767602920532227, Total Loss is: 15.558746337890625\n",
            "MSE Loss is: 0.014013875275850296, h Loss is: 15.494206428527832, L1 loss: 1.5771464109420776, Total Loss is: 15.508220672607422\n",
            "MSE Loss is: 0.013846442103385925, h Loss is: 15.430563926696777, L1 loss: 1.576207160949707, Total Loss is: 15.44441032409668\n",
            "MSE Loss is: 0.013554558157920837, h Loss is: 15.392024040222168, L1 loss: 1.5740056037902832, Total Loss is: 15.40557861328125\n",
            "MSE Loss is: 0.013595271855592728, h Loss is: 15.38513469696045, L1 loss: 1.5727365016937256, Total Loss is: 15.398730278015137\n",
            "MSE Loss is: 0.013939078897237778, h Loss is: 15.4344482421875, L1 loss: 1.5745009183883667, Total Loss is: 15.448387145996094\n",
            "MSE Loss is: 0.014363642781972885, h Loss is: 15.452804565429688, L1 loss: 1.576285481452942, Total Loss is: 15.467167854309082\n",
            "MSE Loss is: 0.014101902022957802, h Loss is: 15.449030876159668, L1 loss: 1.5759902000427246, Total Loss is: 15.463132858276367\n",
            "MSE Loss is: 0.014263201504945755, h Loss is: 15.416687965393066, L1 loss: 1.5746407508850098, Total Loss is: 15.430951118469238\n",
            "MSE Loss is: 0.01407469343394041, h Loss is: 15.391351699829102, L1 loss: 1.572621464729309, Total Loss is: 15.405426025390625\n",
            "MSE Loss is: 0.013729925267398357, h Loss is: 15.391575813293457, L1 loss: 1.5723212957382202, Total Loss is: 15.405305862426758\n",
            "MSE Loss is: 0.01390002854168415, h Loss is: 15.397233963012695, L1 loss: 1.5737532377243042, Total Loss is: 15.411133766174316\n",
            "MSE Loss is: 0.01398384664207697, h Loss is: 15.425463676452637, L1 loss: 1.5762139558792114, Total Loss is: 15.439447402954102\n",
            "MSE Loss is: 0.01367713138461113, h Loss is: 15.436527252197266, L1 loss: 1.5775854587554932, Total Loss is: 15.450204849243164\n",
            "MSE Loss is: 0.01391967386007309, h Loss is: 15.400446891784668, L1 loss: 1.5767898559570312, Total Loss is: 15.414366722106934\n",
            "MSE Loss is: 0.014195911586284637, h Loss is: 15.356352806091309, L1 loss: 1.5747621059417725, Total Loss is: 15.370548248291016\n",
            "MSE Loss is: 0.013809625059366226, h Loss is: 15.334287643432617, L1 loss: 1.572628140449524, Total Loss is: 15.34809684753418\n",
            "MSE Loss is: 0.01427870336920023, h Loss is: 15.339624404907227, L1 loss: 1.571574091911316, Total Loss is: 15.353902816772461\n",
            "MSE Loss is: 0.013857207261025906, h Loss is: 15.37060832977295, L1 loss: 1.5725442171096802, Total Loss is: 15.384465217590332\n",
            "MSE Loss is: 0.013840336352586746, h Loss is: 15.39002513885498, L1 loss: 1.5743259191513062, Total Loss is: 15.403865814208984\n",
            "MSE Loss is: 0.014035306870937347, h Loss is: 15.397233963012695, L1 loss: 1.5759376287460327, Total Loss is: 15.411269187927246\n",
            "MSE Loss is: 0.014030629768967628, h Loss is: 15.345599174499512, L1 loss: 1.5758005380630493, Total Loss is: 15.35962963104248\n",
            "MSE Loss is: 0.01415937952697277, h Loss is: 15.270854949951172, L1 loss: 1.5738881826400757, Total Loss is: 15.285014152526855\n",
            "MSE Loss is: 0.014052708633244038, h Loss is: 15.216117858886719, L1 loss: 1.5720102787017822, Total Loss is: 15.230170249938965\n",
            "MSE Loss is: 0.013889338821172714, h Loss is: 15.21670913696289, L1 loss: 1.5720804929733276, Total Loss is: 15.230598449707031\n",
            "MSE Loss is: 0.0139003312215209, h Loss is: 15.250287055969238, L1 loss: 1.5737158060073853, Total Loss is: 15.264187812805176\n",
            "MSE Loss is: 0.01403904426842928, h Loss is: 15.309093475341797, L1 loss: 1.575236439704895, Total Loss is: 15.323132514953613\n",
            "MSE Loss is: 0.013878673315048218, h Loss is: 15.319732666015625, L1 loss: 1.5754646062850952, Total Loss is: 15.333611488342285\n",
            "MSE Loss is: 0.014075908809900284, h Loss is: 15.298327445983887, L1 loss: 1.5750315189361572, Total Loss is: 15.312403678894043\n",
            "MSE Loss is: 0.013961339369416237, h Loss is: 15.231365203857422, L1 loss: 1.574165940284729, Total Loss is: 15.24532699584961\n",
            "MSE Loss is: 0.013788517564535141, h Loss is: 15.180042266845703, L1 loss: 1.572505235671997, Total Loss is: 15.193830490112305\n",
            "MSE Loss is: 0.014205943793058395, h Loss is: 15.16561508178711, L1 loss: 1.571960687637329, Total Loss is: 15.179821014404297\n",
            "MSE Loss is: 0.013811010867357254, h Loss is: 15.185932159423828, L1 loss: 1.5724010467529297, Total Loss is: 15.199743270874023\n",
            "MSE Loss is: 0.01383390836417675, h Loss is: 15.249999046325684, L1 loss: 1.5735914707183838, Total Loss is: 15.263833045959473\n",
            "MSE Loss is: 0.0137373898178339, h Loss is: 15.275055885314941, L1 loss: 1.5757142305374146, Total Loss is: 15.288793563842773\n",
            "MSE Loss is: 0.014403784647583961, h Loss is: 15.240194320678711, L1 loss: 1.57687509059906, Total Loss is: 15.254597663879395\n",
            "MSE Loss is: 0.013534687459468842, h Loss is: 15.177009582519531, L1 loss: 1.5753097534179688, Total Loss is: 15.190544128417969\n",
            "MSE Loss is: 0.013837965205311775, h Loss is: 15.134288787841797, L1 loss: 1.5729349851608276, Total Loss is: 15.148126602172852\n",
            "New h_val is : tf.Tensor(0.4778223, shape=(), dtype=float32)\n",
            "Epoch: {} 23\n",
            "MSE Loss is: 0.01369215827435255, h Loss is: 17.077781677246094, L1 loss: 1.570652961730957, Total Loss is: 17.091474533081055\n",
            "MSE Loss is: 0.013555675745010376, h Loss is: 17.11467933654785, L1 loss: 1.5709679126739502, Total Loss is: 17.12823486328125\n",
            "MSE Loss is: 0.013727815821766853, h Loss is: 17.157089233398438, L1 loss: 1.5729321241378784, Total Loss is: 17.17081642150879\n",
            "MSE Loss is: 0.013426685705780983, h Loss is: 17.172683715820312, L1 loss: 1.5754331350326538, Total Loss is: 17.18610954284668\n",
            "MSE Loss is: 0.013895714655518532, h Loss is: 17.162790298461914, L1 loss: 1.5776013135910034, Total Loss is: 17.176685333251953\n",
            "MSE Loss is: 0.013733262196183205, h Loss is: 17.123275756835938, L1 loss: 1.5764409303665161, Total Loss is: 17.137008666992188\n",
            "MSE Loss is: 0.014040004462003708, h Loss is: 17.03619956970215, L1 loss: 1.574499487876892, Total Loss is: 17.05023956298828\n",
            "MSE Loss is: 0.014279898256063461, h Loss is: 16.986614227294922, L1 loss: 1.5730011463165283, Total Loss is: 17.00089454650879\n",
            "MSE Loss is: 0.014095458202064037, h Loss is: 17.021480560302734, L1 loss: 1.573070764541626, Total Loss is: 17.03557586669922\n",
            "MSE Loss is: 0.014491898007690907, h Loss is: 17.07636070251465, L1 loss: 1.5735002756118774, Total Loss is: 17.090852737426758\n",
            "MSE Loss is: 0.013911955058574677, h Loss is: 17.1406307220459, L1 loss: 1.5752537250518799, Total Loss is: 17.154542922973633\n",
            "MSE Loss is: 0.013915687799453735, h Loss is: 17.141693115234375, L1 loss: 1.5750917196273804, Total Loss is: 17.155609130859375\n",
            "MSE Loss is: 0.014007845893502235, h Loss is: 17.069494247436523, L1 loss: 1.5750946998596191, Total Loss is: 17.0835018157959\n",
            "MSE Loss is: 0.01383240707218647, h Loss is: 17.004369735717773, L1 loss: 1.5742026567459106, Total Loss is: 17.01820182800293\n",
            "MSE Loss is: 0.013542093336582184, h Loss is: 16.984601974487305, L1 loss: 1.5723785161972046, Total Loss is: 16.998144149780273\n",
            "MSE Loss is: 0.013591321185231209, h Loss is: 17.000289916992188, L1 loss: 1.5716432332992554, Total Loss is: 17.01388168334961\n",
            "MSE Loss is: 0.013933612033724785, h Loss is: 17.063255310058594, L1 loss: 1.5737789869308472, Total Loss is: 17.07718849182129\n",
            "MSE Loss is: 0.014354811981320381, h Loss is: 17.06890106201172, L1 loss: 1.5753421783447266, Total Loss is: 17.083255767822266\n",
            "MSE Loss is: 0.01410252507776022, h Loss is: 17.040424346923828, L1 loss: 1.574372410774231, Total Loss is: 17.054527282714844\n",
            "MSE Loss is: 0.014263121411204338, h Loss is: 16.98979377746582, L1 loss: 1.5726243257522583, Total Loss is: 17.004056930541992\n",
            "MSE Loss is: 0.014062460511922836, h Loss is: 16.968429565429688, L1 loss: 1.5708483457565308, Total Loss is: 16.982492446899414\n",
            "MSE Loss is: 0.013725768774747849, h Loss is: 16.99139404296875, L1 loss: 1.5712730884552002, Total Loss is: 17.00511932373047\n",
            "MSE Loss is: 0.013904533348977566, h Loss is: 17.018083572387695, L1 loss: 1.573219656944275, Total Loss is: 17.0319881439209\n",
            "MSE Loss is: 0.013971112668514252, h Loss is: 17.054086685180664, L1 loss: 1.5756570100784302, Total Loss is: 17.068058013916016\n",
            "MSE Loss is: 0.01366841234266758, h Loss is: 17.05225372314453, L1 loss: 1.5766328573226929, Total Loss is: 17.065921783447266\n",
            "MSE Loss is: 0.013910806737840176, h Loss is: 16.99139404296875, L1 loss: 1.5753936767578125, Total Loss is: 17.00530433654785\n",
            "MSE Loss is: 0.014195015653967857, h Loss is: 16.933429718017578, L1 loss: 1.5731838941574097, Total Loss is: 16.94762420654297\n",
            "MSE Loss is: 0.013797170482575893, h Loss is: 16.918880462646484, L1 loss: 1.5714079141616821, Total Loss is: 16.93267822265625\n",
            "MSE Loss is: 0.014270996674895287, h Loss is: 16.945072174072266, L1 loss: 1.570955753326416, Total Loss is: 16.95934295654297\n",
            "MSE Loss is: 0.013841788284480572, h Loss is: 16.994340896606445, L1 loss: 1.5723270177841187, Total Loss is: 17.008182525634766\n",
            "MSE Loss is: 0.01382370013743639, h Loss is: 17.01353645324707, L1 loss: 1.573866367340088, Total Loss is: 17.027360916137695\n",
            "MSE Loss is: 0.014016312547028065, h Loss is: 17.005931854248047, L1 loss: 1.574976921081543, Total Loss is: 17.019948959350586\n",
            "MSE Loss is: 0.014014299027621746, h Loss is: 16.931848526000977, L1 loss: 1.5744999647140503, Total Loss is: 16.945863723754883\n",
            "MSE Loss is: 0.014152777381241322, h Loss is: 16.845712661743164, L1 loss: 1.5726613998413086, Total Loss is: 16.859865188598633\n",
            "MSE Loss is: 0.014052321203052998, h Loss is: 16.798873901367188, L1 loss: 1.5711736679077148, Total Loss is: 16.812925338745117\n",
            "MSE Loss is: 0.013873502612113953, h Loss is: 16.82018280029297, L1 loss: 1.5717724561691284, Total Loss is: 16.834056854248047\n",
            "MSE Loss is: 0.013883400708436966, h Loss is: 16.86837387084961, L1 loss: 1.5735770463943481, Total Loss is: 16.88225746154785\n",
            "MSE Loss is: 0.014031692408025265, h Loss is: 16.927356719970703, L1 loss: 1.5747603178024292, Total Loss is: 16.941389083862305\n",
            "MSE Loss is: 0.013860387727618217, h Loss is: 16.919706344604492, L1 loss: 1.5744500160217285, Total Loss is: 16.93356704711914\n",
            "MSE Loss is: 0.014054162427783012, h Loss is: 16.880712509155273, L1 loss: 1.5737662315368652, Total Loss is: 16.894765853881836\n",
            "MSE Loss is: 0.013948527164757252, h Loss is: 16.80689811706543, L1 loss: 1.5731323957443237, Total Loss is: 16.820846557617188\n",
            "MSE Loss is: 0.013771389611065388, h Loss is: 16.76694107055664, L1 loss: 1.5719326734542847, Total Loss is: 16.780712127685547\n",
            "MSE Loss is: 0.01419375091791153, h Loss is: 16.77176856994629, L1 loss: 1.5717397928237915, Total Loss is: 16.78596305847168\n",
            "MSE Loss is: 0.013796165585517883, h Loss is: 16.80343246459961, L1 loss: 1.572127103805542, Total Loss is: 16.817228317260742\n",
            "MSE Loss is: 0.013821808621287346, h Loss is: 16.867027282714844, L1 loss: 1.5729678869247437, Total Loss is: 16.880849838256836\n",
            "MSE Loss is: 0.013724331744015217, h Loss is: 16.8753604888916, L1 loss: 1.5747779607772827, Total Loss is: 16.88908576965332\n",
            "MSE Loss is: 0.01439699623733759, h Loss is: 16.82088279724121, L1 loss: 1.5758799314498901, Total Loss is: 16.83527946472168\n",
            "MSE Loss is: 0.013527203351259232, h Loss is: 16.751903533935547, L1 loss: 1.5744884014129639, Total Loss is: 16.765430450439453\n",
            "MSE Loss is: 0.013825110159814358, h Loss is: 16.722562789916992, L1 loss: 1.572404146194458, Total Loss is: 16.736387252807617\n",
            "New h_val is : tf.Tensor(0.46900272, shape=(), dtype=float32)\n",
            "Epoch: {} 24\n",
            "MSE Loss is: 0.013689905405044556, h Loss is: 18.80307960510254, L1 loss: 1.5704107284545898, Total Loss is: 18.816768646240234\n",
            "MSE Loss is: 0.013547748327255249, h Loss is: 18.85344886779785, L1 loss: 1.5708597898483276, Total Loss is: 18.86699676513672\n",
            "MSE Loss is: 0.01371364388614893, h Loss is: 18.889392852783203, L1 loss: 1.5725908279418945, Total Loss is: 18.903106689453125\n",
            "MSE Loss is: 0.013397923670709133, h Loss is: 18.884862899780273, L1 loss: 1.5747865438461304, Total Loss is: 18.89826011657715\n",
            "MSE Loss is: 0.013895053416490555, h Loss is: 18.85967254638672, L1 loss: 1.5769062042236328, Total Loss is: 18.873567581176758\n",
            "MSE Loss is: 0.013713756576180458, h Loss is: 18.81960678100586, L1 loss: 1.5757620334625244, Total Loss is: 18.83332061767578\n",
            "MSE Loss is: 0.014016442932188511, h Loss is: 18.740848541259766, L1 loss: 1.5739144086837769, Total Loss is: 18.754865646362305\n",
            "MSE Loss is: 0.014270256273448467, h Loss is: 18.707143783569336, L1 loss: 1.5725263357162476, Total Loss is: 18.72141456604004\n",
            "MSE Loss is: 0.014083203859627247, h Loss is: 18.756946563720703, L1 loss: 1.5726869106292725, Total Loss is: 18.77103042602539\n",
            "MSE Loss is: 0.01446957141160965, h Loss is: 18.809520721435547, L1 loss: 1.5730805397033691, Total Loss is: 18.823989868164062\n",
            "MSE Loss is: 0.013906020671129227, h Loss is: 18.860637664794922, L1 loss: 1.5748162269592285, Total Loss is: 18.874544143676758\n",
            "MSE Loss is: 0.013890632428228855, h Loss is: 18.84334373474121, L1 loss: 1.574593424797058, Total Loss is: 18.857234954833984\n",
            "MSE Loss is: 0.014002570882439613, h Loss is: 18.760589599609375, L1 loss: 1.5746349096298218, Total Loss is: 18.77459144592285\n",
            "MSE Loss is: 0.013815144076943398, h Loss is: 18.705554962158203, L1 loss: 1.573761224746704, Total Loss is: 18.719369888305664\n",
            "MSE Loss is: 0.013533827848732471, h Loss is: 18.708290100097656, L1 loss: 1.5718790292739868, Total Loss is: 18.721824645996094\n",
            "MSE Loss is: 0.01359099242836237, h Loss is: 18.739540100097656, L1 loss: 1.571401596069336, Total Loss is: 18.753131866455078\n",
            "MSE Loss is: 0.01392380241304636, h Loss is: 18.80412483215332, L1 loss: 1.5735960006713867, Total Loss is: 18.81804847717285\n",
            "MSE Loss is: 0.014349821954965591, h Loss is: 18.788625717163086, L1 loss: 1.5751206874847412, Total Loss is: 18.802974700927734\n",
            "MSE Loss is: 0.014110124669969082, h Loss is: 18.73794937133789, L1 loss: 1.5739414691925049, Total Loss is: 18.752059936523438\n",
            "MSE Loss is: 0.0142604885622859, h Loss is: 18.681049346923828, L1 loss: 1.5722084045410156, Total Loss is: 18.695310592651367\n",
            "MSE Loss is: 0.01405391562730074, h Loss is: 18.676624298095703, L1 loss: 1.5704580545425415, Total Loss is: 18.690677642822266\n",
            "MSE Loss is: 0.013733957894146442, h Loss is: 18.726762771606445, L1 loss: 1.5711616277694702, Total Loss is: 18.740497589111328\n",
            "MSE Loss is: 0.01390956062823534, h Loss is: 18.76698875427246, L1 loss: 1.5733193159103394, Total Loss is: 18.780899047851562\n",
            "MSE Loss is: 0.013962561264634132, h Loss is: 18.797683715820312, L1 loss: 1.5756587982177734, Total Loss is: 18.8116455078125\n",
            "MSE Loss is: 0.013665061444044113, h Loss is: 18.774656295776367, L1 loss: 1.5764504671096802, Total Loss is: 18.788320541381836\n",
            "MSE Loss is: 0.013913488015532494, h Loss is: 18.691892623901367, L1 loss: 1.5749229192733765, Total Loss is: 18.705806732177734\n",
            "MSE Loss is: 0.014192543923854828, h Loss is: 18.63251304626465, L1 loss: 1.5727615356445312, Total Loss is: 18.646705627441406\n",
            "MSE Loss is: 0.013788221403956413, h Loss is: 18.63818359375, L1 loss: 1.571144938468933, Total Loss is: 18.6519718170166\n",
            "MSE Loss is: 0.014269990846514702, h Loss is: 18.688573837280273, L1 loss: 1.5711153745651245, Total Loss is: 18.702844619750977\n",
            "MSE Loss is: 0.013834093697369099, h Loss is: 18.74785041809082, L1 loss: 1.5726326704025269, Total Loss is: 18.76168441772461\n",
            "MSE Loss is: 0.013806350529193878, h Loss is: 18.753686904907227, L1 loss: 1.5740852355957031, Total Loss is: 18.767494201660156\n",
            "MSE Loss is: 0.013994883745908737, h Loss is: 18.724306106567383, L1 loss: 1.575028419494629, Total Loss is: 18.738300323486328\n",
            "MSE Loss is: 0.014001995325088501, h Loss is: 18.632129669189453, L1 loss: 1.5743497610092163, Total Loss is: 18.64613151550293\n",
            "MSE Loss is: 0.014145820401608944, h Loss is: 18.546852111816406, L1 loss: 1.572417140007019, Total Loss is: 18.560997009277344\n",
            "MSE Loss is: 0.01405366137623787, h Loss is: 18.518768310546875, L1 loss: 1.5712263584136963, Total Loss is: 18.532821655273438\n",
            "MSE Loss is: 0.01386160496622324, h Loss is: 18.56211280822754, L1 loss: 1.5719088315963745, Total Loss is: 18.575973510742188\n",
            "MSE Loss is: 0.013872241601347923, h Loss is: 18.615739822387695, L1 loss: 1.5737671852111816, Total Loss is: 18.62961196899414\n",
            "MSE Loss is: 0.01402752473950386, h Loss is: 18.663105010986328, L1 loss: 1.5749143362045288, Total Loss is: 18.67713165283203\n",
            "MSE Loss is: 0.013846353627741337, h Loss is: 18.632129669189453, L1 loss: 1.5745428800582886, Total Loss is: 18.64597511291504\n",
            "MSE Loss is: 0.01403157040476799, h Loss is: 18.58191680908203, L1 loss: 1.5739339590072632, Total Loss is: 18.595949172973633\n",
            "MSE Loss is: 0.013940798118710518, h Loss is: 18.51360321044922, L1 loss: 1.5733650922775269, Total Loss is: 18.527544021606445\n",
            "MSE Loss is: 0.013755831867456436, h Loss is: 18.49420928955078, L1 loss: 1.5722601413726807, Total Loss is: 18.507965087890625\n",
            "MSE Loss is: 0.014181651175022125, h Loss is: 18.517242431640625, L1 loss: 1.5721187591552734, Total Loss is: 18.531423568725586\n",
            "MSE Loss is: 0.013775749132037163, h Loss is: 18.55006980895996, L1 loss: 1.5725663900375366, Total Loss is: 18.563844680786133\n",
            "MSE Loss is: 0.013808619230985641, h Loss is: 18.6026668548584, L1 loss: 1.5734370946884155, Total Loss is: 18.61647605895996\n",
            "MSE Loss is: 0.013708203099668026, h Loss is: 18.59158706665039, L1 loss: 1.5751386880874634, Total Loss is: 18.605295181274414\n",
            "MSE Loss is: 0.014388191513717175, h Loss is: 18.524860382080078, L1 loss: 1.5761442184448242, Total Loss is: 18.539249420166016\n",
            "MSE Loss is: 0.013523345813155174, h Loss is: 18.461971282958984, L1 loss: 1.5746208429336548, Total Loss is: 18.475494384765625\n",
            "MSE Loss is: 0.01380828581750393, h Loss is: 18.45353126525879, L1 loss: 1.572579026222229, Total Loss is: 18.46734046936035\n",
            "New h_val is : tf.Tensor(0.4617386, shape=(), dtype=float32)\n",
            "saving model to: /content//CausalNN_model_final_1707464986.h5\n",
            "53/53 [==============================] - 1s 6ms/step\n",
            "The conv layer 1 weights before training : [[ 0.01729116 -0.03241783  0.20739552  0.21393666  0.05369946  0.34967968\n",
            "  -0.14644605 -0.3412853  -0.11558445 -0.22822307  0.21469983  0.0880048\n",
            "  -0.1930318   0.18624505 -0.17902574 -0.3153036  -0.22727013 -0.03919902\n",
            "   0.06509387 -0.07369781 -0.10506889 -0.03960952  0.17076865  0.15640828]]\n",
            "The conv layer 1 weights after training : [[-0.23312184  0.13806252  0.2729899   0.27568594  0.28196365  0.12669523\n",
            "  -0.7090765   0.02012092  0.4811537   0.11464132  0.21264674  0.19052123\n",
            "  -0.18963216  0.06925663  0.24608025  0.21521682  0.23482773  0.11950021\n",
            "  -0.82986623 -0.03064938  0.00816926  0.1104985   0.1763696   0.11492899]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdgckClHP1y_",
        "outputId": "0b61238e-4186-4504-b606-6c29b0e8cba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the predicted adjacency for visualization as a table."
      ],
      "metadata": {
        "id": "ZatpiLYz0yBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckpp6cuerupo"
      },
      "outputs": [],
      "source": [
        "mat_df_2d_s = pd.DataFrame(mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat_f_df = pd.DataFrame(mat).T"
      ],
      "metadata": {
        "id": "UAa0WohUJ_t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "9CtUItJErupo",
        "outputId": "f955b269-627c-4827-c151-e8c38522ba22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3\n",
              "0  -0.233122  0.043526  0.018320  0.068698\n",
              "1   0.138063  0.053958  0.003803  0.027982\n",
              "2   0.272990  0.247817 -0.277378  0.021471\n",
              "3   0.275686  0.104281 -0.043272  0.026420\n",
              "4   0.281964  0.017565  0.126100  0.014201\n",
              "5   0.126695  0.004009  0.002440  0.016733\n",
              "6  -0.709077  0.026334  0.033168  0.006583\n",
              "7   0.020121  0.048152  0.008929  0.000444\n",
              "8   0.481154 -0.002959 -0.152024  0.019446\n",
              "9   0.114641  0.026313 -0.015932  0.017424\n",
              "10  0.212647  0.029222  0.028919  0.034838\n",
              "11  0.190521  0.019634 -0.034165  0.058747\n",
              "12 -0.189632  0.041779 -0.061465  0.242814\n",
              "13  0.069257  0.035556  0.025312  0.015233\n",
              "14  0.246080  0.231874 -0.240473 -0.041419\n",
              "15  0.215217  0.084713 -0.126795  0.132420\n",
              "16  0.234828  0.774139  1.614583  0.292265\n",
              "17  0.119500  0.017817  0.026789 -0.032045\n",
              "18 -0.829866  0.068498 -0.115307  0.648405\n",
              "19 -0.030649  0.067215  0.236879 -0.940291\n",
              "20  0.000000  0.115544  0.088467  0.005408\n",
              "21  0.110498  0.000000 -0.245076 -0.029004\n",
              "22  0.176370 -0.324637  0.000000  0.684757\n",
              "23  0.114929 -0.059304  0.966341  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7118e9e-e0ac-406b-9c48-7f7094fdcef7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.233122</td>\n",
              "      <td>0.043526</td>\n",
              "      <td>0.018320</td>\n",
              "      <td>0.068698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.138063</td>\n",
              "      <td>0.053958</td>\n",
              "      <td>0.003803</td>\n",
              "      <td>0.027982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.272990</td>\n",
              "      <td>0.247817</td>\n",
              "      <td>-0.277378</td>\n",
              "      <td>0.021471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.275686</td>\n",
              "      <td>0.104281</td>\n",
              "      <td>-0.043272</td>\n",
              "      <td>0.026420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.281964</td>\n",
              "      <td>0.017565</td>\n",
              "      <td>0.126100</td>\n",
              "      <td>0.014201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.126695</td>\n",
              "      <td>0.004009</td>\n",
              "      <td>0.002440</td>\n",
              "      <td>0.016733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.709077</td>\n",
              "      <td>0.026334</td>\n",
              "      <td>0.033168</td>\n",
              "      <td>0.006583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.020121</td>\n",
              "      <td>0.048152</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.000444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.481154</td>\n",
              "      <td>-0.002959</td>\n",
              "      <td>-0.152024</td>\n",
              "      <td>0.019446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.114641</td>\n",
              "      <td>0.026313</td>\n",
              "      <td>-0.015932</td>\n",
              "      <td>0.017424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.212647</td>\n",
              "      <td>0.029222</td>\n",
              "      <td>0.028919</td>\n",
              "      <td>0.034838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.190521</td>\n",
              "      <td>0.019634</td>\n",
              "      <td>-0.034165</td>\n",
              "      <td>0.058747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.189632</td>\n",
              "      <td>0.041779</td>\n",
              "      <td>-0.061465</td>\n",
              "      <td>0.242814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.069257</td>\n",
              "      <td>0.035556</td>\n",
              "      <td>0.025312</td>\n",
              "      <td>0.015233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.246080</td>\n",
              "      <td>0.231874</td>\n",
              "      <td>-0.240473</td>\n",
              "      <td>-0.041419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.215217</td>\n",
              "      <td>0.084713</td>\n",
              "      <td>-0.126795</td>\n",
              "      <td>0.132420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.234828</td>\n",
              "      <td>0.774139</td>\n",
              "      <td>1.614583</td>\n",
              "      <td>0.292265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.119500</td>\n",
              "      <td>0.017817</td>\n",
              "      <td>0.026789</td>\n",
              "      <td>-0.032045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.829866</td>\n",
              "      <td>0.068498</td>\n",
              "      <td>-0.115307</td>\n",
              "      <td>0.648405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.030649</td>\n",
              "      <td>0.067215</td>\n",
              "      <td>0.236879</td>\n",
              "      <td>-0.940291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115544</td>\n",
              "      <td>0.088467</td>\n",
              "      <td>0.005408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.110498</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.245076</td>\n",
              "      <td>-0.029004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.176370</td>\n",
              "      <td>-0.324637</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.114929</td>\n",
              "      <td>-0.059304</td>\n",
              "      <td>0.966341</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7118e9e-e0ac-406b-9c48-7f7094fdcef7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7118e9e-e0ac-406b-9c48-7f7094fdcef7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7118e9e-e0ac-406b-9c48-7f7094fdcef7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b31cdacd-f61e-428f-b029-f5bd46285b96\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b31cdacd-f61e-428f-b029-f5bd46285b96')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b31cdacd-f61e-428f-b029-f5bd46285b96 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "mat_df_2d_s.T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Causal Graph"
      ],
      "metadata": {
        "id": "rK-co1IcGV7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvIvNNEjrupo"
      },
      "outputs": [],
      "source": [
        "matrix_2d_2d_s = mat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/proposed-full-adj-mat-D2.csv', matrix_2d_2d_s, delimiter=\",\")"
      ],
      "metadata": {
        "id": "JVRuNC_KSoSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCrZpZYQrupo"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "G_2d_s = nx.DiGraph()\n",
        "\n",
        "nodes_2d_s = [\"S1(t-5)\", \"S2(t-5)\", \"S3(t-5)\",  \"S4(t-5)\",\n",
        "         \"S1(t-4)\", \"S2(t-4)\", \"S3(t-4)\",  \"S4(t-4)\",\n",
        "         \"S1(t-3)\", \"S2(t-3)\", \"S3(t-3)\",  \"S4(t-3)\",\n",
        "         \"S1(t-2)\", \"S2(t-2)\", \"S3(t-2)\",  \"S4(t-2)\",\n",
        "         \"S1(t-1)\", \"S2(t-1)\", \"S3(t-1)\",  \"S4(t-1)\",\n",
        "         \"S1(t)\", \"S2(t)\", \"S3(t)\",  \"S4(t)\"]\n",
        "nodes_r_2d_s=[\"S1(t)\", \"S2(t)\", \"S3(t)\",  \"S4(t)\"]\n",
        "pred_graph_f = np.zeros((4,24))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nzqj5wyrupo"
      },
      "outputs": [],
      "source": [
        "for i in range (0, 24):\n",
        "  G_2d_s.add_node(nodes_2d_s[i],pos=(int(i/4)+1,(i%4)+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsJrb53Krupp",
        "outputId": "5f51bc84-287c-42a3-e591-f6608ccf57f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 0\n",
            "16 1\n",
            "16 2\n",
            "18 3\n",
            "22 3\n",
            "23 2\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 24):\n",
        "  for j in range (0, 4):\n",
        "    if matrix_2d_2d_s[j,i] > 0.3:\n",
        "      print(i,j)\n",
        "      G_2d_s.add_edge(nodes_2d_s[i], nodes_r_2d_s[j], weight=i)\n",
        "      pred_graph_f[j,i]=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtzcncqBrupp",
        "outputId": "7063b753-50e7-4f6a-a9ce-cf22962779b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'S1(t-5)': (1, 1),\n",
              " 'S2(t-5)': (1, 2),\n",
              " 'S3(t-5)': (1, 3),\n",
              " 'S4(t-5)': (1, 4),\n",
              " 'S1(t-4)': (2, 1),\n",
              " 'S2(t-4)': (2, 2),\n",
              " 'S3(t-4)': (2, 3),\n",
              " 'S4(t-4)': (2, 4),\n",
              " 'S1(t-3)': (3, 1),\n",
              " 'S2(t-3)': (3, 2),\n",
              " 'S3(t-3)': (3, 3),\n",
              " 'S4(t-3)': (3, 4),\n",
              " 'S1(t-2)': (4, 1),\n",
              " 'S2(t-2)': (4, 2),\n",
              " 'S3(t-2)': (4, 3),\n",
              " 'S4(t-2)': (4, 4),\n",
              " 'S1(t-1)': (5, 1),\n",
              " 'S2(t-1)': (5, 2),\n",
              " 'S3(t-1)': (5, 3),\n",
              " 'S4(t-1)': (5, 4),\n",
              " 'S1(t)': (6, 1),\n",
              " 'S2(t)': (6, 2),\n",
              " 'S3(t)': (6, 3),\n",
              " 'S4(t)': (6, 4)}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "pos_2d_s=nx.get_node_attributes(G_2d_s,'pos')\n",
        "pos_2d_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QoGvVgS6SbW"
      },
      "outputs": [],
      "source": [
        "weights_2d_s = nx.get_edge_attributes(G_2d_s,'weight').values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting the predicted full temporal causal graph."
      ],
      "metadata": {
        "id": "nJoZXax_098s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "nx.draw(G_2d_s, pos_2d_s, cmap = plt.get_cmap('jet'), edge_cmap= plt.cm.tab20, edge_color=weights_2d_s, with_labels = True, connectionstyle='arc3, rad = 0.3')\n",
        "#nx.draw_networkx(G, with_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "JVCcYgyIV3vo",
        "outputId": "a981903f-5bc2-4b76-f911-e02dae023d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cdt.metrics.SHD(true_full_graph, pred_graph_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWBPKyEhfReZ",
        "outputId": "aee483dd-d2d1-42af-eb35-9f04965b5c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.0"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_values = true_full_graph\n",
        "predictions = pred_graph_f\n",
        "\n",
        "N = true_values.shape[1]*true_values.shape[0]\n",
        "accuracy = (true_values == predictions).sum() / N\n",
        "TP = ((predictions == 1) & (true_values == 1)).sum()\n",
        "FP = ((predictions == 1) & (true_values == 0)).sum()\n",
        "TN = ((predictions == 0) & (true_values == 0)).sum()\n",
        "FN = ((predictions == 0) & (true_values == 1)).sum()\n",
        "precision = TP / (TP+FP)\n",
        "recall = TP / (TP + FN)\n",
        "FDR = FP / (FP + TP)\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('Accuracy: {}, Precision: {}, Recall: {}, FDR: {}, F1 Score: {}'.format(accuracy, precision, recall, FDR,F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeXtHJDEfJzy",
        "outputId": "d8585041-45a6-40f7-c476-833e2a141d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9270833333333334, Precision: 0.5, Recall: 0.42857142857142855, FDR: 0.5, F1 Score: 0.4615384615384615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_full_graph.reshape(-1), pred_graph_f.reshape(-1), normalize=True)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_full_graph.reshape(-1), pred_graph_f.reshape(-1))\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(true_full_graph.reshape(-1), pred_graph_f.reshape(-1))\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(true_full_graph.reshape(-1), pred_graph_f.reshape(-1))\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-9AJMihfVBw",
        "outputId": "6c35ee23-09ed-45be-d127-9e7884ed9478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9270833333333334\n",
            "Precision: 0.5\n",
            "Recall (Sensitivity): 0.42857142857142855\n",
            "F1-Score: 0.4615384615384615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo5Wyjm6qRR"
      },
      "source": [
        "##Summary Causal Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfX8KQwJ6qRR"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "G_2d_s1 = nx.DiGraph()\n",
        "\n",
        "nodes_2d_s1 = [\"S1\", \"S2\", \"S3\",  \"S4\"]\n",
        "nodes_r_2d_s1= [\"S1\", \"S2\", \"S3\",  \"S4\"]\n",
        "edges_2d_s1 = []\n",
        "pred_graph_s1 = np.zeros((4,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79j7uDTN6qRR"
      },
      "outputs": [],
      "source": [
        "for i in range (0, 4):\n",
        "  G_2d_s1.add_node(nodes_2d_s1[i],pos=(int(i/2)+1,(i%2)+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyYUYXSb6qRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb49657-f0ff-40a1-acac-36f244e3119b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 0\n",
            "16 1\n",
            "16 2\n",
            "18 3\n",
            "22 3\n",
            "23 2\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 24):\n",
        "  for j in range (0, 4):\n",
        "    if matrix_2d_2d_s[j,i] > 0.3:\n",
        "      print(i,j)\n",
        "      G_2d_s1.add_edge(nodes_2d_s1[i%4], nodes_r_2d_s1[j], weight=i)\n",
        "      pred_graph_s1[i%4, j]=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQRq97oL6qRS",
        "outputId": "3cfcb228-17c5-4b6f-f345-e1aa88133541"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'S1': (1, 1), 'S2': (1, 2), 'S3': (2, 1), 'S4': (2, 2)}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "pos_2d_s1=nx.get_node_attributes(G_2d_s1,'pos')\n",
        "pos_2d_s1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938WuKdz6qRS"
      },
      "outputs": [],
      "source": [
        "weights_2d_s1 = nx.get_edge_attributes(G_2d_s1,'weight').values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "0ozgLzAy6qRS",
        "outputId": "d929e515-c5c6-4000-bcea-ccb443377371"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt #####\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "nx.draw(G_2d_s1, pos_2d_s1, cmap = plt.get_cmap('jet'), edge_cmap= plt.cm.tab20, edge_color=weights_2d_s1, with_labels = True, connectionstyle='arc3, rad = 0.3')\n",
        "#nx.draw_networkx(G, with_labels = True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_graph_s1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7XG_bPVY2l4",
        "outputId": "ed94e0d2-422b-4aea-dadf-19206f306c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tiiQoIcRobc",
        "outputId": "f7fdefeb-9d41-41eb-ee62-addb82d0b25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/proposed-summary-adj-mat-D2.csv', pred_graph_s1, delimiter=\",\")"
      ],
      "metadata": {
        "id": "6DgXJmarS6p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdt.metrics.SHD(true_graph, pred_graph_s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_1qPVtYFV73",
        "outputId": "9bade0e3-059a-473c-c30f-947bb5b7ee9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_values = true_graph\n",
        "predictions = pred_graph_s1\n",
        "\n",
        "N = true_values.shape[1]*true_values.shape[0]\n",
        "accuracy = (true_values == predictions).sum() / N\n",
        "TP = ((predictions == 1) & (true_values == 1)).sum()\n",
        "FP = ((predictions == 1) & (true_values == 0)).sum()\n",
        "TN = ((predictions == 0) & (true_values == 0)).sum()\n",
        "FN = ((predictions == 0) & (true_values == 1)).sum()\n",
        "precision = TP / (TP+FP)\n",
        "recall = TP / (TP + FN)\n",
        "FDR = FP / (FP + TP)\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('Accuracy: {}, Precision: {}, Recall: {}, FDR: {}, F1 Score: {}'.format(accuracy, precision, recall, FDR,F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTy-ZiS66Sbr",
        "outputId": "a86be7bc-6371-42a7-82ba-dfd5fac8fdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8125, Precision: 0.8, Recall: 0.6666666666666666, FDR: 0.2, F1 Score: 0.7272727272727272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_graph.reshape(-1), pred_graph_s1.reshape(-1), normalize=True)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_graph.reshape(-1), pred_graph_s1.reshape(-1))\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall (sensitivity)\n",
        "recall = recall_score(true_graph.reshape(-1), pred_graph_s1.reshape(-1))\n",
        "print(\"Recall (Sensitivity):\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(true_graph.reshape(-1), pred_graph_s1.reshape(-1))\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVmKWvf936c9",
        "outputId": "8ad89a8a-b3ff-47f8-dbe9-4a77213c22a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8125\n",
            "Precision: 0.8\n",
            "Recall (Sensitivity): 0.6666666666666666\n",
            "F1-Score: 0.7272727272727272\n"
          ]
        }
      ]
    }
  ]
}